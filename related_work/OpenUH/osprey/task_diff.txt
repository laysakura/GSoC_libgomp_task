Index: libopenmp/omp_runtime.c
===================================================================
--- libopenmp/omp_runtime.c	(リビジョン 3578)
+++ libopenmp/omp_runtime.c	(リビジョン 3580)
@@ -499,9 +499,9 @@
     *pupper = __omp_root_team.loop_upper_bound;
     /* Warning: Don't know how pstride should be properly set*/
     *pstride = __omp_root_team.loop_increament;
-    //*plastiter = 1;
     __omp_root_v_thread.ordered_count = 0;
     __ompc_set_state(THR_WORK_STATE);
+
     return 1;
   }
 
@@ -530,6 +530,7 @@
     *pstride = p_team->loop_increament;
     p_vthread->ordered_count = 0;
     __ompc_set_state(THR_WORK_STATE);
+
     return 1;
   }
 
@@ -539,8 +540,8 @@
     /* specified by OMP_SCHEDULE */
     if (p_vthread->schedule_count != 0) {
       /* no more iteration*/
-       __ompc_set_state(THR_WORK_STATE);
-       return 0;
+      __ompc_set_state(THR_WORK_STATE);
+      return 0;
     }
     global_lower = p_team->loop_lower_bound;
     global_upper = p_team->loop_upper_bound;
@@ -571,6 +572,7 @@
 	return 0;
     } 
     __ompc_set_state(THR_WORK_STATE);
+
     return 1;
     break;
   case OMP_SCHED_STATIC:
@@ -712,6 +714,7 @@
 	return 0;
     }
     __ompc_set_state(THR_WORK_STATE);
+
     return 1;
     break;
   case OMP_SCHED_ORDERED_STATIC:
@@ -834,6 +837,7 @@
     Not_Valid(" unknown schedule type specified");
   }
   __ompc_set_state(THR_WORK_STATE);
+
   return 0;
 }
 
@@ -871,10 +875,10 @@
     *pupper = __omp_root_team.loop_upper_bound;
     /* Warning: Don't know how pstride should be properly set*/
     *pstride = __omp_root_team.loop_increament;
-    //*plastiter = 1;
     __omp_root_v_thread.ordered_count = 0;
     /* no need to schedule anymore iterations*/
     __ompc_set_state(THR_WORK_STATE);
+    /* note: TSU does return 1; */
     return 0;
   }
 
@@ -903,7 +907,8 @@
     *pstride = p_team->loop_increament;
     p_vthread->ordered_count = 0;
     __ompc_set_state(THR_WORK_STATE);
-    return 0;
+
+    return 1;
   }
 
   /* Normal multi-thread multi-time schedule*/
@@ -944,6 +949,7 @@
 	return 0;
     } 
     __ompc_set_state(THR_WORK_STATE);
+
     return 1;
     break;
   case OMP_SCHED_STATIC:
@@ -1085,6 +1091,7 @@
 	return 0;
     }
     __ompc_set_state(THR_WORK_STATE);
+
     return 1;
     break;
   case OMP_SCHED_ORDERED_STATIC:
@@ -1206,9 +1213,445 @@
     Not_Valid(" unknown schedule type specified");
   }
   __ompc_set_state(THR_WORK_STATE);
+
   return 0;
 }
 
+static void
+__ompc_collapsed_loop_info_init(omp_team_t *p_team, omp_uint32 collapse_count, va_list ap)
+{
+  omp_loop_info_t* loop_info;
+  omp_uint64 iter_count = 1;
+  int i;
+  p_team->collapse_count = collapse_count;
+  if (collapse_count > p_team->loop_info_size) {
+    if (p_team->loop_info_size == 0) {
+      p_team->loop_info_size = 4;
+    }
+    while (collapse_count > p_team->loop_info_size) {
+      p_team->loop_info_size *= 2;
+    }
+    if (p_team->loop_info) {
+      free(p_team->loop_info);
+      free(p_team->loop_lenv);
+    }
+    p_team->loop_info = (omp_loop_info_t*) malloc(sizeof(omp_loop_info_t) * p_team->loop_info_size);
+    p_team->loop_lenv = malloc(sizeof(omp_uint64) * p_team->loop_info_size);
+  }
+  loop_info = p_team->loop_info;
+  for (i = 0; i < collapse_count; i++) {
+    loop_info->is_64bit = va_arg(ap, omp_uint32);
+    if (loop_info->is_64bit) {
+      loop_info->lower_bound = va_arg(ap, omp_uint64);
+      loop_info->upper_bound = va_arg(ap, omp_uint64);
+      loop_info->incr = va_arg(ap, omp_int64);
+      if (loop_info->incr > 0) {
+        if (loop_info->upper_bound >= loop_info->lower_bound) {
+          p_team->loop_lenv[i] = ((omp_uint64)(loop_info->upper_bound - loop_info->lower_bound)) / ((omp_uint64)loop_info->incr) + 1;
+        } else {
+          p_team->loop_lenv[i] = 0;
+        }
+      } else {
+        if (loop_info->upper_bound <= loop_info->lower_bound) {
+          p_team->loop_lenv[i] = ((omp_uint64)(loop_info->lower_bound - loop_info->upper_bound)) / ((omp_uint64)(-loop_info->incr)) + 1;
+        } else {
+          p_team->loop_lenv[i] = 0;
+        }
+      }
+    } else {
+      loop_info->lower_bound = va_arg(ap, omp_uint32);
+      loop_info->upper_bound = va_arg(ap, omp_uint32);
+      loop_info->incr = va_arg(ap, omp_int32);
+      if (loop_info->incr > 0) {
+        if (loop_info->upper_bound >= loop_info->lower_bound) {
+          p_team->loop_lenv[i] = ((omp_uint32)(loop_info->upper_bound - loop_info->lower_bound)) / ((omp_uint32)loop_info->incr) + 1;
+        } else {
+          p_team->loop_lenv[i] = 0;
+        }
+      } else {
+        if (loop_info->upper_bound <= loop_info->lower_bound) {
+          p_team->loop_lenv[i] = ((omp_uint32)(loop_info->lower_bound - loop_info->upper_bound)) / ((omp_uint32)(-loop_info->incr)) + 1;
+        } else {
+          p_team->loop_lenv[i] = 0;
+        }
+      }
+    }
+    iter_count *= p_team->loop_lenv[i];
+    loop_info++;
+  }
+  p_team->loop_lower_bound = 0;
+  p_team->loop_upper_bound = iter_count;
+  va_end(ap);
+}
+
+/*
+ * VAARGS:
+ *    for each loop level:
+ *         omp_uint32 is_64bit    // 0 - 32 bit index; 1 - 64 bit index
+ *         omp_int{32,64} lower   // lower bound of current loop
+ *         omp_int{32,64} upper   // upper bound of current loop
+ *         omp_int{32,64} stride  // stride of current loop
+ */
+void 
+__ompc_collapse_init (omp_int32 global_tid, omp_sched_t schedtype,
+                        omp_int64 chunk, omp_uint32 collapse_count, ...)
+{
+  va_list        ap;
+  omp_team_t     *p_team;
+  omp_v_thread_t *p_vthread;
+  unsigned       i;
+  omp_loop_info_t *loop_info;
+
+  va_start(ap, collapse_count);
+  __ompc_set_state(THR_OVHD_STATE);
+  /* TODO: The validity of the parameters should be checked here*/
+  if (schedtype == OMP_SCHED_RUNTIME) {
+    /* The logic is still not complete*/
+    schedtype = __omp_rt_sched_type;
+    chunk = __omp_rt_sched_size;
+		
+  } else if (schedtype == OMP_SCHED_ORDERED_RUNTIME) {
+    schedtype = __omp_rt_sched_type + OMP_SCHED_ORDERED_GAP;
+    chunk = __omp_rt_sched_size;
+  }
+
+  if (__omp_exe_mode & OMP_EXE_MODE_SEQUENTIAL) {
+    /* Need a place to hold the information*/
+    __ompc_collapsed_loop_info_init(&__omp_root_team, collapse_count, ap);
+    va_end(ap);
+    __omp_root_team.schedule_count = 0;
+    return;
+  } else if (__omp_exe_mode & OMP_EXE_MODE_NORMAL) {
+    p_team = &__omp_level_1_team_manager;
+    p_vthread = &__omp_level_1_team[global_tid];
+  } else {
+    p_vthread = __ompc_get_v_thread_by_num(global_tid);
+    p_team  = p_vthread->team;
+  }
+  p_vthread->schedule_count = 0;
+
+  if (p_team->team_size == 1) {
+    __ompc_collapsed_loop_info_init(p_team, collapse_count, ap);
+    va_end(ap);
+    p_team->schedule_count = 0;
+    if (__ompc_is_ordered(schedtype))
+      p_team->ordered_count = 0;
+
+    /* TODO: all set ready?*/
+    return;
+
+  }
+
+  /* Warning: Does threads in the same team request for different 
+   * loop scheduling and thus cause the scheduler collision?
+   */
+
+  p_vthread->loop_count++;
+
+  __ompc_lock_spinlock(&(p_team->schedule_lock));
+  if (p_team->loop_count >= p_vthread->loop_count) {
+    /* We assume that the initialization has already OK */
+    __ompc_unlock_spinlock(&(p_team->schedule_lock));
+    return;
+  } else {
+    /* The first one call schedule_init do the initialization work */
+    /* Initializing */
+    p_team->schedule_type = schedtype;
+    p_team->chunk_size = chunk;
+    p_team->schedule_count = 0;
+    if (__ompc_is_ordered(schedtype))
+      p_team->ordered_count = 0;
+    __ompc_collapsed_loop_info_init(p_team, collapse_count, ap);
+    va_end(ap);
+    /* Initialization finished */
+    p_team->loop_count++;
+    __ompc_unlock_spinlock(&(p_team->schedule_lock));
+    return;
+  }
+}         
+
+/*
+ * VAARGS:
+ *    for each loop level:
+ *         omp_int{32,64} *plower // output lower bound of the local loop
+ *         omp_int{32,64} *pupper // output upper bound of current loop
+ */
+omp_int32 __ompc_collapse_next (omp_int32 global_tid, ...)
+{
+  omp_team_t	*p_team;
+  omp_v_thread_t  *p_vthread;
+  omp_int32	team_size;
+  omp_int64	stride;
+  omp_int64	chunk;
+  omp_uint64	my_lower, my_upper, iter_count;
+  omp_int64	schedule_count;
+  int           schedule_full_loop;
+  va_list       ap;
+  unsigned      collapse_count;           
+  int           i;
+  omp_loop_info_t *loop_info;
+  omp_int64* loop_lenv;
+  omp_int32     result;
+  omp_int64     lb, ub;
+  unsigned      mark0, mark1, remainder;
+
+  __ompc_set_state(THR_OVHD_STATE);
+  schedule_full_loop = 0;
+  
+  if (__omp_exe_mode & OMP_EXE_MODE_SEQUENTIAL) {
+    /*Judge whether there are more iterations*/
+    if ( __omp_root_team.schedule_count != 0) {
+      /* No more iterations */
+      __ompc_set_state(THR_WORK_STATE);
+      return 0;
+    }
+    __omp_root_team.schedule_count = 1;
+    
+    /* Warning: Don't know how pstride should be properly set*/
+    __omp_root_v_thread.ordered_count = 0;
+    /* no need to schedule anymore iterations*/
+    __ompc_set_state(THR_WORK_STATE);
+    p_team = &__omp_root_team;
+    schedule_full_loop = 1;
+  } else {
+    if (__omp_exe_mode & OMP_EXE_MODE_NORMAL) {
+      p_vthread = &__omp_level_1_team[global_tid];
+      p_team = &__omp_level_1_team_manager;
+    } else {
+      p_vthread = __ompc_get_v_thread_by_num(global_tid);
+      p_team = p_vthread->team;
+    }
+
+    team_size = p_team->team_size;
+    if (team_size == 1) {
+      /* Single thread team running: for sequentialized nested team*/
+      /* Judge whether there are more iterations*/
+      if (p_team->schedule_count != 0) {
+        /* No more iterations*/
+        __ompc_set_state(THR_WORK_STATE);
+        return 0;
+      }
+      p_team->schedule_count = 1;
+
+      schedule_full_loop = 1;
+      p_vthread->ordered_count = 0;
+      __ompc_set_state(THR_WORK_STATE);
+    }
+  }
+
+  collapse_count = p_team->collapse_count;
+  loop_info = p_team->loop_info;
+  loop_lenv = p_team->loop_lenv;
+  iter_count = p_team->loop_upper_bound;
+
+  va_start(ap, global_tid);
+  if (schedule_full_loop) {
+    for (i = collapse_count - 1; i >= 0; i++) {
+      if (loop_info[i].is_64bit) {
+        *(va_arg(ap, omp_int64*)) = loop_info[i].lower_bound;
+        *(va_arg(ap, omp_int64*)) = (i == 0 ? (loop_info[i].lower_bound + loop_info[i].incr * (loop_lenv[i] + 1)) : (loop_info[i].lower_bound));
+      } else {
+        *(va_arg(ap, omp_int32*)) = (omp_int32) loop_info[i].lower_bound;
+        *(va_arg(ap, omp_int32*)) = (i == 0 ? ((omp_int32) loop_info[i].lower_bound + (omp_int32)loop_info[i].incr * ((omp_int32) loop_lenv[i] + 1)) : ((omp_int32) loop_info[i].lower_bound));
+      }
+    }
+    va_end(ap);
+    return 1;
+  }
+  
+  /* Normal multi-thread multi-time schedule*/
+  switch (p_team->schedule_type) {
+  case OMP_SCHED_STATIC_EVEN:
+    /* specified by OMP_SCHEDULE */
+    if (p_vthread->schedule_count != 0) {
+      /* no more iteration*/
+      __ompc_set_state(THR_WORK_STATE);
+      va_end(ap);
+      return 0;
+    }
+
+    remainder = iter_count % team_size;
+    mark0 = (remainder * global_tid) / team_size;
+    mark1 = (remainder * (global_tid + 1)) / team_size;
+    my_lower = iter_count / team_size * global_tid + mark0;
+    my_upper = iter_count / team_size * (global_tid + 1) + mark1;
+    p_vthread->schedule_count = 1;
+    /* Need to set plastiter right*/
+    result = (my_lower != my_upper);
+    break;
+  case OMP_SCHED_STATIC:
+    chunk = p_team->chunk_size;
+    /* Determine whether there are more iterations*/
+    
+    my_lower = p_vthread->schedule_count * chunk * team_size 
+      + global_tid * chunk;
+
+    if ( my_lower >= iter_count ) {
+      /* No more iterations*/
+      result = 0;
+    } else {
+      p_vthread->schedule_count ++;
+      my_lower = my_lower;
+      my_upper = my_lower + chunk;
+      if (my_upper > iter_count) my_upper = iter_count;
+      result = 1;
+    }
+    break;
+
+  case OMP_SCHED_DYNAMIC:
+    __ompc_lock_spinlock(&(p_team->schedule_lock));
+
+    my_lower = p_team->loop_lower_bound;
+    if ( my_lower >= iter_count) {
+      __ompc_unlock_spinlock(&(p_team->schedule_lock));
+      /* No more iterations */
+      __ompc_set_state(THR_WORK_STATE);
+      result = 0;
+      break;
+    }
+    my_upper = my_lower + p_team->chunk_size;
+    if (my_upper > iter_count) my_upper = iter_count;
+    p_team->loop_lower_bound = my_upper;
+    __ompc_unlock_spinlock(&(p_team->schedule_lock));
+    result = 1;
+    break;
+
+  case OMP_SCHED_GUIDED:
+    __ompc_lock_spinlock(&(p_team->schedule_lock));
+
+    my_lower = p_team->loop_lower_bound;
+    if ( my_lower >= iter_count) {
+      __ompc_unlock_spinlock(&(p_team->schedule_lock));
+      /* No more iterations */
+      __ompc_set_state(THR_WORK_STATE);
+      result = 0;
+      break;
+    }
+    chunk = (p_team->loop_upper_bound - my_lower) / (2 * team_size);
+    if (chunk < p_team->chunk_size)
+      chunk = p_team->chunk_size;
+    my_upper = my_lower + chunk;
+    if (my_upper > iter_count) my_upper = iter_count;
+    p_team->loop_lower_bound = my_upper;
+    __ompc_unlock_spinlock(&(p_team->schedule_lock));
+    result = 1;
+    break;
+
+  case OMP_SCHED_ORDERED_STATIC_EVEN:
+    /* specified by OMP_SCHEDULE */
+    if (p_vthread->schedule_count != 0) {
+      /* no more iteration*/
+      __ompc_set_state(THR_WORK_STATE);
+      va_end(ap);
+      return 0;
+    }
+
+    remainder = iter_count % team_size;
+    mark0 = (remainder * global_tid) / team_size;
+    mark1 = (remainder * (global_tid + 1)) / team_size;
+    my_lower = iter_count / team_size * global_tid + mark0;
+    my_upper = iter_count / team_size * (global_tid + 1) + mark1;
+
+    p_vthread->schedule_count = 1;
+    p_vthread->ordered_count = global_tid;
+    p_vthread->rest_iter_count = my_upper - my_lower;
+
+    /* Need to set plastiter right*/
+    result = (my_lower != my_upper);
+    break;
+
+  case OMP_SCHED_ORDERED_STATIC:
+    chunk = p_team->chunk_size;
+    /* Determine whether there are more iterations*/
+    
+    my_lower = p_vthread->schedule_count * chunk * team_size 
+      + global_tid * chunk;
+
+    if ( my_lower >= iter_count ) {
+      /* No more iterations*/
+      result = 0;
+    } else {
+      my_upper = my_lower + chunk;
+      if (my_upper > iter_count) my_upper = iter_count;
+      p_vthread->ordered_count = p_vthread->schedule_count * team_size 
+        + global_tid;
+      p_vthread->schedule_count++;
+      p_vthread->rest_iter_count = my_upper - my_lower;
+      result = 1;
+    }
+    break;
+
+  case OMP_SCHED_ORDERED_DYNAMIC:
+    __ompc_lock_spinlock(&(p_team->schedule_lock));
+
+    my_lower = p_team->loop_lower_bound;
+    schedule_count = p_team->schedule_count;
+    if ( my_lower >= iter_count) {
+      __ompc_unlock_spinlock(&(p_team->schedule_lock));
+      /* No more iterations */
+      __ompc_set_state(THR_WORK_STATE);
+      result = 0;
+      break;
+    }
+    my_upper = my_lower + p_team->chunk_size;
+    if (my_upper > iter_count) my_upper = iter_count;
+    p_team->loop_lower_bound = my_upper;
+    p_team->schedule_count++;
+    __ompc_unlock_spinlock(&(p_team->schedule_lock));
+    p_vthread->ordered_count = schedule_count;
+    p_vthread->rest_iter_count = my_upper - my_lower;
+    result = 1;
+    break;
+
+  case OMP_SCHED_ORDERED_GUIDED:
+    __ompc_lock_spinlock(&(p_team->schedule_lock));
+
+    my_lower = p_team->loop_lower_bound;
+    schedule_count = p_team->schedule_count;
+    if ( my_lower >= iter_count) {
+      __ompc_unlock_spinlock(&(p_team->schedule_lock));
+      /* No more iterations */
+      __ompc_set_state(THR_WORK_STATE);
+      result = 0;
+      break;
+    }
+    chunk = (p_team->loop_upper_bound - my_lower) / (2 * team_size);
+    if (chunk < p_team->chunk_size)
+      chunk = p_team->chunk_size;
+    my_upper = my_lower + chunk;
+    if (my_upper > iter_count) my_upper = iter_count;
+    p_team->loop_lower_bound = my_upper;
+    p_team->schedule_count++;
+    __ompc_unlock_spinlock(&(p_team->schedule_lock));
+    p_vthread->ordered_count = schedule_count;
+    p_vthread->rest_iter_count = my_upper - my_lower;
+    result = 1;
+    break;
+
+  default:
+    /* runtime schedule type should have been resolved yet*/
+    Not_Valid(" unknown schedule type specified");
+  }
+  if (result) {
+    for (i = collapse_count - 1; i >= 0; i--) {
+      lb = loop_info[i].lower_bound + loop_info[i].incr * (omp_int64)(i == 0 ? my_lower : my_lower % loop_lenv[i]);
+      my_lower /= loop_lenv[i];
+      ub = loop_info[i].lower_bound + loop_info[i].incr * (omp_int64)(i == 0 ? my_upper : my_upper % loop_lenv[i]);
+      my_upper /= loop_lenv[i];
+      if (loop_info[i].is_64bit) {
+        *(va_arg(ap, omp_int64*)) = lb;
+        *(va_arg(ap, omp_int64*)) = ub;
+      } else {
+        *(va_arg(ap, omp_int32*)) = lb;
+        *(va_arg(ap, omp_int32*)) = ub;
+      }
+    }
+  }
+  va_end(ap);
+  __ompc_set_state(THR_WORK_STATE);
+  return result;
+}
+
 /* Must be called after the schedule*/
 void
 __ompc_scheduler_fini(omp_int32 global_tid)
@@ -1234,6 +1677,7 @@
     return;
   
   __ompc_event_callback(OMP_EVENT_THR_BEGIN_ORDERED);
+  __ompc_event_callback(OMP_EVENT_THR_BEGIN_ORDERED);
   p_vthread = __ompc_get_v_thread_by_num(global_tid);
   p_team = p_vthread->team;
 
@@ -1242,7 +1686,7 @@
 
   pthread_mutex_lock(&(p_team->ordered_mutex));
   while (p_team->ordered_count != p_vthread->ordered_count) {
-    omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
+    omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num(__omp_myid);
     p_vthread->thr_odwt_state_id++;
     __ompc_set_state(THR_ODWT_STATE);
     __ompc_event_callback(OMP_EVENT_THR_BEGIN_ODWT);
@@ -1327,6 +1771,7 @@
     is_first = 1;
   }
   __ompc_unlock(&(p_team->single_lock));
+  if (is_first) __ompc_set_state(THR_WORK_STATE);
 
   if (is_first) __ompc_set_state(THR_WORK_STATE);
 
@@ -1340,7 +1785,6 @@
   __ompc_event_callback(OMP_EVENT_THR_END_SINGLE);
   __ompc_set_state(THR_WORK_STATE);
   /* The single flags should be reset here*/
-  /* Do nothing*/
 }
 
 
@@ -1406,7 +1850,7 @@
     }
     pp[global_tid] = p;
   }
-//  printf("ompc_get_thdprv: pp[%d] = %d\n", global_tid, *((int *)p));
+
   return 1;
 }
 
Index: libopenmp/omp_sys.h
===================================================================
--- libopenmp/omp_sys.h	(リビジョン 3578)
+++ libopenmp/omp_sys.h	(リビジョン 3580)
@@ -38,69 +38,11 @@
 #ifndef __omp_sys_included
 #define __omp_sys_included
 
-/*
- * We use the pthread_spin_lock, rather this inlined-assembly version of spin lock for two reasons
- * (1) portblity: one version for all platform.
- * (2) the code generated by open64 compiler for this inlined-assembly version is not efficient.
- */     
+#if defined(TARG_X8664) || defined(TARG_IA32) || defined(TARG_LOONGSON)
 
-static inline void
-__ompc_spin_init(volatile int* lock)
-{
-  *lock = 0;
-}
-
-static inline void
-__ompc_spin_destroy(volatile int* lock)
-{
- // nothing need to be done
- ;
-}
-
-#if defined(TARG_X8664) || defined(TARG_IA32)
-
-static inline void 
-__ompc_spin_lock(volatile int* lock)
-{
-  int result = 1;
-  do {
-    __asm__ __volatile__ ("xchgl %0,(%1)":"=r"(result):"r"(lock),"0"(result));
-  }while(result);
-}
-
-
-static inline void
-__ompc_spin_unlock(volatile int* lock)
-{
-  *lock = 0;
-}
-
-static inline int
-__ompc_spin_trylock(volatile int* lock)
-{
-  int result = 1;
-  __asm__ __volatile__ ("xchgl %0,(%1)":"=r"(result):"r"(lock),"0"(result));
-  return (result==0);
-}
-
 //TODO: should use __sync_add_and_fetch(), but it's
 // not yet implemented in open64.
 
-static inline unsigned long
-__ompc_atomic_inc_unsigned_long(volatile unsigned long*value)
-{
-#ifdef OMP_USE_GCC_ATOMIC_BUILTINS
-  return __sync_add_and_fetch(value, 1);
-#else
-  unsigned long  result;
-  static int lock = 0;
-  __ompc_spin_lock(&lock);
-  result = ++(*value);
-  __ompc_spin_unlock(&lock);
-  return result;
-#endif
-}
-
 static inline int
 __ompc_atomic_inc(volatile int* value)
 {
@@ -122,7 +64,8 @@
 #endif
 }
 
-#  else
+#else
+
 /* under observation not sure if next function works */
 static inline unsigned long
 __ompc_atomic_inc_unsigned_long(volatile unsigned long*value)
Index: libopenmp/omp_queue.c
===================================================================
--- libopenmp/omp_queue.c	(リビジョン 3578)
+++ libopenmp/omp_queue.c	(リビジョン 3580)
@@ -30,7 +30,6 @@
   if(tq->tail == NULL)
     printf("tail = NULL\n");
 
-
   tq->size = 0;
   tq->head->next = tq->tail;
   tq->tail->prev = tq->head;
@@ -41,7 +40,6 @@
 void __ompc_task_q_get_head(omp_task_q_t *tq, omp_task_t **task)
 {
   __ompc_lock(&tq->lock);
-
   if(tq->head->next == tq->tail) {
     *task = NULL;
   } else {
@@ -86,27 +84,26 @@
 
 void __ompc_task_q_put_tail(omp_task_q_t *tq, omp_task_t *task)
 {
-    omp_task_t *temp;
+  omp_task_t *temp;
 
-    __ompc_lock(&tq->lock);
+  __ompc_lock(&tq->lock);
 #ifdef TASK_DEBUG
-    printf("%d: putting %X on quueue %lu\n", __omp_myid, task, tq);
-    printf("%d: tail = %X; tail->prev = %X;\n", __omp_myid, tq->tail, tq->tail->prev);
+  printf("%d: putting %X on quueue %lu\n", __omp_myid, task, tq);
+  printf("%d: tail = %X; tail->prev = %X;\n", __omp_myid, tq->tail, tq->tail->prev);
 #endif
-    temp = tq->tail->prev;
-    tq->tail->prev = task;
-    task->next = tq->tail;
-    task->prev = temp;
-    temp->next = task;
+  temp = tq->tail->prev;
+  tq->tail->prev = task;
+  task->next = tq->tail;
+  task->prev = temp;
+  temp->next = task;
 #ifdef TASK_DEBUG
-    printf("%d: task->prev = %X; task->next = %X\n", __omp_myid, task->prev, task->next);
+  printf("%d: task->prev = %X; task->next = %X\n", __omp_myid, task->prev, task->next);
 #endif
-    tq->size = tq->size + 1;
-    __ompc_unlock(&tq->lock);
+  tq->size = tq->size + 1;
+  __ompc_unlock(&tq->lock);
 }
 
 
-
 /* old task queue implementation */
 
 /*
Index: libopenmp/omp_type.h
===================================================================
--- libopenmp/omp_type.h	(リビジョン 3578)
+++ libopenmp/omp_type.h	(リビジョン 3580)
@@ -39,7 +39,9 @@
  */
 
 typedef int 	  omp_int32;
+typedef unsigned  omp_uint32;
 typedef long long omp_int64;
+typedef unsigned long long omp_uint64;
 typedef float	  omp_real32;
 typedef double	  omp_real64;
 typedef int	  omp_bool;
Index: libopenmp/omp_lock.c
===================================================================
--- libopenmp/omp_lock.c	(リビジョン 3578)
+++ libopenmp/omp_lock.c	(リビジョン 3580)
@@ -44,7 +44,7 @@
 #include "omp_sys.h"
 
 extern int __omp_spin_user_lock;
-omp_int_t omp_get_thread_num(void);
+
 inline void 
 __ompc_init_lock (volatile ompc_lock_t *lp)
 {
@@ -72,8 +72,8 @@
       __ompc_event_callback(OMP_EVENT_THR_BEGIN_LKWT);
       pthread_mutex_lock((pthread_mutex_t *)lp);
       __ompc_event_callback(OMP_EVENT_THR_END_LKWT);
-  }
-   __ompc_set_state(THR_WORK_STATE);
+    }
+  __ompc_set_state(THR_WORK_STATE);
 }
 
 
@@ -283,9 +283,9 @@
 __ompc_critical(int gtid, volatile ompc_lock_t **lck)
 {
   __ompc_set_state(THR_OVHD_STATE);
-  if (*lck ==NULL) {
+  if (*lck == NULL) {
     __ompc_lock_spinlock(&_ompc_thread_lock);
-    if ((ompc_lock_t*)*lck == NULL){
+    if ((ompc_lock_t*)*lck == NULL) {
       // put the shared data aligned with cache line
       volatile ompc_lock_t* new_lock = 
         aligned_malloc(sizeof(ompc_lock_t), CACHE_LINE_SIZE);
@@ -297,28 +297,28 @@
     __ompc_unlock_spinlock(&_ompc_thread_lock);
   }
 
-  if (!__ompc_test_lock(*lck)) {
-      omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
-      p_vthread->thr_ctwt_state_id++;
-      __ompc_set_state(THR_CTWT_STATE);
-      __ompc_event_callback(OMP_EVENT_THR_BEGIN_CTWT);
-      __ompc_lock((volatile ompc_lock_t *)*lck);
-       __ompc_event_callback(OMP_EVENT_THR_END_CTWT);
+  if(!__ompc_test_lock(*lck)) {
+    omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
+    p_vthread->thr_ctwt_state_id++;
+    __ompc_set_state(THR_CTWT_STATE);
+    __ompc_event_callback(OMP_EVENT_THR_BEGIN_CTWT);
+    __ompc_lock(*lck);
+    __ompc_event_callback(OMP_EVENT_THR_END_CTWT);
   }
-   __ompc_set_state(THR_WORK_STATE);
+  __ompc_set_state(THR_WORK_STATE);
 }
 
 inline void
 __ompc_end_critical(int gtid, volatile ompc_lock_t **lck)
 {
-  __ompc_unlock((volatile ompc_lock_t *)*lck);
+  __ompc_unlock(*lck);
   __ompc_set_state(THR_WORK_STATE);
 }
 
 inline void
 __ompc_reduction(int gtid, volatile ompc_lock_t **lck)
 {
-    __ompc_set_state(THR_OVHD_STATE);
+  __ompc_set_state(THR_OVHD_STATE);
   if (*lck ==NULL) {
     __ompc_lock_spinlock(&_ompc_thread_lock);
     if ((ompc_lock_t*)*lck == NULL){
@@ -339,6 +339,6 @@
 inline void
 __ompc_end_reduction(int gtid, volatile ompc_lock_t **lck)
 {
-  __ompc_unlock((volatile ompc_lock_t *)*lck);
+  __ompc_unlock(*lck);
   __ompc_set_state(THR_WORK_STATE);
 }
Index: libopenmp/omp_lock.h
===================================================================
--- libopenmp/omp_lock.h	(リビジョン 3578)
+++ libopenmp/omp_lock.h	(リビジョン 3580)
@@ -67,6 +67,7 @@
 
 #endif
 
+
 static inline void
 __ompc_init_spinlock(ompc_spinlock_t *lck_p)
 {
Index: libopenmp/omp_rtl_api.h
===================================================================
--- libopenmp/omp_rtl_api.h	(リビジョン 3578)
+++ libopenmp/omp_rtl_api.h	(リビジョン 3580)
@@ -37,7 +37,7 @@
  * include it in the RTL implementation.
  */
 
-#define frame_pointer_t void*
+typedef void *frame_pointer_t;
 
 typedef int omp_int32;
 typedef long long omp_int64;
@@ -48,7 +48,7 @@
 
 typedef void (*omp_micro)(omp_int32 , frame_pointer_t);
 typedef void (*omp_task_func)(void *args);
-typedef void (*omp_cond_func)();
+typedef int (*omp_cond_func)();
 
 typedef enum {
   OMP_SCHED_UNKNOWN             = 0,
Index: libopenmp/omp_collector_validation.c
===================================================================
--- libopenmp/omp_collector_validation.c	(リビジョン 3578)
+++ libopenmp/omp_collector_validation.c	(リビジョン 3580)
@@ -6,14 +6,11 @@
 
 const int OMP_COLLECTORAPI_HEADERSIZE=4*sizeof(int);
 
-/*This routine is to test the OMP_REQ_START request to the collector */
+/* This routine is to test the OMP_REQ_START request to the collector */
 
 void dummyfunc(OMP_COLLECTORAPI_EVENT event)
 {
   printf("\n** EVENT:%s **",OMP_EVENT_NAME[event-1]);
-  // omp_v_thread_t *p_vthread =  __ompc_get_current_v_thread();
-  //printf("Thread %d EVENT=%s STATE=%s\n",p_vthread->vthread_id,OMP_EVENT_NAME[event-1], OMP_STATE_NAME[p_vthread->state-1]);  
-
 }
 
 void fill_header(void *message, int sz, OMP_COLLECTORAPI_REQUEST rq, OMP_COLLECTORAPI_EC ec, int rsz, int append_zero)
Index: libopenmp/Makefile.gbase
===================================================================
--- libopenmp/Makefile.gbase	(リビジョン 3578)
+++ libopenmp/Makefile.gbase	(リビジョン 3580)
@@ -76,11 +76,12 @@
 	omp_util.c \
 	omp_queue.c \
 	omp_task.c \
-	pcl.c  \
-	omp_collector_util.c
+	pcl.c \
+	omp_collector_util.c \
 
+
 CXXFILES = \
-	omp_init.cxx
+	omp_init.cxx \
 
 ifeq ($(BUILD_COMPILER), GNU)
 NO_UNWIND=-fno-unwind-tables
@@ -90,7 +91,7 @@
 
 # Adding -fno-exceptions to avoid EH related stuffs, 
 #  which may cause undefined symbols in C program
-CXXFLAGS += -fno-exceptions -nostdinc++ $(NO_UNWIND)
+CXXFLAGS += -fno-exceptions $(NO_UNWIND)
 
 default:
 	$(MAKE)  first
Index: libopenmp/omp_task.c
===================================================================
--- libopenmp/omp_task.c	(リビジョン 3578)
+++ libopenmp/omp_task.c	(リビジョン 3580)
@@ -52,61 +52,73 @@
 }
 
 
-int __ompc_task_create(omp_task_func taskfunc, void* fp, int is_tied)
+int __ompc_task_create(omp_task_func taskfunc, void* fp, int may_delay, int is_tied)
 {
 
   __omp_tasks_created++;
   omp_task_t *newtask = __ompc_task_get(taskfunc, fp, __omp_task_stack_size);
 
 #if defined(TASK_DEBUG)
-    fprintf(stdout,"%d: %lX created %lX\n", __omp_myid, __omp_current_task, newtask);
+  fprintf(stdout,"%d: %lX created %lX\n", __omp_myid, __omp_current_task, newtask);
 #endif
 
-    if(newtask == NULL) {
-      fprintf(stderr, "%d: not able to create new tasks\n", __omp_myid);
-      exit(1);
-    }
-    newtask->num_children = 0;
-    newtask->is_parallel_task = 0;
-    newtask->is_tied = is_tied;
-    newtask->started = 0;
-    newtask->creator = __omp_current_task;
-    newtask->safe_to_enqueue = 0;
-    newtask->depth = __omp_current_task->depth + 1;
-    newtask->pdepth = newtask->depth;
-    __ompc_init_lock(&__omp_current_task->lock);
+  if(newtask == NULL) {
+    fprintf(stderr, "%d: not able to create new tasks\n", __omp_myid);
+    exit(1);
+  }
+  newtask->num_children = 0;
+  newtask->is_parallel_task = 0;
+  newtask->is_tied = is_tied;
+  newtask->started = 0;
+  newtask->creator = __omp_current_task;
+  newtask->safe_to_enqueue = 0;
+  newtask->depth = __omp_current_task->depth + 1;
+  newtask->pdepth = newtask->depth;
+  pthread_mutex_init(&newtask->lock, NULL);
 
-    /*update number of children - use atomic operation if possible */
-    int x;
-    x = __ompc_atomic_inc(&__omp_current_task->num_children);
-    __ompc_atomic_inc(&__omp_level_1_team_manager.num_tasks);
+  /* update number of children - use atomic operation if possible */
+  int x;
+  x = __ompc_atomic_inc(&__omp_current_task->num_children);
+  __ompc_atomic_inc(&__omp_level_1_team_manager.num_tasks);
 
+  __ompc_task_switch(__omp_current_task, newtask);
+
+  if (may_delay) {
 #ifdef SCHED1
     newtask->threadid = friend;
     __sync_bool_compare_and_swap(&__omp_empty_flags[friend], 1, 0);
     __ompc_task_q_put_tail(&__omp_local_task_q[friend], newtask);
 #else
-	__ompc_task_q_put_tail(&__omp_local_task_q[__omp_myid], newtask);
+    __ompc_task_q_put_tail(&__omp_local_task_q[__omp_myid], newtask);
 #endif
+  } else {
+    __ompc_task_switch(__omp_current_task, newtask);
+  }
 
-    return 0;
+  return 0;
 }
 
+void __ompc_task_body_start()
+{
+  __ompc_task_switch(__omp_current_task, __omp_current_task->creator);
+}
 
+
 void __ompc_task_exit()
 {
 
   omp_task_t *next;
   next = NULL;
-  /*decrement num_children of parent*/
+  /* decrement num_children of parent*/
   if(__omp_current_task->creator != NULL) {
 
 #ifdef TASK_DEBUG
     printf("%X: %X->num_children = %d\n", __omp_current_task, __omp_current_task->creator,__omp_current_task->creator->num_children);
 #endif 
-    __ompc_lock(&__omp_current_task->creator->lock);
+    pthread_mutex_lock(&__omp_current_task->creator->lock);
 
     int x;
+
     x = __ompc_atomic_dec(&__omp_current_task->creator->num_children);
 
 #ifdef TASK_DEBUG
@@ -117,7 +129,7 @@
     if((x) == 0 && !__omp_current_task->creator->is_parallel_task ) {
       if(__omp_current_task->creator->state == OMP_TASK_SUSPENDED) {
         __omp_current_task->creator->state = OMP_TASK_DEFAULT;
-        while(!__omp_current_task->creator->safe_to_enqueue) { };
+        while(!__omp_current_task->creator->safe_to_enqueue){};
 
 #ifdef TASK_DEBUG
         printf("%d: task_exit: %X placing %X on queue ", __omp_myid, __omp_current_task, __omp_current_task->creator);
@@ -130,7 +142,7 @@
 
 #else
         if(__omp_current_task->creator->is_tied &&
-            __omp_current_task->creator->started) {
+           __omp_current_task->creator->started) {
           threadid = __omp_current_task->creator->threadid;
           q = __omp_private_task_q;
         } else {
@@ -138,28 +150,26 @@
           q = __omp_local_task_q;
         }
 #endif
-        __ompc_task_q_put_tail(
-            &q[threadid],
-            __omp_current_task->creator);
+        __ompc_task_q_put_tail(&q[threadid], __omp_current_task->creator);
 
       }
-      else if(__omp_current_task->creator->state == OMP_TASK_EXIT) {
-        //	       __ompc_task_delete(__omp_current_task->creator);
-      }
 #ifdef TASK_DEBUG
       else
         printf("%d: taskexit: %X state = %d\n", __omp_myid, __omp_current_task->creator, __omp_current_task->creator->state);
 #endif
 
     }
-    __ompc_unlock(&__omp_current_task->creator->lock);
+    pthread_mutex_unlock(&__omp_current_task->creator->lock);
   }
 
   __ompc_atomic_dec(&__omp_level_1_team_manager.num_tasks);
   __omp_current_task->state = OMP_TASK_EXIT;
 
-  /*before we delete anything we need to wait for all children to complete */
+
+  /* before we delete anything we need to wait for all children to complete */
+
   __ompc_task_wait2(OMP_TASK_EXIT);
+  
 }
 
 void __ompc_task_wait()
@@ -169,154 +179,110 @@
 
 void __ompc_task_wait2(omp_task_state_t state)
 {
-    /* tasks calling this function are not in a ready queue, set state to blocked and find another task to execute */
+  /* tasks calling this function are not in a ready queue, set state to blocked and find another task to execute */
 
-    /*if task still has outstanding children, it must wait until its num_children value is zero, when this happens,
-      the last child to complete will either a.) add it to the queue if state == OMP_TASK_SUSPENDED or
-      b.) reclaim/free the parent v_thread for later use
-    */
+  /* if task still has outstanding children, it must wait until its num_children value is zero, when this happens,
+     the last child to complete will either a) add it to the queue if state == OMP_TASK_SUSPENDED or b) reclaim/free the parent v_thread for later use
+  */
 
   assert(state == OMP_TASK_SUSPENDED || state == OMP_TASK_EXIT);
 
-    omp_task_t *next;
-    omp_task_t *old, *new;
+  omp_task_t *next;
+  omp_task_t *old, *new;
+  omp_task_t *current_task = __omp_current_task;
 
 
 #ifdef TASK_DEBUG
-    fprintf(stdout,"%d: taskwait: %X num_children = %d; state = %d\n", __omp_myid, __omp_current_task, __omp_current_task->num_children, state);
+  fprintf(stdout,"%d: taskwait: %X num_children = %d; state = %d\n", __omp_myid, __omp_current_task, __omp_current_task->num_children, state);
 #endif
 
 
-        while(1)
-	{
-	  __ompc_lock(&__omp_current_task->lock);
+  while(1) {
+    pthread_mutex_lock(&current_task->lock);
 
-	  if(__omp_current_task->num_children == 0)
-	    {
-	      /*all children have completed */
-	      /*if state == OMP_TASK_EXIT, we need to delete ourselves and schedule 
-		the next task
-	      */
+    if(current_task->num_children == 0) {
+      /* all children have completed */
+      /* if state == OMP_TASK_EXIT, we need to delete ourselves and schedule 
+         the next task
+      */
 #ifdef TASK_DEBUG
-	      printf("%d: taskwait2: %X num_children = 0; state = %d\n", __omp_myid, __omp_current_task, state);
+      printf("%d: taskwait2: %X num_children = 0; state = %d\n", __omp_myid, current_task, state);
 #endif
 
-
-	      {
-		if(state == OMP_TASK_EXIT)
-		  {
+      if(state == OMP_TASK_EXIT) {
 		    
-		    __ompc_task_schedule(&next);
+        __ompc_task_schedule(&next);
 		    
-		    __omp_current_task->state = OMP_TASK_EXIT;
-		    if(next == NULL)
-		       next = __omp_level_1_team_tasks[__omp_myid];
+        current_task->state = OMP_TASK_EXIT;
+        if(next == NULL)
+          next = __omp_level_1_team_tasks[__omp_myid];
 
-		    old = NULL;
+        old = NULL;
 
-		    __ompc_unlock(&__omp_current_task->lock);
-		    __ompc_task_switch(old, next);
-		    /*this causes seg fault, need to look into it more
-		      without deleting the task memory leaks occur */
-		    
-		    //		    co_exit_to(next);
+        pthread_mutex_unlock(&current_task->lock);
+        __ompc_task_exit_to(current_task, next);
+      } else if(state == OMP_TASK_SUSPENDED) {
+        pthread_mutex_unlock(&current_task->lock);
+        return;
+      }
 
-		  }
-		else if(state == OMP_TASK_SUSPENDED)
-		  {
-		    __ompc_unlock(&__omp_current_task->lock);
-		    return;
-		  }
+    } else {
 
-	      }
-	    }
-	  else
-	    {
-
 #ifdef TASK_DEBUG
-	      printf("%d: taskwait: %X num_children = %d\n", __omp_myid, __omp_current_task, __omp_current_task->num_children);
+      printf("%d: taskwait: %X num_children = %d\n", __omp_myid, current_task, current_task->num_children);
 #endif
-	      __omp_current_task->state = state;
-	      __ompc_task_schedule(&next);
+      current_task->state = state;
+      __ompc_task_schedule(&next);
 	
-	      if(next != NULL || !__omp_current_task->is_parallel_task)
-	      {
-		if(next == NULL)
-		  next = __omp_level_1_team_tasks[__omp_myid];
+      if(next != NULL || !current_task->is_parallel_task) {
+        if(next == NULL)
+          next = __omp_level_1_team_tasks[__omp_myid];
 
-		if(state == OMP_TASK_EXIT)
-		  {
-		    old = NULL;
-		  }
-		else if(state == OMP_TASK_SUSPENDED)
-		  {
-		    old = __omp_current_task;
-		  }
-   		__ompc_unlock(&__omp_current_task->lock);
-		__ompc_task_switch(old, next);
+        if(state == OMP_TASK_EXIT) {
+          old = NULL;
+        } else if(state == OMP_TASK_SUSPENDED) {
+          old = current_task;
+        }
+        pthread_mutex_unlock(&current_task->lock);
+        __ompc_task_switch(old, next);
 
-	      }
-	
-	    }
-	  __ompc_unlock(&__omp_current_task->lock);
+      } else
+        pthread_mutex_unlock(&current_task->lock);
 
+    }
 
-	}
+  }
 
 }
 
 void __ompc_task_schedule(omp_task_t **next)
 {
-      __ompc_task_q_get_tail(&__omp_private_task_q[__omp_myid], next);
+  __ompc_task_q_get_tail(&__omp_private_task_q[__omp_myid], next);
      
-      if(*next == NULL) {
-	__ompc_task_q_get_tail(&__omp_local_task_q[__omp_myid], next);
-      }
+  if(*next == NULL) {
+    __ompc_task_q_get_tail(&__omp_local_task_q[__omp_myid], next);
+  }
 
-
-
 #ifdef SCHED1
-  if(*next == NULL)
-    {
-      /*
-      struct timespec ts;
-      ts.tv_nsec = 2;
-      ts.tv_sec = 0;
-
-     nanosleep(&ts, NULL);
-      __ompc_task_q_get_tail(&__omp_local_task_q[__omp_myid], next);
-
-      if(*next == NULL)
-      */
-	__sync_bool_compare_and_swap(&__omp_empty_flags[__omp_myid], 0, 1);
-    }
+  if(*next == NULL) {
+    __sync_bool_compare_and_swap(&__omp_empty_flags[__omp_myid], 0, 1);
+  }
 #else
-  if(*next == NULL)
-    {
+  if(*next == NULL) {
 
-	  int victim = (rand_r(&__omp_seed) % __omp_level_1_team_size);
+    int victim = (rand_r(&__omp_seed) % __omp_level_1_team_size);
 	  
-	  if(__omp_myid != victim) {
-	    __ompc_task_q_get_head(&__omp_local_task_q[victim], next);
+    if(__omp_myid != victim) {
+      __ompc_task_q_get_head(&__omp_local_task_q[victim], next);
 
-	    if(*next != NULL)
-	      __omp_tasks_stolen++;
-	  }
-
+      if(*next != NULL)
+        __omp_tasks_stolen++;
     }
+
+  }
 #endif
 }
 
 inline void __ompc_enqueue_parent()
 {
-#if 0
-    if(!__omp_current_task->creator->is_parallel_task)
-	__ompc_task_q_put_tail(&__omp_local_task_q[__omp_myid], __omp_current_task->creator);
-#endif
 }
-
-void __ompc_cody_atomic_inc(volatile int *x)
-{
-  __ompc_atomic_inc(x);
-  //  __sync_fetch_and_add(x,1);
-}
Index: libopenmp/pcl.c
===================================================================
--- libopenmp/pcl.c	(リビジョン 3578)
+++ libopenmp/pcl.c	(リビジョン 3580)
@@ -24,6 +24,8 @@
 #include <stdlib.h>
 #include "pcl.h"
 
+
+
 #if defined(CO_USE_SIGCONTEXT)
 #include <signal.h>
 #endif
@@ -76,17 +78,17 @@
 
 
 static int co_ctx_sdir(unsigned long psp) {
-	int nav = 0;
-	unsigned long csp = (unsigned long) &nav;
+  int nav = 0;
+  unsigned long csp = (unsigned long) &nav;
 
-	return psp > csp ? -1: +1;
+  return psp > csp ? -1: +1;
 }
 
 
 static int co_ctx_stackdir(void) {
-	int cav = 0;
+  int cav = 0;
 
-	return co_ctx_sdir((unsigned long) &cav);
+  return co_ctx_sdir((unsigned long) &cav);
 }
 
 
@@ -94,29 +96,29 @@
 
 static int co_set_context(co_ctx_t *ctx, void *func, char *stkbase, long stksiz) {
 
-	if (getcontext(&ctx->cc))
-		return -1;
+  if (getcontext(&ctx->cc))
+    return -1;
  
-	ctx->cc.uc_link = NULL;
+  ctx->cc.uc_link = NULL;
  
-	ctx->cc.uc_stack.ss_sp = stkbase;
-	ctx->cc.uc_stack.ss_size = stksiz - sizeof(long);
-	ctx->cc.uc_stack.ss_flags = 0;
+  ctx->cc.uc_stack.ss_sp = stkbase;
+  ctx->cc.uc_stack.ss_size = stksiz - sizeof(long);
+  ctx->cc.uc_stack.ss_flags = 0;
  
-	makecontext(&ctx->cc, func, 1);
+  makecontext(&ctx->cc, func, 1);
 
-	return 0;
+  return 0;
 }
 
 
 static void co_switch_context(co_ctx_t *octx, co_ctx_t *nctx) {
 
 
-	if (swapcontext(&octx->cc, &nctx->cc) < 0) {
-		fprintf(stderr, "[PCL] Context switch failed: curr=%p\n",
-			co_curr);
-		exit(1);
-	}
+  if (swapcontext(&octx->cc, &nctx->cc) < 0) {
+    fprintf(stderr, "[PCL] Context switch failed: curr=%p\n",
+            co_curr);
+    exit(1);
+  }
 
 }
 
@@ -141,219 +143,219 @@
  */
 #error POINT1
 static void co_ctx_bootstrap(void) {
-	co_ctx_t * volatile ctx_starting;
-	void (* volatile ctx_starting_func)(void);
+  co_ctx_t * volatile ctx_starting;
+  void (* volatile ctx_starting_func)(void);
  
-	/*
-	 * Switch to the final signal mask (inherited from parent)
-	 */
-	sigprocmask(SIG_SETMASK, &ctx_creating_sigs, NULL);
+  /*
+   * Switch to the final signal mask (inherited from parent)
+   */
+  sigprocmask(SIG_SETMASK, &ctx_creating_sigs, NULL);
  
-	/*
-	 * Move startup details from static storage to local auto
-	 * variables which is necessary because it has to survive in
-	 * a local context until the thread is scheduled for real.
-	 */
-	ctx_starting = ctx_creating;
-	ctx_starting_func = (void (*)(void)) ctx_creating_func;
+  /*
+   * Move startup details from static storage to local auto
+   * variables which is necessary because it has to survive in
+   * a local context until the thread is scheduled for real.
+   */
+  ctx_starting = ctx_creating;
+  ctx_starting_func = (void (*)(void)) ctx_creating_func;
  
-	/*
-	 * Save current machine state (on new stack) and
-	 * go back to caller until we're scheduled for real...
-	 */
-	if (!setjmp(ctx_starting->cc))
-		longjmp(ctx_caller.cc, 1);
+  /*
+   * Save current machine state (on new stack) and
+   * go back to caller until we're scheduled for real...
+   */
+  if (!setjmp(ctx_starting->cc))
+    longjmp(ctx_caller.cc, 1);
 
-	/*
-	 * The new thread is now running: GREAT!
-	 * Now we just invoke its init function....
-	 */
-	ctx_starting_func();
+  /*
+   * The new thread is now running: GREAT!
+   * Now we just invoke its init function....
+   */
+  ctx_starting_func();
 
-	fprintf(stderr, "[PCL] Hmm, you really shouldn't reach this point: curr=%p\n",
-		co_curr);
-	exit(1);
+  fprintf(stderr, "[PCL] Hmm, you really shouldn't reach this point: curr=%p\n",
+          co_curr);
+  exit(1);
 }
 
 
 static void co_ctx_trampoline(int sig) {
-	/*
-	 * Save current machine state and _immediately_ go back with
-	 * a standard "return" (to stop the signal handler situation)
-	 * to let him remove the stack again. Notice that we really
-	 * have do a normal "return" here, or the OS would consider
-	 * the thread to be running on a signal stack which isn't
-	 * good (for instance it wouldn't allow us to spawn a thread
-	 * from within a thread, etc.)
-	 */
-	if (!setjmp(ctx_trampoline.cc)) {
-		ctx_called = 1;
-		return;
-	}
+  /*
+   * Save current machine state and _immediately_ go back with
+   * a standard "return" (to stop the signal handler situation)
+   * to let him remove the stack again. Notice that we really
+   * have do a normal "return" here, or the OS would consider
+   * the thread to be running on a signal stack which isn't
+   * good (for instance it wouldn't allow us to spawn a thread
+   * from within a thread, etc.)
+   */
+  if (!setjmp(ctx_trampoline.cc)) {
+    ctx_called = 1;
+    return;
+  }
  
-	/*
-	 * Ok, the caller has longjmp'ed back to us, so now prepare
-	 * us for the real machine state switching. We have to jump
-	 * into another function here to get a new stack context for
-	 * the auto variables (which have to be auto-variables
-	 * because the start of the thread happens later).
-	 */
-	co_ctx_bootstrap();
+  /*
+   * Ok, the caller has longjmp'ed back to us, so now prepare
+   * us for the real machine state switching. We have to jump
+   * into another function here to get a new stack context for
+   * the auto variables (which have to be auto-variables
+   * because the start of the thread happens later).
+   */
+  co_ctx_bootstrap();
 }
 
 
 static int co_set_context(co_ctx_t *ctx, void *func, char *stkbase, long stksiz) {
-	struct sigaction sa;
-	struct sigaction osa;
-	sigset_t osigs;
-	sigset_t sigs;
+  struct sigaction sa;
+  struct sigaction osa;
+  sigset_t osigs;
+  sigset_t sigs;
 #if defined(CO_HAS_SIGSTACK)
-	struct sigstack ss;
-	struct sigstack oss;
+  struct sigstack ss;
+  struct sigstack oss;
 #elif defined(CO_HAS_SIGALTSTACK)
-	struct sigaltstack ss;
-	struct sigaltstack oss;
+  struct sigaltstack ss;
+  struct sigaltstack oss;
 #else
 #error "PCL: Unknown context stack type"
 #endif
 
-	/*
-	 * Preserve the SIGUSR1 signal state, block SIGUSR1,
-	 * and establish our signal handler. The signal will
-	 * later transfer control onto the signal stack.
-	 */
-	sigemptyset(&sigs);
-	sigaddset(&sigs, SIGUSR1);
-	sigprocmask(SIG_BLOCK, &sigs, &osigs);
-	sa.sa_handler = co_ctx_trampoline;
-	sigemptyset(&sa.sa_mask);
-	sa.sa_flags = SA_ONSTACK;
-	if (sigaction(SIGUSR1, &sa, &osa) != 0)
-		return -1;
+  /*
+   * Preserve the SIGUSR1 signal state, block SIGUSR1,
+   * and establish our signal handler. The signal will
+   * later transfer control onto the signal stack.
+   */
+  sigemptyset(&sigs);
+  sigaddset(&sigs, SIGUSR1);
+  sigprocmask(SIG_BLOCK, &sigs, &osigs);
+  sa.sa_handler = co_ctx_trampoline;
+  sigemptyset(&sa.sa_mask);
+  sa.sa_flags = SA_ONSTACK;
+  if (sigaction(SIGUSR1, &sa, &osa) != 0)
+    return -1;
 
-	/*
-	 * Set the new stack.
-	 *
-	 * For sigaltstack we're lucky [from sigaltstack(2) on
-	 * FreeBSD 3.1]: ``Signal stacks are automatically adjusted
-	 * for the direction of stack growth and alignment
-	 * requirements''
-	 *
-	 * For sigstack we have to decide ourself [from sigstack(2)
-	 * on Solaris 2.6]: ``The direction of stack growth is not
-	 * indicated in the historical definition of struct sigstack.
-	 * The only way to portably establish a stack pointer is for
-	 * the application to determine stack growth direction.''
-	 */
+  /*
+   * Set the new stack.
+   *
+   * For sigaltstack we're lucky [from sigaltstack(2) on
+   * FreeBSD 3.1]: ``Signal stacks are automatically adjusted
+   * for the direction of stack growth and alignment
+   * requirements''
+   *
+   * For sigstack we have to decide ourself [from sigstack(2)
+   * on Solaris 2.6]: ``The direction of stack growth is not
+   * indicated in the historical definition of struct sigstack.
+   * The only way to portably establish a stack pointer is for
+   * the application to determine stack growth direction.''
+   */
 #if defined(CO_HAS_SIGALTSTACK)
-	ss.ss_sp = stkbase;
-	ss.ss_size = stksiz - sizeof(long);
-	ss.ss_flags = 0;
-	if (sigaltstack(&ss, &oss) < 0)
-		return -1;
+  ss.ss_sp = stkbase;
+  ss.ss_size = stksiz - sizeof(long);
+  ss.ss_flags = 0;
+  if (sigaltstack(&ss, &oss) < 0)
+    return -1;
 #elif defined(CO_HAS_SIGSTACK)
-	if (co_ctx_stackdir() < 0)
-		ss.ss_sp = (stkbase + stksiz - sizeof(long));
-	else
-		ss.ss_sp = stkbase;
-	ss.ss_onstack = 0;
-	if (sigstack(&ss, &oss) < 0)
-		return -1;
+  if (co_ctx_stackdir() < 0)
+    ss.ss_sp = (stkbase + stksiz - sizeof(long));
+  else
+    ss.ss_sp = stkbase;
+  ss.ss_onstack = 0;
+  if (sigstack(&ss, &oss) < 0)
+    return -1;
 #else
 #error "PCL: Unknown context stack type"
 #endif
 
-	/*
-	 * Now transfer control onto the signal stack and set it up.
-	 * It will return immediately via "return" after the setjmp()
-	 * was performed. Be careful here with race conditions.  The
-	 * signal can be delivered the first time sigsuspend() is
-	 * called.
-	 */
-	ctx_called = 0;
-	kill(getpid(), SIGUSR1);
-	sigfillset(&sigs);
-	sigdelset(&sigs, SIGUSR1);
-	while (!ctx_called)
-		sigsuspend(&sigs);
+  /*
+   * Now transfer control onto the signal stack and set it up.
+   * It will return immediately via "return" after the setjmp()
+   * was performed. Be careful here with race conditions.  The
+   * signal can be delivered the first time sigsuspend() is
+   * called.
+   */
+  ctx_called = 0;
+  kill(getpid(), SIGUSR1);
+  sigfillset(&sigs);
+  sigdelset(&sigs, SIGUSR1);
+  while (!ctx_called)
+    sigsuspend(&sigs);
 
-	/*
-	 * Inform the system that we are back off the signal stack by
-	 * removing the alternative signal stack. Be careful here: It
-	 * first has to be disabled, before it can be removed.
-	 */
+  /*
+   * Inform the system that we are back off the signal stack by
+   * removing the alternative signal stack. Be careful here: It
+   * first has to be disabled, before it can be removed.
+   */
 #if defined(CO_HAS_SIGALTSTACK)
-	sigaltstack(NULL, &ss);
-	ss.ss_flags = SS_DISABLE;
-	if (sigaltstack(&ss, NULL) < 0)
-		return -1;
-	sigaltstack(NULL, &ss);
-	if (!(ss.ss_flags & SS_DISABLE))
-		return -1;
-	if (!(oss.ss_flags & SS_DISABLE))
-		sigaltstack(&oss, NULL);
+  sigaltstack(NULL, &ss);
+  ss.ss_flags = SS_DISABLE;
+  if (sigaltstack(&ss, NULL) < 0)
+    return -1;
+  sigaltstack(NULL, &ss);
+  if (!(ss.ss_flags & SS_DISABLE))
+    return -1;
+  if (!(oss.ss_flags & SS_DISABLE))
+    sigaltstack(&oss, NULL);
 #elif defined(CO_HAS_SIGSTACK)
-	if (sigstack(&oss, NULL))
-		return -1;
+  if (sigstack(&oss, NULL))
+    return -1;
 #else
 #error "PCL: Unknown context stack type"
 #endif
 
-	/*
-	 * Restore the old SIGUSR1 signal handler and mask
-	 */
-	sigaction(SIGUSR1, &osa, NULL);
-	sigprocmask(SIG_SETMASK, &osigs, NULL);
+  /*
+   * Restore the old SIGUSR1 signal handler and mask
+   */
+  sigaction(SIGUSR1, &osa, NULL);
+  sigprocmask(SIG_SETMASK, &osigs, NULL);
 
-	/*
-	 * Set creation information.
-	 */
-	ctx_creating = ctx;
-	ctx_creating_func = func;
-	memcpy(&ctx_creating_sigs, &osigs, sizeof(sigset_t));
+  /*
+   * Set creation information.
+   */
+  ctx_creating = ctx;
+  ctx_creating_func = func;
+  memcpy(&ctx_creating_sigs, &osigs, sizeof(sigset_t));
 
-	/*
-	 * Now enter the trampoline again, but this time not as a signal
-	 * handler. Instead we jump into it directly.
-	 */
-	if (!setjmp(ctx_caller.cc))
-		longjmp(ctx_trampoline.cc, 1);
+  /*
+   * Now enter the trampoline again, but this time not as a signal
+   * handler. Instead we jump into it directly.
+   */
+  if (!setjmp(ctx_caller.cc))
+    longjmp(ctx_trampoline.cc, 1);
 
-	return 0;
+  return 0;
 }
 
 #else /* #if defined(CO_USE_SIGCONTEXT) */
 
 static int co_set_context(co_ctx_t *ctx, void *func, char *stkbase, long stksiz) {
-	char *stack;
+  char *stack;
 
-	stack = stkbase + stksiz - sizeof(long);
+  stack = stkbase + stksiz - sizeof(long);
 
-	setjmp(ctx->cc);
+  setjmp(ctx->cc);
 
-#if defined(__GLIBC__) && defined(__GLIBC_MINOR__) \
-    && __GLIBC__ >= 2 && __GLIBC_MINOR__ >= 0 && defined(JB_PC) && defined(JB_SP)
-	ctx->cc[0].__jmpbuf[JB_PC] = (int) func;
-	ctx->cc[0].__jmpbuf[JB_SP] = (int) stack;
-#elif defined(__GLIBC__) && defined(__GLIBC_MINOR__) \
-    && __GLIBC__ >= 2 && __GLIBC_MINOR__ >= 0 && defined(__mc68000__)
-	ctx->cc[0].__jmpbuf[0].__aregs[0] = (long) func;
-	ctx->cc[0].__jmpbuf[0].__sp = (int *) stack;
+#if defined(__GLIBC__) && defined(__GLIBC_MINOR__)                      \
+  && __GLIBC__ >= 2 && __GLIBC_MINOR__ >= 0 && defined(JB_PC) && defined(JB_SP)
+  ctx->cc[0].__jmpbuf[JB_PC] = (int) func;
+  ctx->cc[0].__jmpbuf[JB_SP] = (int) stack;
+#elif defined(__GLIBC__) && defined(__GLIBC_MINOR__)                    \
+  && __GLIBC__ >= 2 && __GLIBC_MINOR__ >= 0 && defined(__mc68000__)
+  ctx->cc[0].__jmpbuf[0].__aregs[0] = (long) func;
+  ctx->cc[0].__jmpbuf[0].__sp = (int *) stack;
 #elif defined(__GNU_LIBRARY__) && defined(__i386__)
-	ctx->cc[0].__jmpbuf[0].__pc = func;
-	ctx->cc[0].__jmpbuf[0].__sp = stack;
+  ctx->cc[0].__jmpbuf[0].__pc = func;
+  ctx->cc[0].__jmpbuf[0].__sp = stack;
 #elif defined(_WIN32) && defined(_MSC_VER)
-	((_JUMP_BUFFER *) &ctx->cc)->Eip = (long) func;
-	((_JUMP_BUFFER *) &ctx->cc)->Esp = (long) stack;
-#elif defined(__GLIBC__) && defined(__GLIBC_MINOR__) \
-    && __GLIBC__ >= 2 && __GLIBC_MINOR__ >= 0 && (defined(__powerpc64__) || defined(__powerpc__))
-	ctx->cc[0].__jmpbuf[JB_LR] = (int) func;
-	ctx->cc[0].__jmpbuf[JB_GPR1] = (int) stack;
+  ((_JUMP_BUFFER *) &ctx->cc)->Eip = (long) func;
+  ((_JUMP_BUFFER *) &ctx->cc)->Esp = (long) stack;
+#elif defined(__GLIBC__) && defined(__GLIBC_MINOR__)                    \
+  && __GLIBC__ >= 2 && __GLIBC_MINOR__ >= 0 && (defined(__powerpc64__) || defined(__powerpc__))
+  ctx->cc[0].__jmpbuf[JB_LR] = (int) func;
+  ctx->cc[0].__jmpbuf[JB_GPR1] = (int) stack;
 #else
 #error "PCL: Unsupported setjmp/longjmp platform. Please report to <davidel@xmailserver.org>"
 #endif
 
-	return 0;
+  return 0;
 }
 
 #endif /* #if defined(CO_USE_SIGCONTEXT) */
@@ -361,19 +363,19 @@
 
 static void co_switch_context(co_ctx_t *octx, co_ctx_t *nctx) {
 
-	if (!setjmp(octx->cc))
-		longjmp(nctx->cc, 1);
+  if (!setjmp(octx->cc))
+    longjmp(nctx->cc, 1);
 }
 
 #endif /* #if defined(CO_USE_UCONEXT) */
 
 
 static void co_runner(void) {
-	coroutine *co = co_curr;
+  coroutine *co = co_curr;
 
-	co->restarget = co->caller;
-	co->func(co->data);
-	co_exit();
+  co->restarget = co->caller;
+  co->func(co->data);
+  co_exit();
 }
 
 void co_vp_init()
@@ -382,139 +384,117 @@
 }
 
 coroutine_t co_create(void (*func)(void *), void *data, void *stack, int size) {
-	int alloc = 0, r = CO_STK_COROSIZE;
-	coroutine *co;
+  int alloc = 0, r = CO_STK_COROSIZE;
+  coroutine *co;
 
-	if ((size &= ~(sizeof(long) - 1)) < CO_MIN_SIZE)
-		return NULL;
-	if (!stack) {
-		size = (size + sizeof(coroutine) + CO_STK_ALIGN - 1) & ~(CO_STK_ALIGN - 1);
-		stack = malloc(size);
-		if (!stack)
-			return NULL;
-		alloc = size;
-	}
-	co = stack;
-	stack = (char *) stack + CO_STK_COROSIZE;
-	co->alloc = alloc;
-	co->func = func;
-	co->data = data;
-	if (co_set_context(&co->ctx, co_runner, stack, size - CO_STK_COROSIZE) < 0) {
-		if (alloc)
-			free(co);
-		return NULL;
-	}
+  if ((size &= ~(sizeof(long) - 1)) < CO_MIN_SIZE)
+    return NULL;
+  if (!stack) {
+    size = (size + sizeof(coroutine) + CO_STK_ALIGN - 1) & ~(CO_STK_ALIGN - 1);
+    stack = malloc(size);
+    if (!stack)
+      return NULL;
+    alloc = size;
+  }
+  co = stack;
+  stack = (char *) stack + CO_STK_COROSIZE;
+  co->alloc = alloc;
+  co->func = func;
+  co->data = data;
+  if (co_set_context(&co->ctx, co_runner, stack, size - CO_STK_COROSIZE) < 0) {
+    if (alloc)
+      free(co);
+    return NULL;
+  }
 
-	return (coroutine_t) co;
+  return (coroutine_t) co;
 }
 
 
 void co_delete(coroutine_t coro) {
-	coroutine *co = (coroutine *) coro;
+  coroutine *co = (coroutine *) coro;
 
-	if (co == co_curr) {
-		fprintf(stderr, "[PCL] Cannot delete itself: curr=%p\n",
-			co_curr);
-		exit(1);
-	}
-	if (co->alloc)
-		free(co);
+  if (co == co_curr) {
+    fprintf(stderr, "[PCL] Cannot delete itself: curr=%p\n",
+            co_curr);
+    exit(1);
+  }
+  if (co->alloc)
+    free(co);
 }
 
 
 void co_call(coroutine_t coro) {
   coroutine *co = (coroutine *) coro, *oldco;
 
-  //  printf("co = %X\n", co);
-  //  printf("co_curr = %X\n", co_curr);
   oldco = co_curr;
 
-	co->caller = co_curr;
-	co_curr = co;
+  co->caller = co_curr;
+  co_curr = co;
 
-	//	co_switch_context(&oldco->ctx, &co->ctx);
 
-#if 0
-	oldco->context_flag = 0;
-	getcontext(&oldco->ctx.cc);
-	if(oldco->context_flag == 0)
-	  {
-	    co->safe_to_enqueue = 0;
-	    oldco->safe_to_enqueue = 1;
-
-	    setcontext(&co->ctx.cc);
-	  }
-#else
-
-
-			    co->safe_to_enqueue = 0;
-			    oldco->safe_to_enqueue = 1;
-	if (swapcontext(&oldco->ctx.cc, &co->ctx.cc) < 0) {
-		fprintf(stderr, "[PCL] Context switch failed: curr=%p\n",
-			co_curr);
-		exit(1);
-	}
-#endif
-
-
-
+  co->safe_to_enqueue = 0;
+  oldco->safe_to_enqueue = 1;
+  if (swapcontext(&oldco->ctx.cc, &co->ctx.cc) < 0) {
+    fprintf(stderr, "[PCL] Context switch failed: curr=%p\n",
+            co_curr);
+    exit(1);
+  }
 }
 
 
 void co_resume(void) {
 
-	co_call(co_curr->restarget);
-	co_curr->restarget = co_curr->caller;
+  co_call(co_curr->restarget);
+  co_curr->restarget = co_curr->caller;
 }
 
 
 static void co_del_helper(void *data) {
-	coroutine *cdh;
+  coroutine *cdh;
 
-	for (;;) {
-		cdh = co_dhelper;
-		co_dhelper = NULL;
-		co_delete(co_curr->caller);
-		co_call((coroutine_t) cdh);
-		if (!co_dhelper) {
-			fprintf(stderr, "[PCL] Resume to delete helper coroutine: curr=%p\n",
-				co_curr);
-			exit(1);
-		}
-	}
+  for (;;) {
+    cdh = co_dhelper;
+    co_dhelper = NULL;
+    co_delete(co_curr->caller);
+    co_call((coroutine_t) cdh);
+    if (!co_dhelper) {
+      fprintf(stderr, "[PCL] Resume to delete helper coroutine: curr=%p\n",
+              co_curr);
+      exit(1);
+    }
+  }
 }
 
 
 void co_exit_to(coroutine_t coro) {
-	coroutine *co = (coroutine *) coro;
-	static __thread coroutine *dchelper = NULL;
-	static __thread char stk[CO_MIN_SIZE];
+  coroutine *co = (coroutine *) coro;
+  static __thread coroutine *dchelper = NULL;
+  static __thread char stk[CO_MIN_SIZE];
 
-	if (!dchelper &&
-	    !(dchelper = co_create(co_del_helper, NULL, stk, sizeof(stk)))) {
-		fprintf(stderr, "[PCL] Unable to create delete helper coroutine: curr=%p\n",
-			co_curr);
-		exit(1);
-	}
+  if (!dchelper &&
+      !(dchelper = co_create(co_del_helper, NULL, stk, sizeof(stk)))) {
+    fprintf(stderr, "[PCL] Unable to create delete helper coroutine: curr=%p\n",
+            co_curr);
+    exit(1);
+  }
 
-	co_dhelper = co;
+  co_dhelper = co;
  
-	co_call((coroutine_t) dchelper);
+  co_call((coroutine_t) dchelper);
 
-	fprintf(stderr, "[PCL] Stale coroutine called: curr=%p\n",
-		co_curr);
-	exit(1);
+  fprintf(stderr, "[PCL] Stale coroutine called: curr=%p\n",
+          co_curr);
+  exit(1);
 }
 
 
 void co_exit(void) {
-
-	co_exit_to((coroutine_t) co_curr->restarget);
+  co_exit_to((coroutine_t) co_curr->restarget);
 }
 
 
 coroutine_t co_current(void) {
-
-	return (coroutine_t) co_curr;
+  return (coroutine_t) co_curr;
 }
 
Index: libopenmp/omp_collector_validation.h
===================================================================
--- libopenmp/omp_collector_validation.h	(リビジョン 3578)
+++ libopenmp/omp_collector_validation.h	(リビジョン 3580)
@@ -1 +1,14 @@
+#ifndef	_OMP_COLLECTOR_VALIDATION_H
+#define	_OMP_COLLECTOR_VALIDATION_H
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
 int init_collector(void);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* _OMP_COLLECTOR_VALIDATION_H */
Index: libopenmp/omp_rtl.h
===================================================================
--- libopenmp/omp_rtl.h	(リビジョン 3578)
+++ libopenmp/omp_rtl.h	(リビジョン 3580)
@@ -48,7 +48,7 @@
 #include "omp_lock.h"
 #include "omp_collector_api.h"
 
-/*Cody - header files for tasks */
+/* Cody - header files for tasks */
 #include "pcl.h"
 
 
@@ -71,9 +71,9 @@
  */
 #define OMP_MAX_NUM_THREADS 	256
 #define OMP_STACK_SIZE_DEFAULT	0x400000L /* 4MB*/
+
 #define OMP_POINTER_SIZE	8
 
-/* cody - task related defaults */
 #define OMP_TASK_STACK_SIZE_DEFAULT     0x010000L /* 1KB */
 #define OMP_TASK_Q_UPPER_LIMIT_DEFAULT 10
 #define OMP_TASK_Q_LOWER_LIMIT_DEFAULT 1
@@ -90,18 +90,6 @@
 */
 // The following def should not be changed
 // It should be consistent with def in wn_mp.cxx
-
-struct collector_message
-{
-   int sz;
-   int r;
-   int ec;   
-   int rsz;
-   char *mem;
-};
-
-
-
 typedef enum {
   OMP_SCHED_UNKNOWN     = 0,
   OMP_SCHED_STATIC      = 1,
@@ -130,10 +118,11 @@
  *  Maybe need to revise.
  */
 
+
 /*Cody - changed frame_pointer from char * to void *, seems to be OK
   Did it because the Portable Coroutine Library takes a void * as an argument
 */
-typedef void * frame_pointer_t;
+typedef void* frame_pointer_t;
 
 /* The entry function prototype is nolonger 
  * the seem as GUIDE*/
@@ -166,7 +155,6 @@
 /* Can only be set through environment variable OMP_sTACK_SIZE*/
 extern volatile unsigned long int __omp_stack_size;
 
-
 /* a system level lock, used for malloc in __ompc_get_thdprv ,by Liao*/
 extern ompc_spinlock_t _ompc_thread_lock;
 
@@ -189,8 +177,8 @@
 typedef struct omp_u_thread omp_u_thread_t;
 typedef struct omp_v_thread omp_v_thread_t;
 typedef struct omp_team	    omp_team_t;
+typedef struct omp_loop_info omp_loop_info_t;
 
-
 /* kernel thread*/
 struct omp_u_thread{
   pthread_t uthread_id;		/* pthread id*/
@@ -199,6 +187,14 @@
   char *stack_pointer;
 } __attribute__ ((__aligned__(CACHE_LINE_SIZE))) ;
 
+struct omp_loop_info {
+  int       is_64bit;
+  omp_int64 lower_bound;
+  omp_int64 upper_bound;
+  omp_int64 incr;
+  omp_uint64 next_index;
+};
+
 /* team*/
 struct omp_team{
   volatile int barrier_flag; // To indicate all arrived
@@ -210,24 +206,31 @@
   /* for loop schedule*/
 
   ompc_spinlock_t schedule_lock;
-  volatile long loop_lower_bound;
-  long	loop_upper_bound;
-  long	loop_increament;
+  volatile omp_int64 loop_lower_bound;
+  omp_int64	loop_upper_bound;
+  omp_int64	loop_increament;
 	
   int	schedule_type;
-  long	chunk_size;
+  omp_int64	chunk_size;
   /* For static schedule*/
   //	long	loop_stride;
   /* For ordered dynamic schedule*/
-  volatile long schedule_count;
+  volatile omp_int64 schedule_count;
   /* We still need a semphore for scheduler initialization */
   volatile int loop_count;
+
+  /* for collapsed loop */
+  unsigned collapse_count;
+  unsigned loop_info_size;
+  omp_loop_info_t* loop_info;
+  omp_uint64* loop_lenv;
+
   /* For scheduler initialization count. */
   //	volatile int schedule_in_count;
 
   /* for ordered schedule*/
   /* Using schedule_lock as the ordered lock*/
-  volatile long	ordered_count;
+  volatile omp_int64 ordered_count;
   // using a dummy field to make the following layout better
   int dummy11;
   // offset = 128, when -m64 
@@ -249,13 +252,12 @@
   // offset = 320
   volatile int barrier_count;
 
-
-
   /* Still need a flag to indicate there are new tasks for level_1 team,
    * To avoid pthread allowed spurious wake up, and for nested teams,
    * use this as a semphore to synchronize all thread before they really start*/
   volatile int new_task;
   pthread_mutex_t ordered_mutex;
+
   /* Maybe a few more bytes should be here for alignment.*/
   /* TODO: stuff bytes*/
 
@@ -267,7 +269,6 @@
   callback callbacks[OMP_EVENT_THR_END_ATWT+1];
 } __attribute__ ((__aligned__(CACHE_LINE_SIZE_L2L3)));
 
-
 /* user thread*/
 struct omp_v_thread {
   int	vthread_id;
@@ -275,47 +276,44 @@
   int	team_size;	/* redundant with team->team_size */
   
   omp_u_thread_t *executor;	/* needed? used anywhere?*/
-  //  omp_v_thread_t *creator;      
+  //	omp_v_thread_t *creator;      
   omp_team_t     *team;
 	
   volatile omp_micro entry_func;
   volatile frame_pointer_t frame_pointer;
 
-
   /* For RUNTIME assigned STATIC schedule only*/
-  long schedule_count;
+  omp_int64 schedule_count;
   /* For ordered schedule*/
-  long ordered_count;
-  long rest_iter_count;
+  omp_int64 ordered_count;
+  omp_int64 rest_iter_count;
   /* For single sections*/
   int	single_count;
   /* For Dynamic scheduler initialization*/
   int	loop_count;
   /* for 'lastprivate'? used ?*/
   //	int is_last;
-   unsigned long thr_lkwt_state_id;
-   unsigned long thr_ctwt_state_id;
-   unsigned long thr_atwt_state_id;
-   unsigned long thr_ibar_state_id;
-   unsigned long thr_ebar_state_id;
-   unsigned long thr_odwt_state_id;
+  unsigned long thr_lkwt_state_id;
+  unsigned long thr_ctwt_state_id;
+  unsigned long thr_atwt_state_id;
+  unsigned long thr_ibar_state_id;
+  unsigned long thr_ebar_state_id;
+  unsigned long thr_odwt_state_id;
 
   /* Maybe a few more bytes should be here for alignment.*/
   /* TODO: stuff bytes*/
 } __attribute__ ((__aligned__(CACHE_LINE_SIZE)));
 
-
-
 /* The array for level 1 thread team, 
  * using vthread_id to index them
  */
-
 extern omp_v_thread_t *  __omp_level_1_team; 
 extern omp_u_thread_t *  __omp_level_1_pthread;
 extern int		 __omp_level_1_team_size;
 extern volatile omp_team_t	 __omp_level_1_team_manager;
 extern int		 __omp_level_1_team_alloc_size;
 extern omp_u_thread_t *  __omp_uthread_hash_table[UTHREAD_HASH_SIZE]; 
+
 /* Where do they should be initialized? */
 extern pthread_t 	 __omp_root_thread_id;
 extern omp_v_thread_t	 __omp_root_v_thread; /* necessary?*/
@@ -457,7 +455,7 @@
 
 
 /*External tasking API*/
-extern int __ompc_task_create(omp_task_func func, void *args, int is_tied);
+extern int __ompc_task_create(omp_task_func func, void *args, int may_delay, int is_tied);
 extern void __ompc_task_wait();
 extern void __ompc_task_exit();
 extern cond_func __ompc_task_create_cond;
Index: libopenmp/omp_task.h
===================================================================
--- libopenmp/omp_task.h	(リビジョン 3578)
+++ libopenmp/omp_task.h	(リビジョン 3580)
@@ -4,47 +4,51 @@
 #include "omp_rtl.h"
 
 
-//#define TASK_DEBUG
-
-void __ompc_task_switch(omp_task_t *old, omp_task_t *new)
+void inline __ompc_task_switch(omp_task_t *old, omp_task_t *new)
 {
 
 #ifdef TASK_DEBUG
   printf("%d: switching from %X to %X\n", __omp_myid, old, new);
 #endif
 
-  if(new->started == 0)
-    {
-      __omp_tasks_started++;
-      new->started = 1;
-      new->threadid = __omp_myid;
-    }
+  if(new->started == 0) {
+    __omp_tasks_started++;
+    new->started = 1;
+    new->threadid = __omp_myid;
+  }
   __omp_current_task = new;
 
-  /*
-  uth_switchto_ex(__omp_myid, old, new);
-  */
   new->context_flag=1;
   co_call(new);
 }
 
-/*
-void inline __ompc_task_enqueue(omp_task_t *task)
+void inline __ompc_task_exit_to(omp_task_t *current, omp_task_t *new)
 {
-  if(!task->is_parallel_task)
-    __ompc_task_q_put_tail(&__omp_local_task_q[__omp_myid], task);
+
+#ifdef TASK_DEBUG
+  printf("%d: switching from %X to %X\n", __omp_myid, current, new);
+#endif
+
+  if(new->started == 0) {
+    __omp_tasks_started++;
+    new->started = 1;
+    new->threadid = __omp_myid;
+  }
+  __omp_current_task = new;
+  new->context_flag=1;
+  pthread_mutex_destroy(&current->lock);
+  co_exit_to(new);
 }
-*/
 
 void inline __ompc_task_inc_depth()
 {
-    __omp_tasks_skipped++;
-  (__omp_current_task->pdepth)++;
+  __omp_tasks_skipped++;
+  __omp_current_task->pdepth++;
 }
 
 void inline __ompc_task_dec_depth()
 {
-  (__omp_current_task->pdepth)--;
+  __omp_current_task->pdepth--;
 }
 
 inline omp_task_t* __ompc_task_get(omp_task_func func, void *args, int stacksize)
@@ -62,3 +66,4 @@
   co_vp_init();
 }
 #endif
+
Index: libopenmp/omp_thread.c
===================================================================
--- libopenmp/omp_thread.c	(リビジョン 3578)
+++ libopenmp/omp_thread.c	(リビジョン 3580)
@@ -50,11 +50,10 @@
 #include "pcl.h"
 #include "omp_collector_util.h"
 #include "omp_collector_validation.h"
-
 /*To align with the Pathscale OMP lowering, CWG */
 
-unsigned long current_region_id=0;
-unsigned long current_parent_id=0;
+unsigned long current_region_id = 0;
+unsigned long current_parent_id = 0;
 
 
 #define debug 0
@@ -85,6 +84,7 @@
 
 volatile unsigned long int __omp_stack_size = OMP_STACK_SIZE_DEFAULT;
 volatile unsigned long int __omp_task_stack_size = OMP_TASK_STACK_SIZE_DEFAULT;
+
 volatile int __omp_task_q_upper_limit = OMP_TASK_Q_UPPER_LIMIT_DEFAULT;
 volatile int __omp_task_q_lower_limit = OMP_TASK_Q_LOWER_LIMIT_DEFAULT;
 volatile int __omp_task_level_limit = OMP_TASK_LEVEL_LIMIT_DEFAULT;
@@ -95,7 +95,7 @@
 volatile int __omp_empty_flags[OMP_MAX_NUM_THREADS];
 
 omp_v_thread_t * __omp_level_1_team = NULL;
-omp_task_t **     __omp_level_1_team_tasks = NULL;
+omp_task_t **    __omp_level_1_team_tasks = NULL;
 omp_u_thread_t * __omp_level_1_pthread = NULL;
 int		 __omp_level_1_team_size = 1;
 int		 __omp_level_1_team_alloc_size = 1;
@@ -106,9 +106,9 @@
 
 
 /* Cody - Task Queues */
-omp_task_q_t    __omp_global_task_q;
+omp_task_q_t  __omp_global_task_q;
 omp_task_q_t *__omp_private_task_q;
-omp_task_q_t    *__omp_local_task_q;
+omp_task_q_t *__omp_local_task_q;
 
 __thread omp_task_t *__omp_current_task;
 
@@ -124,15 +124,14 @@
 
 int __attribute__ ((__aligned__(CACHE_LINE_SIZE)))__omp_spin_user_lock = 0;
 
-omp_team_t	 __omp_root_team;
+omp_team_t       __omp_root_team;
 omp_u_thread_t * __omp_root_u_thread;
 omp_v_thread_t	 __omp_root_v_thread={0,THR_SERIAL_STATE,0,NULL,NULL,0,NULL,0,0,0,0,0,0,0,0,0,0,0};
 
+pthread_t	     __omp_root_thread_id = -1;
 
-pthread_t	 __omp_root_thread_id = -1;
+int              __omp_rtl_initialized = 0;
 
-int		  __omp_rtl_initialized = 0;
-
 // control variable for spin lock, it can be set by O64_OMP_SPIN_COUNT
 long int __omp_spin_count = SPIN_COUNT_DEFAULT;
 
@@ -172,9 +171,6 @@
 void *__ompc_nested_slave(void *_v_thread); 
 
 
-
-
-
 void __ompc_set_state(OMP_COLLECTOR_API_THR_STATE state)
 {
   omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
@@ -458,36 +454,27 @@
   int team_size = __omp_level_1_team_size;
   long int max_count = __omp_spin_count;
   int myrank;
-    omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
-    omp_task_t *next;
+  omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
+  omp_task_t *next;
 
-    p_vthread->thr_ebar_state_id++; 
-     __ompc_set_state(THR_IBAR_STATE);
-    __ompc_event_callback(OMP_EVENT_THR_BEGIN_IBAR);
+  p_vthread->thr_ebar_state_id++; 
+  __ompc_set_state(THR_IBAR_STATE);
+  __ompc_event_callback(OMP_EVENT_THR_BEGIN_IBAR);
+  __ompc_atomic_dec(&__omp_level_1_team_manager.num_tasks);
     
-    /*
-    __ompc_atomic_dec(&__omp_level_1_team_manager.num_tasks);
-    
 
-    while(__omp_level_1_team_manager.num_tasks != 0)
-    {
-      __ompc_task_schedule(&next);
-      if(next != NULL) {
-        __ompc_task_switch(__omp_current_task, next);
-      }
+  while(__omp_level_1_team_manager.num_tasks != 0) {
+    __ompc_task_schedule(&next);
+    if(next != NULL) {
+      __ompc_task_switch(__omp_current_task, next);
     }
-    */
+  }
+  __omp_task_stats[__omp_myid].tasks_started += __omp_tasks_started;
+  __omp_task_stats[__omp_myid].tasks_skipped += __omp_tasks_skipped;
+  __omp_task_stats[__omp_myid].tasks_stolen += __omp_tasks_stolen;
+  __omp_task_stats[__omp_myid].tasks_created += __omp_tasks_created;
 
-
-    __omp_task_stats[__omp_myid].tasks_started += __omp_tasks_started;
-    __omp_task_stats[__omp_myid].tasks_skipped += __omp_tasks_skipped;
-    __omp_task_stats[__omp_myid].tasks_stolen += __omp_tasks_stolen;
-    __omp_task_stats[__omp_myid].tasks_created += __omp_tasks_created;
-
-
-  myrank = 1 + __sync_fetch_and_add(&__omp_level_1_exit_count,1);
-  /* myrank = __ompc_atomic_inc(&__omp_level_1_exit_count); */
-
+  myrank = __ompc_atomic_inc(&__omp_level_1_exit_count);
   if (vthread_id == 0) {
     if (myrank != team_size)
     {
@@ -522,12 +509,12 @@
   Is_True((vthread != NULL) && (vthread->team != NULL), 
 	  ("bad vthread or vthread->team in nested groups"));
   
-    omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
-    p_vthread->thr_ibar_state_id++;
-     __ompc_set_state(THR_IBAR_STATE);
+  omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
+  p_vthread->thr_ibar_state_id++;
+  __ompc_set_state(THR_IBAR_STATE);
   __ompc_event_callback(OMP_EVENT_THR_BEGIN_IBAR);
 
-  __sync_fetch_and_add(&(vthread->team->barrier_count),1);
+  __ompc_atomic_inc(&(vthread->team->barrier_count));
 
   // Master wait all slaves arrived
   if(vthread->vthread_id == 0) {
@@ -558,10 +545,9 @@
   __ompc_event_callback(OMP_EVENT_THR_BEGIN_IDLE);
   initial_idle = 0;
 
-  __sync_fetch_and_add(&__omp_level_1_pthread_count,1);
-  /* __ompc_atomic_inc(&(vthread->team->barrier_count)); */
+  __ompc_atomic_inc(&__omp_level_1_pthread_count);
 
-  /*initialize virtual processor - Cody*/
+  /* initialize virtual processor - Cody */
   __ompc_init_vp();
 
   for(;;) {
@@ -614,7 +600,6 @@
 
       __ompc_set_state(THR_IDLE_STATE);
       __ompc_event_callback(OMP_EVENT_THR_BEGIN_IDLE);
-
     }
   }
 
@@ -629,6 +614,7 @@
   /* need to wait for others ready? */
 
   __ompc_set_state(THR_IDLE_STATE);
+  /* printf("IDLE called from nested\n"); */
 
   OMPC_WAIT_WHILE(my_vthread->team->new_task != 1);
   /* The relationship between vthread, uthread, and team should be OK here*/
@@ -643,7 +629,6 @@
   __ompc_exit_barrier(my_vthread);
 
   pthread_exit(NULL);
-
 }
 
 void
@@ -742,14 +727,18 @@
   __omp_num_processors = __omp_get_cpu_num();
   __omp_nthreads_var = __omp_num_processors;
 
+#ifndef TARG_LOONGSON
   /* get the list of available processors, later we can bind pthreads
      to the available processors */ 
   __omp_get_available_processors();
+#endif //TARG_LOONGSON
 
   /* parse OpenMP environment variables */
   __ompc_environment_variables();
+#ifndef TARG_LOONGSON
   __ompc_sug_numthreads = __omp_nthreads_var;
   __ompc_cur_numthreads = __omp_nthreads_var;
+#endif //TARG_LOONGSON
 
   /* register the finalize function*/
   atexit(__ompc_fini_rtl);
@@ -760,7 +749,7 @@
   if(threads_to_create == 1)
       __ompc_task_create_cond = __ompc_task_false_cond;
 
-  /*keep it as nthreads-var suggested in spec. Liao*/
+  /* keep it as nthreads-var suggested in spec. Liao*/
   __omp_nthreads_var = threads_to_create;
   /* setup pthread attributes */
   pthread_attr_init(&__omp_pthread_attr);
@@ -775,7 +764,6 @@
   pthread_cond_init(&__omp_level_1_barrier_cond, NULL);
   __ompc_init_spinlock(&_ompc_thread_lock);
 
-
   __omp_task_limit = OMP_TASK_LIMIT_DEFAULT;
 
   for(i=0; i<threads_to_create; i++) {
@@ -838,6 +826,8 @@
   __omp_level_1_team_manager.single_count = 0;
   __omp_level_1_team_manager.new_task = 0;
   __omp_level_1_team_manager.loop_count = 0;
+  __omp_level_1_team_manager.loop_info_size = 0;
+  __omp_level_1_team_manager.loop_info = NULL;
   /*Cody - initialize num tasks to 0 */
   __omp_level_1_team_manager.num_tasks = 0;
 	
@@ -889,11 +879,13 @@
   __omp_root_u_thread->hash_next = NULL;
   __ompc_insert_into_hash_table(__omp_root_u_thread);
 
-
+#ifndef TARG_LOONGSON
   if (__omp_set_affinity) {
     //bind the current thread to the first available cpu 
     __ompc_bind_pthread_to_cpu(__omp_root_thread_id);
   }
+#endif //TARG_LOONGSON
+  __ompc_event_callback(OMP_EVENT_FORK);
 
   __ompc_event_callback(OMP_EVENT_FORK);
 
@@ -924,8 +916,10 @@
   for (i=1; i< threads_to_create; i++) {
     stack_pointer = malloc(__omp_stack_size);
     Is_True(stack_pointer != NULL, ("Cannot allocate stack for slave"));
+#ifndef TARG_LOONGSON
     return_value = pthread_attr_setstack(&__omp_pthread_attr, stack_pointer, __omp_stack_size); 
     Is_True(return_value == 0, ("Cannot set stack pointer for thread"));
+#endif //TARG_LOONGSON
     return_value = pthread_create( &(__omp_level_1_pthread[i].uthread_id),
 				   &__omp_pthread_attr, (pthread_entry) __ompc_level_1_slave, 
 				   (void *)((unsigned long int)i));
@@ -933,10 +927,12 @@
 
     __omp_level_1_pthread[i].stack_pointer = stack_pointer;
 
+#ifndef TARG_LOONGSON
     if (__omp_set_affinity) {
       // bind pthread to a specific cpu
       __ompc_bind_pthread_to_cpu(__omp_level_1_pthread[i].uthread_id);
     }
+#endif //TARG_LOONGSON
 
     __ompc_insert_into_hash_table(&(__omp_level_1_pthread[i]));
   }
@@ -1018,6 +1014,7 @@
 	 sizeof(omp_v_thread_t) * (new_num_threads - __omp_level_1_team_alloc_size));
   __omp_level_1_team_manager.team_size = new_num_threads;
   __ompc_event_callback(OMP_EVENT_FORK);
+
   for (i=__omp_level_1_team_alloc_size; i<new_num_threads; i++) {
     /* for v_thread */
     __omp_level_1_team[i].team = &__omp_level_1_team_manager;
@@ -1046,10 +1043,12 @@
 
     __omp_level_1_pthread[i].stack_pointer = stack_pointer;
 
+#ifndef TARG_LOONGSON
     if (__omp_set_affinity) {
       // bind pthread to a specific cpu
       __ompc_bind_pthread_to_cpu(__omp_level_1_pthread[i].uthread_id);
     }
+#endif //TARG_LOONGSON
 
     __ompc_insert_into_hash_table(&(__omp_level_1_pthread[i]));
   }
@@ -1082,9 +1081,14 @@
   void * stack_pointer;
   unsigned int region_used = 0; // TODO: make it one-bit.
 
-  
+#ifndef TARG_LOONGSON
   Is_True(__omp_rtl_initialized != 0,
           (" RTL should have been initialized!"));
+#else
+  if (!__omp_rtl_initialized) {
+    __ompc_init_rtl(0);
+  }
+#endif //TARG_LOONGSON
 
   // check the validity of num_threads
   if (num_threads != 0) 
@@ -1100,7 +1104,6 @@
       current_region_id++;
       __ompc_set_state(THR_OVHD_STATE);
       __ompc_event_callback(OMP_EVENT_FORK);
-
     } else {
       first_time=1;
     } 
@@ -1117,10 +1120,8 @@
       __omp_level_1_team_size = num_threads;
       __omp_level_1_team_manager.team_size = num_threads;
     }
-   
-        
-     __omp_level_1_team_manager.num_tasks = __omp_level_1_team_size;
-
+    
+    __omp_level_1_team_manager.num_tasks = __omp_level_1_team_size;
     for (i=0; i<__omp_level_1_team_size; i++) {
       __omp_level_1_team[i].frame_pointer = frame_pointer;
       __omp_level_1_team[i].team_size = __omp_level_1_team_size;
@@ -1187,6 +1188,8 @@
     temp_team.new_task = 0;
     /* Used anywhere. obsoleted*/
     temp_team.loop_count = 0;
+    temp_team.loop_info_size = 0;
+    temp_team.loop_info = NULL;
     temp_team.single_count = 0;
 
     __ompc_init_spinlock(&(temp_team.schedule_lock));
@@ -1252,6 +1255,7 @@
     /* execution */
     /* A start barrier should also be presented here?*/
     __ompc_set_state(THR_WORK_STATE);
+
     micro_task(0, frame_pointer);
 
     __ompc_exit_barrier(&(nest_v_thread_team[0]));
@@ -1288,6 +1292,8 @@
     /* The lock can be eliminated, anyway */
     /* no need to use lock in this case*/
     temp_team.loop_count = 0;
+    temp_team.loop_info_size = 0;
+    temp_team.loop_info = NULL;
     temp_team.single_count = 0;
 
     __ompc_init_spinlock(&(temp_team.schedule_lock));
@@ -1300,6 +1306,7 @@
     __omp_exe_mode = OMP_EXE_MODE_NESTED_SEQUENTIAL;
     /* execute the task */
     __ompc_set_state(THR_WORK_STATE);
+    __ompc_set_state(THR_WORK_STATE);
     micro_task(0, frame_pointer);
 
     /* lock should be added here */
@@ -1318,6 +1325,37 @@
  
 
 }
+/* Check the _num_threads against __omp_max_num_threads*/
+int
+__ompc_check_num_threads(const int _num_threads)
+{
+  int num_threads = _num_threads;
+  int request_threads;
+  /* How about _num_threads == 1*/
+  Is_Valid( num_threads > 0,
+	    ("number of threads must be possitive!"));
+  if (__omp_exe_mode & OMP_EXE_MODE_SEQUENTIAL) {
+    /* request for level 1 threads*/
+    request_threads = num_threads - __omp_level_1_team_alloc_size;
+    if (request_threads <= __omp_max_num_threads) {
+      /* OK. we can fulfill your request*/
+    } else {
+      /* Exceed current limitat*/
+      Warning(" Exceed the thread number limit: Reduce to Max");
+      num_threads = __omp_level_1_team_alloc_size + __omp_max_num_threads;
+    }
+  } else {/* Request for nest team*/
+    if ((num_threads - 1) > __omp_max_num_threads) {
+      /* Exceed current limit*/
+      /* The master is already there, need not to be allocated*/
+      num_threads = __omp_max_num_threads + 1; 
+      Warning(" Exceed the thread number limit: Reduce to Max");
+    } else {
+      /* OK. we can fulfill your request*/
+    }
+  }
+  return num_threads;
+}
 
 /* How about Critical/Atomic? */
 
@@ -1334,3 +1372,4 @@
 {
 
 }
+
Index: libopenmp/pcl.h
===================================================================
--- libopenmp/pcl.h	(リビジョン 3578)
+++ libopenmp/pcl.h	(リビジョン 3580)
@@ -28,6 +28,7 @@
 extern "C" {
 #endif
 
+
 #define CO_USE_UCONEXT
 
 #if defined(CO_USE_UCONEXT)
@@ -51,16 +52,16 @@
 
 
 typedef struct s_co_ctx {
-	co_core_ctx_t cc;
+  co_core_ctx_t cc;
 } co_ctx_t;
 
 typedef struct s_coroutine {
-	co_ctx_t ctx;
-	int alloc;
-	struct s_coroutine *caller;
-	struct s_coroutine *restarget;
-	void (*func)(void *);
-	void *data;
+  co_ctx_t ctx;
+  int alloc;
+  struct s_coroutine *caller;
+  struct s_coroutine *restarget;
+  void (*func)(void *);
+  void *data;
 
   struct s_coroutine *next;
   struct s_coroutine *prev;
Index: libopenmp/omp_thread.h
===================================================================
--- libopenmp/omp_thread.h	(リビジョン 3578)
+++ libopenmp/omp_thread.h	(リビジョン 3580)
@@ -43,6 +43,7 @@
 #include "omp_util.h"
 #include "pcl.h"
 #include <time.h>
+#include <unistd.h>
 
 /* Interfaces have already been defined in omp_rtl.h.
  * Since implementations are included here, not include
@@ -51,6 +52,17 @@
 
 extern pthread_mutex_t __omp_hash_table_lock;
 
+extern int __ompc_init_rtl(int num_threads);
+
+extern int __ompc_check_num_threads(const int _num_threads);
+extern void __ompc_expand_level_1_team(int new_num_threads);
+
+static inline void __ompc_check_rtl_init()
+{
+  if (__omp_rtl_initialized == 0)
+    __ompc_init_rtl(0);
+}
+
 inline void __ompc_set_nested(const int __nested)
 {
   /* A lock is needed here to protect it?*/
@@ -74,6 +86,7 @@
 
 inline int __omp_get_cpu_num()
 {
+#ifndef TARG_LOONGSON
   cpu_set_t cpuset;
   int return_val, i, cur_count = 0;
 
@@ -84,38 +97,26 @@
     if (CPU_ISSET(i, &cpuset)) cur_count ++;
 
   return cur_count;
+#else
+  return (int) sysconf(_SC_NPROCESSORS_ONLN);
+#endif
 }
 
 inline int __ompc_get_max_threads(void)
 {
   /*Could be called in 1. sequential part or 2. parallel region
 
-    1.  for sequential part invoking: 
+  1.  for sequential part invoking: 
     return the value of OMP_NUM_THREADS or
-    number of available processors
+		    number of available processors
     cannot use the internal var because the RTL may not yet been initialized!!
-    2. for a parallel region: return the initialized internal var.
-    By Liao. 8/30/2006 bug 157
-    */
+  2. for a parallel region: return the initialized internal var.
+  */
   char *env_var_str;
   int  env_var_val;
 
-  if (__omp_rtl_initialized == 1)
-    return __omp_nthreads_var;
-  else {
-    /*
-    env_var_str = getenv("OMP_NUM_THREADS");
-    if (env_var_str != NULL) {
-      sscanf(env_var_str, "%d", &env_var_val);
-      Is_Valid(env_var_val > 0, ("OMP_NUM_THREAD should > 0"));
-      if (env_var_val > __omp_max_num_threads)
-        env_var_val = __omp_max_num_threads;
-      return env_var_val;
-    } 
-    else
-    */
-      return __omp_get_cpu_num(); /* return Get_SMP_CPU_num();*/
-  }
+  __ompc_check_rtl_init();
+  return __omp_nthreads_var;
 }
 
 inline int __ompc_get_num_procs(void)
@@ -130,6 +131,8 @@
 {
   int num_threads;
 
+  __ompc_check_rtl_init();
+
   // check the validity of _num_threads
   num_threads = __ompc_check_num_threads(__num_threads);
 
@@ -286,6 +289,7 @@
 
 extern __thread int total_tasks;
 extern long int __omp_spin_count; // defined in omp_thread.c
+extern __thread int total_tasks;
 
 /* Should not be called directly, use __ompc_barrier instead*/
 inline void __ompc_barrier_wait(omp_team_t *team)
@@ -300,21 +304,20 @@
 
   barrier_flag = *barrier_flag_p;
 
-  /*
   __ompc_atomic_dec(&__omp_level_1_team_manager.num_tasks);
 
-  while (__omp_level_1_team_manager.num_tasks != 0) {
+  while(__omp_level_1_team_manager.num_tasks != 0) {
     __ompc_task_schedule(&next);
-    if (next != NULL) {
+    if(next != NULL) {
       __ompc_task_switch(__omp_level_1_team_tasks[__omp_myid], next);
     }
   }
-  */
 
   new_count = __ompc_atomic_inc(&team->barrier_count);
 
   if (new_count == team->team_size) {
     /* The last one reset flags*/
+    __omp_level_1_team_manager.num_tasks += new_count;
     team->barrier_count = 0;
     team->barrier_flag = barrier_flag ^ 1; /* Xor: toggle*/
 
@@ -364,11 +367,9 @@
   __ompc_set_state(THR_IBAR_STATE);
   __ompc_event_callback(OMP_EVENT_THR_BEGIN_IBAR); 
   if (__omp_exe_mode & OMP_EXE_MODE_NORMAL) {
-    //		if (__omp_level_1_team_size != 1)
-    //		{
     __ompc_barrier_wait(&__omp_level_1_team_manager);
     __ompc_event_callback(OMP_EVENT_THR_END_IBAR);
-     __ompc_set_state(THR_WORK_STATE);
+    __ompc_set_state(THR_WORK_STATE);
     return;
     //		}
     //		else
@@ -382,19 +383,19 @@
   temp_v_thread = __ompc_get_current_v_thread();
   if(temp_v_thread->team_size == 1) {
     __ompc_event_callback(OMP_EVENT_THR_END_IBAR);
-     __ompc_set_state(THR_WORK_STATE);
+    __ompc_set_state(THR_WORK_STATE);
     return;
   }
   else {
     __ompc_barrier_wait(temp_v_thread->team);
   }
-  __ompc_event_callback(OMP_EVENT_THR_END_IBAR);
-   __ompc_set_state(THR_WORK_STATE);
+__ompc_event_callback(OMP_EVENT_THR_END_IBAR);
+__ompc_set_state(THR_WORK_STATE);
 }
 
 inline void __ompc_ebarrier(void)
 {
-  omp_v_thread_t *temp_v_thread;
+    omp_v_thread_t *temp_v_thread;
    omp_v_thread_t *p_vthread = __ompc_get_v_thread_by_num( __omp_myid);
     p_vthread->thr_ebar_state_id++;
   __ompc_set_state(THR_EBAR_STATE);
@@ -418,44 +419,17 @@
   /* other situations*/
   temp_v_thread = __ompc_get_current_v_thread();
   if(temp_v_thread->team_size == 1) {
-    __ompc_event_callback(OMP_EVENT_THR_END_EBAR);
-     __ompc_set_state(THR_WORK_STATE);
+    __ompc_event_callback(OMP_EVENT_THR_END_IBAR);
+    __ompc_set_state(THR_WORK_STATE);
     return;
-    }
+  }
   else {
     __ompc_barrier_wait(temp_v_thread->team);
   }
-   __ompc_set_state(THR_WORK_STATE);
-  __ompc_event_callback(OMP_EVENT_THR_END_EBAR);
+  __ompc_event_callback(OMP_EVENT_THR_END_IBAR);
+  __ompc_set_state(THR_WORK_STATE);
 }
 
-/* Check the _num_threads against __omp_max_num_threads*/
-int
-__ompc_check_num_threads(const int _num_threads)
-{
-  int num_threads = _num_threads;
-  Is_Valid( num_threads > 0,
-	    ("number of threads must be positive!"));
-
-  if (__omp_exe_mode & OMP_EXE_MODE_SEQUENTIAL) {
-    /* request for level 1 threads*/
-    if (num_threads - __omp_level_1_team_alloc_size > __omp_max_num_threads) {
-      /* Exceed current limitat*/
-      Warning(" Exceed the thread number limit: Reduce to Max");
-      num_threads = __omp_level_1_team_alloc_size + __omp_max_num_threads;
-    }
-  } else {/* Request for nest team*/
-    if ((num_threads - 1) > __omp_max_num_threads) {
-      /* Exceed current limit*/
-      /* The master is already there, need not to be allocated*/
-      Warning(" Exceed the thread number limit: Reduce to Max");
-      num_threads = __omp_max_num_threads + 1; 
-    } 
-  }
-
-  return num_threads;
-}
-
 /* Exposed API should be moved to somewhere else, instead of been inlined*/
 /* flush needs to do nothing on IA64 based platforms?*/
 inline void __ompc_flush(void *p)
@@ -480,6 +454,7 @@
   /* do nothing */
 }
 
+#ifndef TARG_LOONGSON
 inline void __omp_get_available_processors()
 {
   cpu_set_t cpuset;
@@ -542,5 +517,6 @@
   cur_cpu_to_bind = (cur_cpu_to_bind + 1) % __omp_core_list_size; 
 
 }
+#endif //TARG_LOONGSON
 
 #endif /* __omp_rtl_thread_included */
Index: libopenmp/libopenmp.vs
===================================================================
--- libopenmp/libopenmp.vs	(リビジョン 3578)
+++ libopenmp/libopenmp.vs	(リビジョン 3580)
@@ -24,6 +24,8 @@
     __ompc_schedule_next_8;
     __ompc_scheduler_init_4;
     __ompc_scheduler_init_8;
+    __ompc_collapse_init;
+    __ompc_collapse_next;
     __ompc_serialized_parallel;
     __ompc_set_num_threads;
     __ompc_single;
@@ -32,9 +34,17 @@
     __ompc_static_init_8;
     __ompc_sug_numthreads;
     __ompc_cur_numthreads;
+    __ompc_task_wait;
     __ompc_reduction;
     __ompc_end_reduction;
     __ompc_ebarrier;
+    __ompc_task_create_cond;
+    __ompc_task_create;
+    __ompc_task_body_start;
+    __ompc_task_delete;
+    __ompc_task_exit;
+    __ompc_task_inc_depth;
+    __ompc_task_dec_depth;
     omp_destroy_lock;
     omp_destroy_lock_;
     omp_destroy_nest_lock;

属性に変更があったパス: libopenmp
___________________________________________________________________
追加: svn:mergeinfo
   /branches/config/osprey/libopenmp:r3003-3087 をマージしました
   /branches/open64-booster/osprey/libopenmp:r2027-3263 をマージしました
   /trunk/osprey/libopenmp:r1120-3577 をマージしました
   /branches/openmp3.0/osprey/libopenmp:r3478-3556 をマージしました
   /branches/open64-ppc32/osprey/libopenmp:r2293-2750 をマージしました
   /branches/nextgenalias/osprey/libopenmp:r2766-3433 をマージしました

Index: be/be/omp_lower.cxx
===================================================================
--- be/be/omp_lower.cxx	(リビジョン 3578)
+++ be/be/omp_lower.cxx	(リビジョン 3580)
@@ -2522,6 +2522,9 @@
 {
   WN *store = WN_next(atomic);
 
+#ifdef TARG_LOONGSON
+  Atomic_Using_Critical(atomic, store);
+#else
   if (OPCODE_is_store(WN_opcode(store)) && 
 	(!WN_kid_count(WN_kid0(store))) || 
 	(OPCODE_is_load(WN_opcode(WN_kid0(store))))) {
@@ -2585,6 +2588,7 @@
   default:
     Fail_FmtAssertion("invalid ATOMIC_Lowering_Class");
   }
+#endif //TARG_LOONGSON
 }
 
   // Determine lowering class of an ATOMIC, also validates that atomic is
Index: be/com/wn_mp.cxx
===================================================================
--- be/com/wn_mp.cxx	(リビジョン 3578)
+++ be/com/wn_mp.cxx	(リビジョン 3580)
@@ -253,98 +253,100 @@
     MPR_PARALLEL_DO_32       = 9,
     MPR_PARALLEL_DO_64       = 10,
     MPR_PARALLEL_REGION      = 11,
+    MPR_TASK_REGION          = 12,
 
-    MPR_BEGIN_PDO_32         = 12,
-    MPR_BEGIN_PDO_64         = 13,
-    MPR_NEXT_ITERS_32        = 14,
-    MPR_NEXT_ITERS_64        = 15,
-    MPR_END_PDO              = 16,
+    MPR_BEGIN_PDO_32         = 13,
+    MPR_BEGIN_PDO_64         = 14,
+    MPR_NEXT_ITERS_32        = 15,
+    MPR_NEXT_ITERS_64        = 16,
+    MPR_END_PDO              = 17,
 
-    MPR_BEGIN_SINGLE_PROCESS = 17,
-    MPR_END_SINGLE_PROCESS   = 18,
+    MPR_BEGIN_SINGLE_PROCESS = 18,
+    MPR_END_SINGLE_PROCESS   = 19,
 
-    MPR_ENTER_GATE           = 19,
-    MPR_EXIT_GATE            = 20,
+    MPR_ENTER_GATE           = 20,
+    MPR_EXIT_GATE            = 21,
 
-    MPR_BEGIN_INDEPENDENT    = 21,
-    MPR_END_INDEPENDENT      = 22,
+    MPR_BEGIN_INDEPENDENT    = 22,
+    MPR_END_INDEPENDENT      = 23,
 
-    MPR_MY_THREADNUM         = 23,
+    MPR_MY_THREADNUM         = 24,
 
-  MPR_OMP_PARALLEL_REGION    = 24,
-  MPR_OMP_BEGIN_SPR          = 25,      /* serialized parallel region */
-  MPR_OMP_END_SPR            = 26,
-  MPR_OMP_PARALLEL_DO_32     = 27,
-  MPR_OMP_PARALLEL_DO_64     = 28,
+  MPR_OMP_PARALLEL_REGION    = 25,
+  MPR_OMP_BEGIN_SPR          = 26,      /* serialized parallel region */
+  MPR_OMP_END_SPR            = 27,
+  MPR_OMP_PARALLEL_DO_32     = 28,
+  MPR_OMP_PARALLEL_DO_64     = 29,
   
-  MPR_OMP_BEGIN_PDO_64         = 29,
-  MPR_OMP_NEXT_ITERS_64        = 30,
-  MPR_OMP_END_PDO              = 31,
+  MPR_OMP_BEGIN_PDO_64         = 30,
+  MPR_OMP_NEXT_ITERS_64        = 31,
+  MPR_OMP_END_PDO              = 32,
 
-  MPR_OMP_BEGIN_SINGLE_PROCESS = 32,
-  MPR_OMP_END_SINGLE_PROCESS   = 33,
-  MPR_OMP_BARRIER_OLD          = 34, /* conflict with new API */
+  MPR_OMP_BEGIN_SINGLE_PROCESS = 33,
+  MPR_OMP_END_SINGLE_PROCESS   = 34,
+  MPR_OMP_BARRIER_OLD          = 35, /* conflict with new API */
 
-  MPR_OMP_PDO_ORDERED_BEGIN    = 35,
-  MPR_OMP_PDO_ORDERED_END      = 36,
-  MPR_OMP_ORDERED_BEGIN_OLD    = 37, /* conflict with new API */
-  MPR_OMP_ORDERED_END_OLD      = 38, /* conflict with new API */
+  MPR_OMP_PDO_ORDERED_BEGIN    = 36,
+  MPR_OMP_PDO_ORDERED_END      = 37,
+  MPR_OMP_ORDERED_BEGIN_OLD    = 38, /* conflict with new API */
+  MPR_OMP_ORDERED_END_OLD      = 39, /* conflict with new API */
 
-  MPR_OMP_COPYIN               = 39,
-  MPR_OMP_NONPOD_COPYIN        = 40,
-  MPR_OMP_NONPOD_ARRAY_COPYIN  = 41,
+  MPR_OMP_COPYIN               = 40,
+  MPR_OMP_NONPOD_COPYIN        = 41,
+  MPR_OMP_NONPOD_ARRAY_COPYIN  = 42,
 
   // Begin KMPC RTL calls.
-  MPR_OMP_IN_PARALLEL		  = 42,	/* test if in parallel region */
-  MPR_OMP_CAN_FORK		  = 43,	/* test fork, to be eliminated*/
-  MPR_OMP_SET_NUM_THREADS	  = 44,	/* for threadnum setting, not used*/
-  MPR_OMP_INIT_RTL		  = 45, /* obsoleted */
-  MPR_OMP_FINI_RTL		  = 46, /* obsoleted */
+  MPR_OMP_IN_PARALLEL		  = 43,	/* test if in parallel region */
+  MPR_OMP_CAN_FORK		  = 44,	/* test fork, to be eliminated*/
+  MPR_OMP_SET_NUM_THREADS	  = 45,	/* for threadnum setting, not used*/
+  MPR_OMP_INIT_RTL		  = 46, /* obsoleted */
+  MPR_OMP_FINI_RTL		  = 47, /* obsoleted */
   
-  MPR_OMP_SERIALIZED_PARALLEL     = 47, /* para-region serialized, can delete */
-  MPR_OMP_END_SERIALIZED_PARALLEL = 48, /* Can be deleted */
-  MPR_OMP_GET_THREAD_NUM	  = 49,
-  MPR_OMP_GET_NUM_THREADS	  = 50, 
+  MPR_OMP_SERIALIZED_PARALLEL     = 48, /* para-region serialized, can delete */
+  MPR_OMP_END_SERIALIZED_PARALLEL = 49, /* Can be deleted */
+  MPR_OMP_GET_THREAD_NUM	  = 50,
+  MPR_OMP_GET_NUM_THREADS	  = 51, 
 
-  MPR_OMP_FORK			  = 51,
+  MPR_OMP_FORK			  = 52,
 
-  MPR_OMP_STATIC_INIT_4		 = 52,
-  MPR_OMP_STATIC_INIT_8	 	 = 53,
-  MPR_OMP_STATIC_FINI	  	 = 54, /* Can be deleted */
+  MPR_OMP_STATIC_INIT_4		 = 53,
+  MPR_OMP_STATIC_INIT_8	 	 = 54,
+  MPR_OMP_STATIC_FINI	  	 = 55, /* Can be deleted */
 
-  MPR_OMP_SCHEDULER_INIT_4	  = 55,
-  MPR_OMP_SCHEDULER_INIT_8	  = 56,
-  MPR_OMP_SCHEDULER_NEXT_4	  = 57,
-  MPR_OMP_SCHEDULER_NEXT_8	  = 58,
+  MPR_OMP_SCHEDULER_INIT_4	  = 56,
+  MPR_OMP_SCHEDULER_INIT_8	  = 57,
+  MPR_OMP_SCHEDULER_NEXT_4	  = 58,
+  MPR_OMP_SCHEDULER_NEXT_8	  = 59,
   
-  MPR_OMP_SINGLE		  = 59,
-  MPR_OMP_END_SINGLE		  = 60,
+  MPR_OMP_SINGLE		  = 60,
+  MPR_OMP_END_SINGLE		  = 61,
 
-  MPR_OMP_MASTER		  = 61,
-  MPR_OMP_END_MASTER		  = 62,
+
+  MPR_OMP_MASTER		  = 62,
+  MPR_OMP_END_MASTER		  = 63,
   
-  MPR_OMP_BARRIER		  = 63,
-  MPR_OMP_EBARRIER              = 64,
-  MPR_OMP_CRITICAL		= 65,
-  MPR_OMP_END_CRITICAL		= 66,
-  MPR_OMP_REDUCTION             = 67,
-  MPR_OMP_END_REDUCTION         = 68,
-  MPR_OMP_ORDERED		= 69,
-  MPR_OMP_END_ORDERED		= 70,
+  MPR_OMP_BARRIER		  = 64,
+  MPR_OMP_EBARRIER              = 65,
+  MPR_OMP_CRITICAL		= 66,
+  MPR_OMP_END_CRITICAL		= 67,
+  MPR_OMP_REDUCTION             = 68,
+  MPR_OMP_END_REDUCTION         = 69,
+  MPR_OMP_ORDERED		= 70,
+  MPR_OMP_END_ORDERED		= 71,
 
-  MPR_OMP_FLUSH			= 71,	/* Not really needed? to be deleted*/
-  MPR_OMP_TASKWAIT              = 72,
-  MPR_OMP_TASK                  = 73,
-  MPR_OMP_END_TASK              = 74,
-#ifdef KEY
-  MPR_OMP_GET_THDPRV            = 75,
-  MPR_OMP_COPYIN_THDPRV         = 76,
-  MPR_OMP_COPYPRIVATE           = 77,
-  MPRUNTIME_LAST = MPR_OMP_COPYPRIVATE
-#else
-  MPRUNTIME_LAST = MPR_OMP_END_TASK
-#endif
+  MPR_OMP_FLUSH			= 72,	/* Not really needed? to be deleted*/
+  MPR_OMP_TASKWAIT              = 73,
+  MPR_OMP_TASK_CREATE           = 74,
+  MPR_OMP_TASK_BODY_START       = 75,
+  MPR_OMP_TASK_EXIT             = 76,
+  MPR_OMP_GET_THDPRV            = 77,
+  MPR_OMP_COPYIN_THDPRV         = 78,
+  MPR_OMP_COPYPRIVATE           = 79,
 
+  MPR_OMP_COLLAPSE_INIT         = 80,
+  MPR_OMP_COLLAPSE_NEXT	        = 81,
+
+  MPRUNTIME_LAST = MPR_OMP_COLLAPSE_NEXT
 } MPRUNTIME;
 
 
@@ -375,6 +377,7 @@
   PAR_FUNC_DO32,
   PAR_FUNC_DO64,
   PAR_FUNC_REGION,
+  PAR_FUNC_TASK,
   PAR_FUNC_LAST = PAR_FUNC_REGION
 } PAR_FUNC_TYPE;
 
@@ -517,28 +520,26 @@
 static INT64 line_number;	/* Line number of parallel do/region */
 
 static ST *parallel_proc;	/* Extracted parallel process */
-static ST *local_start;   /* Parallel Do local start, obsolete */
-static ST *local_ntrip;   /* Parallel Do local ntrip, obsolete */
-static ST *thread_info;   /* Parallel Do thread info, obsolete */
-static ST *local_upper = NULL;   /* Parallel Do local upper bound */
-static ST *local_lower = NULL;   /* Parallel Do local lower bound */
-static ST *local_stride = NULL;  /* Parallel Do local stride for next chunk */
+static vector<ST *> local_upper;   /* Parallel Do local upper bound */
+static vector<ST *> local_lower;   /* Parallel Do local lower bound */
+static vector<ST *> local_stride;  /* Parallel Do local stride for next chunk */
 static ST *last_iter;      /* Is local execute last iteration? */
-static ST *local_limit;   /* Parallel Do local limit */
-static ST *limit_st;      /* Temp var to store do_limit. can be preg. */
-static WN_OFFSET limit_ofst;
+static vector<ST *> limit_st;      /* Temp var to store do_limit. can be preg. */
+static vector<WN_OFFSET> limit_ofst;
+static vector<ST *> base_st;      /* Temp var to store do_base. can be preg. */
+static vector<WN_OFFSET> base_ofst;
 static ST *local_gtid;		/* Microtask local gtid */
 static ST *local_btid;		/* Microtask local btid */
-static WN *base_node;		  /* Parallel do base */
-static WN *limit_node = NULL;		/* Parallel do limit */
+static vector<WN *> base_node;		  /* Parallel do base */
+static vector<WN *> limit_node;		/* Parallel do limit */
 static WN *ntrip_node;		/* Parallel do trip count */
-static WN *stride_node;		/* Parallel do stride */
+static vector<WN *> stride_node;		/* Parallel do stride */
 static WN *parallel_func;	/* Parallel do function */
 static FEEDBACK *parallel_pu_fb;  /* Feedback for parallel function */
 static WN *reference_block;	/* Parallel funciton reference block */
 static INT32 func_level;	/* Parallel function stab level */
-static ST *do_index_st;		/* User do index variable ST */
-static TYPE_ID do_index_type;	/* User do index variable type */
+static vector<ST *> do_index_st;		/* User do index variable ST */
+static vector<TYPE_ID> do_index_type;	/* User do index variable type */
 static BOOL fast_doacross;	/* Flag if doacross meets fastpath requirement*/
 
 static INT32 copyin_count;	/* Count of copyins */
@@ -553,6 +554,8 @@
 static WN *copyin_nodes;	/* Points to (optional) copyin nodes */
 static WN *copyin_nodes_end;	/* Points to (optional) copyin nodes end */
 static WN *if_node;		/* Points to (optional) if node */
+static BOOL untied;             /* untied flag */
+static UINT32 collapse_count;   /* collapse count */
 static WN *lastlocal_nodes;	/* Points to (optional) lastlocal nodes */
 static WN *lastthread_node;	/* Points to (optional) lastthread node */
 static WN *local_nodes;		/* Points to (optional) local nodes */
@@ -677,6 +680,7 @@
   "__mp_parallel_do",		/* MPR_PARALLEL_DO_32 */
   "__mp_parallel_do_64",	/* MPR_PARALLEL_DO_64 */
   "__mp_region",		/* MPR_PARALLEL_REGION */
+  "__mp_task_region",		/* MPR_TASK_REGION */
   "__mp_begin_pdo",		/* MPR_BEGIN_PDO_32 */
   "__mp_begin_pdo_64",		/* MPR_BEGIN_PDO_64 */
   "__mp_next_iters",		/* MPR_NEXT_ITERS_32 */
@@ -738,13 +742,16 @@
   "__ompc_end_ordered",         /* MPR_OMP_ORDERED */
   "__ompc_flush",               /* MPR_OMP_FLUSH  */
   "__ompc_task_wait",           /* MPR_OMP_TASKWAIT */
-  "__ompc_task",                /* MPR_OMP_TASK */
-  "__ompc_end_task",            /* MPR_OMP_END_TASK */
+  "__ompc_task_create",         /* MPR_OMP_TASK_CREATE */
+  "__ompc_task_body_start",     /* MPR_OMP_TASK_BODY_START */
+  "__ompc_task_exit",           /* MPR_OMP_TASK_EXIT */
 #ifdef KEY
   "__ompc_get_thdprv",          /* MPR_OMP_GET_THDPRV */
   "__ompc_copyin_thdprv",       /* MPR_OMP_COPYIN_THDPRV */
   "__ompc_copyprivate",         /* MPR_OMP_COPYPRIVATE */
 #endif
+  "__ompc_collapse_init",       /* MPR_OMP_COLLAPSE_INIT */
+  "__ompc_collapse_next",       /* MPR_OMP_COLLAPSE_NEXT */
 };
 
 
@@ -765,6 +772,7 @@
   ST_IDX_ZERO,	 /* MPR_PARALLEL_DO_32 */
   ST_IDX_ZERO,	 /* MPR_PARALLEL_DO_64 */
   ST_IDX_ZERO,	 /* MPR_PARALLEL_REGION */
+  ST_IDX_ZERO,	 /* MPR_TASK_REGION */
   ST_IDX_ZERO,	 /* MPR_BEGIN_PDO_32 */
   ST_IDX_ZERO,	 /* MPR_BEGIN_PDO_64 */
   ST_IDX_ZERO,	 /* MPR_NEXT_ITERS_32 */
@@ -826,8 +834,9 @@
   ST_IDX_ZERO,   /* MPR_OMP_END_ORDERED */
   ST_IDX_ZERO,   /* MPR_OMP_FLUSH */
   ST_IDX_ZERO,   /* MPR_OMP_TASKWAIT */
-  ST_IDX_ZERO,   /* MPR_OMP_TASK */
-  ST_IDX_ZERO,   /* MPR_OMP_END_TASK */
+  ST_IDX_ZERO,   /* MPR_OMP_TASK_CREATE */
+  ST_IDX_ZERO,   /* MPR_OMP_TASK_BODY_START */
+  ST_IDX_ZERO,   /* MPR_OMP_TASK_EXIT */
 #ifdef KEY
   ST_IDX_ZERO,   /* MPR_OMP_GET_THDPRV */
   ST_IDX_ZERO,   /* MPR_OMP_COPYIN_THDPRV */
@@ -1166,6 +1175,86 @@
 }
 
 /*
+Generate RT calls to create task.
+*/
+static WN *
+Gen_Task_Create (ST *proc) 
+{
+  WN * wn;
+  WN * wnx;
+  wn = WN_Create(OPC_VCALL, 4 );
+  WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_TASK_CREATE);
+
+  WN_Set_Call_Non_Data_Mod(wn);
+  WN_Set_Call_Non_Data_Ref(wn);
+  WN_Set_Call_Non_Parm_Mod(wn);
+  WN_Set_Call_Non_Parm_Ref(wn);
+  WN_Set_Call_Parm_Ref(wn);
+  WN_linenum(wn) = line_number;
+
+  wnx = WN_Lda( Pointer_type, 0, proc);
+  WN_kid(wn, 0) = WN_CreateParm(Pointer_type, wnx, 
+                       WN_ty(wnx), WN_PARM_BY_REFERENCE);
+  WN *link = WN_LdidPreg( Pointer_type, Frame_Pointer_Preg_Offset);
+  WN_kid(wn, 1) = WN_CreateParm(Pointer_type, link, WN_ty(link),
+                     WN_PARM_BY_REFERENCE);
+
+  WN *may_delay_wn;
+  if (if_node)
+    may_delay_wn = WN_COPY_Tree(WN_kid0(if_node));
+  else
+    may_delay_wn = WN_Intconst(MTYPE_I4, 1);
+  WN_kid(wn, 2) = WN_CreateParm(MTYPE_I4, may_delay_wn, MTYPE_To_TY(MTYPE_I4), WN_PARM_BY_VALUE);
+
+  WN_kid(wn, 3) = WN_CreateParm(MTYPE_I4, WN_Intconst(MTYPE_I4, untied ? 0 : 1), MTYPE_To_TY(MTYPE_I4), WN_PARM_BY_VALUE);
+
+  return wn;
+}
+
+/*
+Generate RT call to start the body of a task.
+*/
+static WN *
+Gen_Task_Body_Start ()
+{
+  WN * wn;
+  wn = WN_Create(OPC_VCALL, 0 );
+  WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_TASK_BODY_START);
+  return wn;
+}
+
+
+/*
+Generate RT call to terminate a task.
+*/
+static WN *
+Gen_Task_Exit ()
+{
+  WN * wn;
+  wn = WN_Create(OPC_VCALL, 0 );
+  WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_TASK_EXIT);
+  return wn;
+}
+
+/*
+Insert RT calls to terminate a task before any return statement.
+*/
+static void
+Insert_Task_Exits (WN *node)
+{
+  if (WN_operator(node) == OPR_BLOCK) {
+    for (WN *stmt = WN_first(node); stmt; stmt = WN_next(stmt)) {
+      if (WN_operator(stmt) == OPR_RETURN)
+        WN_INSERT_BlockBefore(node, stmt, Gen_Task_Exit());
+      Insert_Task_Exits(stmt);
+    }
+  } else {
+    for (UINT i = 0; i < WN_kid_count(node); i++)
+      Insert_Task_Exits(WN_kid(node, i));
+  }
+}
+
+/*
 Generate RT calls to initialize RTL. This call should be invocated at the 
 beginning of the program. For Profiling only. obsoleted.
 */
@@ -1429,6 +1518,8 @@
 static WN *Gen_Master( ST *gtid );
 static void Create_Preg_or_Temp ( TYPE_ID mtype, char *name, ST **st,
                                   WN_OFFSET *ofst );
+static void Create_Preg_or_Temp ( TYPE_ID mtype, char *name, UINT32 id, ST **st,
+                                  WN_OFFSET *ofst );
 static WN * Gen_MP_Load ( ST * st, WN_OFFSET offset, BOOL scalar_only = FALSE );
 static WN * Gen_Barrier ( ST* gtid );
 static WN * Gen_EBarrier ( ST* gtid);
@@ -1438,11 +1529,11 @@
 * Generate RT calls to start critical section
 */
 static WN *
-Gen_Critical (ST *gtid, ST *lck, BOOL isReduction)
+Gen_Critical (ST *gtid, ST *lck, BOOL is_reduction)
 {
   WN *wn = WN_Create(OPC_VCALL, 2);
 
-  if(isReduction)
+  if(is_reduction)
    WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_REDUCTION);
   else
     WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_CRITICAL);
@@ -1519,13 +1610,13 @@
 * Generate RT calls to end critical section
 */
 static WN *
-Gen_End_Critical (ST *gtid, ST *lck, BOOL isReduction=FALSE)
+Gen_End_Critical (ST *gtid, ST *lck, BOOL is_reduction=FALSE)
 {
   WN *wn = WN_Create(OPC_VCALL, 2);
-  if(isReduction)
-  WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_END_REDUCTION);
+  if(is_reduction)
+    WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_END_REDUCTION);
   else
-  WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_END_CRITICAL);
+    WN_st_idx(wn) = GET_MPRUNTIME_ST(MPR_OMP_END_CRITICAL);
   
   WN_Set_Call_Non_Data_Mod(wn);
   WN_Set_Call_Non_Data_Ref(wn);
@@ -1994,8 +2085,12 @@
 static void 
 Init_PU_Globals( )
 {
-
   local_gtid = NULL;
+  if (PU_mp(PU_Info_pu(Current_PU_Info))) {
+    WN* func_entry = PU_Info_tree_ptr(Current_PU_Info);
+    if (WN_num_formals(func_entry) == 2)
+      local_gtid = WN_st(WN_formal(func_entry, 0));
+  }
 }
 
 /*
@@ -2323,6 +2418,26 @@
   *st = new_st;
 }
 
+/*
+Because sometimes we may merely need a temp var, not a preg, e.g. the var
+to accept a value as the function's formal parameter.
+So, this function just allocate a temp var in current scope.
+*/
+static void 
+Create_Temp( TYPE_ID mtype, const char *name, UINT32 id, ST **st )
+{
+  ST *new_st;
+  new_st = New_ST (CURRENT_SYMTAB);
+  ST_Init (new_st,
+           Save_Str2i ( "__ompv_temp_", name, id),
+           CLASS_VAR,
+           SCLASS_AUTO,
+           EXPORT_LOCAL,
+           MTYPE_To_TY (mtype));
+  Set_ST_is_temp_var ( new_st );
+  *st = new_st;
+}
+
 #ifdef KEY
 // If old_st_idx has already been localized, return the new
 // st_idx. Else, create a new local symbol and return its st_idx.
@@ -2542,24 +2657,30 @@
 static void 
 Create_MicroTask ( PAR_FUNC_TYPE func_type )
 {
-  BOOL is_do32 = FALSE, is_do64 = FALSE, is_region = FALSE;
+  BOOL is_do32 = FALSE, is_do64 = FALSE, is_region = FALSE, has_gtid = FALSE;
   switch (func_type) {  // validate input, set type flag
   case PAR_FUNC_DO32:
     is_do32 = TRUE;
+    has_gtid = TRUE;
     break;
   case PAR_FUNC_DO64:
     is_do64 = TRUE;
+    has_gtid = TRUE;
     break;
   case PAR_FUNC_REGION:
     is_region = TRUE;
+    has_gtid = TRUE;
     break;
+  case PAR_FUNC_TASK:
+    is_region = TRUE;
+    break;
   default:
     Fail_FmtAssertion("invalid parallel function type %d", (INT) func_type);
     break;
   }
 
     // should be merged up after done. Currently reserved for Debug
-  const char *construct_type_str = is_region ? "ompregion" : "ompdo";
+  const char *construct_type_str = is_region ? "omprg" : "ompdo";
   char temp_str[64];
 
 
@@ -2581,10 +2702,9 @@
     Set_TYLIST_type(parm_list, Be_Type_Tbl(MTYPE_V));  // return type
 
       // Two basic parameters
-    Set_TYLIST_type(New_TYLIST(parm_idx), // gtid
-                      Be_Type_Tbl(Pointer_type));
-    Set_TYLIST_type(New_TYLIST(parm_idx), // btid
-                      Be_Type_Tbl(Pointer_type));
+    if (has_gtid)
+      Set_TYLIST_type(New_TYLIST(parm_idx), Be_Type_Tbl(Pointer_type)); // gtid
+    Set_TYLIST_type(New_TYLIST(parm_idx), Be_Type_Tbl(Pointer_type)); // btid
 
     Set_TYLIST_type(New_TYLIST(parm_idx), TY_IDX_ZERO); // end of parm list
 
@@ -2613,10 +2733,10 @@
   const char *old_st_name = ST_name(PU_Info_proc_sym(Current_PU_Info));
   char *st_name = (char *) alloca(strlen(old_st_name) + 32);
   if (mp_construct_num == 0)
-    sprintf ( st_name, "__%s_%s%d", construct_type_str, old_st_name,
+    sprintf ( st_name, "__%s_%s_%d", construct_type_str, old_st_name,
 	      mp_region_num );
   else
-    sprintf ( st_name, "__%s_%s%d.%d", construct_type_str, old_st_name,
+    sprintf ( st_name, "__%s_%s_%d.%d", construct_type_str, old_st_name,
 	      mp_region_num, mp_construct_num );
 
 
@@ -2640,6 +2760,7 @@
   Set_PU_no_inline(pu);
   Set_PU_is_nested_func(pu);
   Set_PU_mp(pu);
+  Set_PU_has_mp(pu);
 #ifdef KEY
   Set_PU_mp_lower_generated(pu);
 #endif // KEY
@@ -2695,14 +2816,17 @@
 
     // create ST's for parameters
 
-  ST *arg_gtid = New_ST( CURRENT_SYMTAB );
-  ST_Init (arg_gtid,
+  ST *arg_gtid = NULL;
+  if (has_gtid) {
+    arg_gtid = New_ST( CURRENT_SYMTAB );
+    ST_Init (arg_gtid,
              Save_Str ( "__ompv_gtid_a" ),
              CLASS_VAR,
              SCLASS_FORMAL,
              EXPORT_LOCAL,
              Be_Type_Tbl(MTYPE_I4));
-  Set_ST_is_value_parm( arg_gtid );
+    Set_ST_is_value_parm( arg_gtid );
+  }
 
   ST *arg_slink = New_ST( CURRENT_SYMTAB );
   ST_Init( arg_slink,
@@ -2737,12 +2861,16 @@
   }
 #endif
   // Currently, don't pass data via arguments.
-  WN *func_entry = WN_CreateEntry ( 2, parallel_proc,
+  UINT arg_cnt = (has_gtid ? 2 : 1);
+  UINT slink_arg_pos = arg_cnt - 1;
+  WN *func_entry = WN_CreateEntry ( arg_cnt, parallel_proc,
                                     parallel_func, WN_CreateBlock ( ),
 				    reference_block );
 
-  WN_kid0(func_entry) = WN_CreateIdname ( 0, arg_gtid );
-  WN_kid1(func_entry) = WN_CreateIdname ( 0, arg_slink );
+  if (has_gtid) {
+    WN_kid0(func_entry) = WN_CreateIdname ( 0, arg_gtid );
+  }
+  WN_kid(func_entry, slink_arg_pos) = WN_CreateIdname ( 0, arg_slink );
      // TODO: variable arguments list should be added here.
 
   WN_linenum(func_entry) = line_number;
@@ -2762,15 +2890,17 @@
   // of gtid in another temp vars.
   // The gtid can be eniminated, since the symbol of arguments are also 
   // local storage.
-  Create_Temp( MTYPE_I4, "gtid", &local_gtid );
-  WN *wn_store_gtid = Gen_MP_Store( local_gtid, 0,
-      WN_Ldid( MTYPE_I4, 0, arg_gtid, ST_type(arg_gtid), 0 ));
-//      WN_IloadLdid( MTYPE_I4, 0, Be_Type_Tbl( MTYPE_I4 ), arg_gtid, 0 ));
-  WN_linenum( wn_store_gtid ) = line_number;
-  WN_INSERT_BlockLast( parallel_func, wn_store_gtid );    
+  if (has_gtid) {
+    Create_Temp( MTYPE_I4, "gtid", &local_gtid );
+    WN *wn_store_gtid = Gen_MP_Store( local_gtid, 0,
+                                      WN_Ldid( MTYPE_I4, 0, arg_gtid, ST_type(arg_gtid), 0 ));
+    //      WN_IloadLdid( MTYPE_I4, 0, Be_Type_Tbl( MTYPE_I4 ), arg_gtid, 0 ));
+    WN_linenum( wn_store_gtid ) = line_number;
+    WN_INSERT_BlockLast( parallel_func, wn_store_gtid );    
+  }
+
   // create PU_Info for nested function
 
-
   PU_Info *parallel_pu = TYPE_MEM_POOL_ALLOC ( PU_Info, Malloc_Mem_Pool );
   PU_Info_init ( parallel_pu );
   Set_PU_Info_tree_ptr (parallel_pu, func_entry );
@@ -2849,7 +2979,8 @@
   Current_pu = &Current_PU_Info_pu();
   Current_Map_Tab = pmaptab;
 
-  Add_DST_variable ( arg_gtid, nested_dst, line_number, DST_INVALID_IDX );
+  if (has_gtid)
+    Add_DST_variable ( arg_gtid, nested_dst, line_number, DST_INVALID_IDX );
   Add_DST_variable ( arg_slink, nested_dst, line_number, DST_INVALID_IDX );
 
 }
@@ -2880,7 +3011,32 @@
   }
 }
 
+/*  Create either a preg or a temp depending on presence of C++ exception
+    handling.  */
 
+static void 
+Create_Preg_or_Temp ( TYPE_ID mtype, const char *name, UINT32 id, ST **st,
+				  WN_OFFSET *ofst )
+{
+  ST *new_st;
+
+  if (!pu_has_eh) {
+    *st = MTYPE_To_PREG ( mtype );
+    *ofst = Create_Preg (mtype, name);
+  } else {
+    new_st = New_ST (CURRENT_SYMTAB);
+    ST_Init (new_st,
+             Save_Str2i ( "__ompv_temp_", name, id ),
+             CLASS_VAR,
+             SCLASS_AUTO,
+             EXPORT_LOCAL,
+             MTYPE_To_TY (mtype));
+    Set_ST_is_temp_var ( new_st );
+    *st = new_st;
+    *ofst = 0;
+  }
+}
+
 /*
 If tree is the test for whether a thread is the MASTER, return TRUE, else
 return FALSE.
@@ -3075,7 +3231,69 @@
     Enter_Guarded_WNs(guarded_set, nodes_in_critsect[i], TRUE);
 }
 
+static WN_PRAGMA_ACCESSED_FLAGS Get_Access_Flag(WN* node, WN* parent, WN* grandparent)
+{
+  WN_PRAGMA_ACCESSED_FLAGS flags;
+  OPERATOR opr = WN_operator(node);
+  ST* st = WN_st(node);
+  BOOL a_pointer = (TY_kind(ST_type(st)) == KIND_POINTER);
 
+  if (opr == OPR_LDID && !a_pointer)
+    flags = ACCESSED_LOAD;
+  else if (opr == OPR_STID)
+    flags = ACCESSED_STORE;
+  else if ((opr == OPR_LDA) && !a_pointer)
+    if (WN_operator(parent) == OPR_ILOAD)
+      flags = ACCESSED_LOAD;
+    else if (WN_operator(parent) == OPR_ISTORE)
+      flags = ACCESSED_STORE;
+    else if (WN_operator(parent) == OPR_ARRAY)
+      if (WN_operator(grandparent) == OPR_ILOAD)
+        flags = ACCESSED_LOAD;
+      else if (WN_operator(grandparent) == OPR_ISTORE)
+        flags = ACCESSED_STORE;
+      else
+        flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                            ACCESSED_STORE |
+                                            ACCESSED_ILOAD |
+                                            ACCESSED_ISTORE);
+    else
+      flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                          ACCESSED_STORE |
+                                          ACCESSED_ILOAD |
+                                          ACCESSED_ISTORE);
+  else if ((opr == OPR_LDID) && a_pointer)
+    if (WN_operator(parent) == OPR_ILOAD)
+      flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                          ACCESSED_ILOAD);
+    else if (WN_operator(parent) == OPR_ISTORE)
+      flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                          ACCESSED_ISTORE);
+    else if (WN_operator(parent) == OPR_ARRAY)
+      if (WN_operator(grandparent) == OPR_ILOAD)
+        flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                            ACCESSED_ILOAD);
+      else if (WN_operator(grandparent) == OPR_ISTORE)
+        flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                            ACCESSED_ISTORE);
+      else
+        flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                            ACCESSED_STORE |
+                                            ACCESSED_ILOAD |
+                                            ACCESSED_ISTORE);
+    else
+      flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                          ACCESSED_STORE |
+                                          ACCESSED_ILOAD |
+                                          ACCESSED_ISTORE);
+  else
+    flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
+                                        ACCESSED_STORE |
+                                        ACCESSED_ILOAD |
+                                        ACCESSED_ISTORE);
+  return flags;
+}
+
 /*  Walk tree to locate all uplevel references and build list of them.
     guarded_set is the set of WNs guarded by SINGLE or MASTER constructs;
     guared stores to non-SHARED STs do not elicit a warning message. */
@@ -3092,7 +3310,6 @@
   OPCODE op;
   OPERATOR opr;
 
-  BOOL a_pointer;
   WN_PRAGMA_ACCESSED_FLAGS flags;
 
   Is_True(level >= 2, ("impossible symtab level == %d", level));
@@ -3119,62 +3336,7 @@
     if (OPCODE_has_sym(op) && (st = WN_st(tree)) != NULL &&
         ST_level(st) < level && ST_class(st) == CLASS_VAR) {
 
-      a_pointer = (TY_kind(ST_type(st)) == KIND_POINTER);
-
-      if (opr == OPR_LDID && !a_pointer)
-         flags = ACCESSED_LOAD;
-      else if (opr == OPR_STID)
-         flags = ACCESSED_STORE;
-      else if ((opr == OPR_LDA) && !a_pointer)
-        if (WN_operator(parent) == OPR_ILOAD)
-           flags = ACCESSED_LOAD;
-        else if (WN_operator(parent) == OPR_ISTORE)
-           flags = ACCESSED_STORE;
-        else if (WN_operator(parent) == OPR_ARRAY)
-	        if (WN_operator(grandparent) == OPR_ILOAD)
-             flags = ACCESSED_LOAD;
-          else if (WN_operator(grandparent) == OPR_ISTORE)
-             flags = ACCESSED_STORE;
-          else
-            flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-                     ACCESSED_STORE |
-                     ACCESSED_ILOAD |
-                     ACCESSED_ISTORE);
-        else
-           flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-					      ACCESSED_STORE |
-					      ACCESSED_ILOAD |
-					      ACCESSED_ISTORE);
-      else if ((opr == OPR_LDID) && a_pointer)
-	      if (WN_operator(parent) == OPR_ILOAD)
-          flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-					      ACCESSED_ILOAD);
-        else if (WN_operator(parent) == OPR_ISTORE)
-          flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-					      ACCESSED_ISTORE);
-        else if (WN_operator(parent) == OPR_ARRAY)
-          if (WN_operator(grandparent) == OPR_ILOAD)
-             flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-                      ACCESSED_ILOAD);
-          else if (WN_operator(grandparent) == OPR_ISTORE)
-             flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-                      ACCESSED_ISTORE);
-        else
-             flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-                      ACCESSED_STORE |
-                      ACCESSED_ILOAD |
-                      ACCESSED_ISTORE);
-      else
-        flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-            ACCESSED_STORE |
-            ACCESSED_ILOAD |
-            ACCESSED_ISTORE);
-    else
-        flags = (WN_PRAGMA_ACCESSED_FLAGS) (ACCESSED_LOAD |
-					    ACCESSED_STORE |
-					    ACCESSED_ILOAD |
-					    ACCESSED_ISTORE);
-
+      flags = Get_Access_Flag(tree, parent, grandparent);
       wn = WN_first(block);
       while (wn && (WN_st(wn) < st))
           wn = WN_next(wn);
@@ -3242,7 +3404,63 @@
   }
 }
 
+static BOOL
+St_Is_In_List(WN* node_list, ST* st)
+{
+  for (WN* wn = node_list; wn; wn = WN_next(wn))
+    if (WN_st_idx(wn) == ST_st_idx(st))
+      return TRUE;
+  return FALSE;
+}
 
+/*  Walk tree to locate all variable references which storage is not
+    specified, and add them in node_list. */
+
+static void 
+Collect_Default_Variables (  WN * tree, WN *& node_list, WN_PRAGMA_ID pragma_id, WN * parent = NULL, WN * grandparent = NULL )
+{
+  WN *wn;
+  ST *st;
+  OPCODE op;
+  OPERATOR opr;
+  BOOL a_pointer;
+  WN_PRAGMA_ACCESSED_FLAGS flags;
+
+  if (tree) {
+
+    op = WN_opcode(tree);
+    opr = WN_operator(tree);
+
+    if (op == OPC_BLOCK)
+      for (WN* node = WN_first(tree); node; node = WN_next(node))
+	Collect_Default_Variables(node, node_list, pragma_id, tree, parent);
+    else
+      for (INT32 i = 0; i < WN_kid_count(tree); i++)
+	Collect_Default_Variables(WN_kid(tree, i), node_list, pragma_id, tree, parent);
+
+    if (OPCODE_has_sym(op) && (st = WN_st(tree)) != NULL &&
+        ST_class(st) == CLASS_VAR && ST_level(st) == PU_lexical_level(Current_PU_Info_pu())) {
+      if ((local_nodes == node_list || !St_Is_In_List(local_nodes, st)) && 
+          (shared_nodes == node_list || !St_Is_In_List(shared_nodes, st)) &&
+          (firstprivate_nodes == node_list || !St_Is_In_List(firstprivate_nodes, st))) {
+        flags = Get_Access_Flag(tree, parent, grandparent);
+        for (wn = node_list; wn; wn = WN_next(wn))
+          if (WN_st_idx(wn) == ST_st_idx(st))
+            break;
+        if (wn) {
+          WN_pragma_flags(wn) |= flags;
+        } else {
+          wn = WN_CreatePragma(pragma_id, st, 0, 0);
+          WN_pragma_flags(wn) = flags;
+          WN_next(wn) = node_list;
+          node_list = wn;
+        }
+      }
+    }
+  }
+}
+
+
 /*  Walk tree gathering information about all interesting preg usage.  */
 
 static void 
@@ -3639,6 +3857,7 @@
     case MPP_PDO:
     case MPP_ORPHANED_PDO:
     case MPP_PARALLEL_REGION:
+    case MPP_TASK_REGION:
       break;
     default:
       Fail_FmtAssertion("non-POD object invalidly privatized on construct "
@@ -4380,6 +4599,7 @@
     break;
   case MPP_PARALLEL_DO:
   case MPP_PARALLEL_REGION:
+  case MPP_TASK_REGION:
 #ifdef KEY
   case MPP_ORPHAN:
 #endif
@@ -4656,6 +4876,7 @@
   case MPP_PDO:
   case MPP_PARALLEL_DO:
   case MPP_PARALLEL_REGION:
+  case MPR_TASK_REGION:
   case MPP_ORPHAN:
     break;
   default:
@@ -5156,6 +5377,25 @@
 }   
 
 
+static void
+Extract_Index_Info ( WN* pdo_node )
+{
+  do_index_st.clear();
+  do_index_type.clear();
+  for (UINT32 i = 0; i < collapse_count; i++) {
+    ST * tmp_do_index_st = WN_st(WN_index(pdo_node));
+    TYPE_ID tmp_do_index_type = TY_mtype(ST_type(tmp_do_index_st));
+    if (tmp_do_index_type == MTYPE_I1 || tmp_do_index_type == MTYPE_I2)
+      tmp_do_index_type = MTYPE_I4;
+    else if (tmp_do_index_type == MTYPE_U1 || tmp_do_index_type == MTYPE_U2)
+      tmp_do_index_type = MTYPE_U4;
+    do_index_st.push_back(tmp_do_index_st);
+    do_index_type.push_back(tmp_do_index_type);
+    pdo_node = WN_first(WN_do_body(pdo_node));
+  }
+}
+
+
 /*
 * Create any needed temporaries in local scope. especially for Dos 
 * This function has been modified by csc.
@@ -5165,24 +5405,49 @@
 static void 
 Make_Local_Temps ( void )
 {
+  limit_st.clear();
+  base_st.clear();
+  limit_ofst.clear();
+  base_ofst.clear();
+  local_upper.clear();
+  local_lower.clear();
+  local_stride.clear();
+
+  for (UINT32 i = 0; i < collapse_count; i++) {
+    ST *tmp_limit_st;
+    ST *tmp_base_st;
+    WN_OFFSET tmp_limit_ofst;
+    WN_OFFSET tmp_base_ofst;
+    ST *tmp_local_upper;
+    ST *tmp_local_lower;
+    ST *tmp_local_stride;
+
     // note that if multiple orphaned PDO's appear in a PU, then multiple
     // instances of these local variables will be inserted into the PU's
     // symbol table
-  //Maybe need to be changed to Create_Temp
-  Create_Preg_or_Temp( do_index_type, "temp_limit", &limit_st, &limit_ofst );
+    //Maybe need to be changed to Create_Temp
+    Create_Preg_or_Temp( do_index_type[0], "temp_limit", i, &tmp_limit_st, &tmp_limit_ofst );
+    limit_st.push_back(tmp_limit_st);
+    limit_ofst.push_back(tmp_limit_ofst);
 
-  Create_Temp( do_index_type, "do_upper", &local_upper );
-  Set_ST_addr_passed( local_upper );
+    Create_Preg_or_Temp( do_index_type[0], "temp_base", i, &tmp_base_st, &tmp_base_ofst );
+    base_st.push_back(tmp_base_st);
+    base_ofst.push_back(tmp_base_ofst);
 
-  Create_Temp( do_index_type, "do_lower", &local_lower );
-  Set_ST_addr_passed( local_lower );
+    Create_Temp( do_index_type[i], "do_upper", i, &tmp_local_upper );
+    Set_ST_addr_passed( tmp_local_upper );
+    local_upper.push_back(tmp_local_upper);
 
-  Create_Temp( do_index_type, "do_stride", &local_stride );
-  Set_ST_addr_passed( local_stride );
+    Create_Temp( do_index_type[i], "do_lower", i, &tmp_local_lower );
+    Set_ST_addr_passed( tmp_local_lower );
+    local_lower.push_back(tmp_local_lower);
 
+    Create_Temp( do_index_type[i], "do_stride", i, &tmp_local_stride );
+    Set_ST_addr_passed( tmp_local_stride );
+    local_stride.push_back(tmp_local_stride);
+  }
   Create_Temp( MTYPE_I4, "last_iter", &last_iter );
   Set_ST_addr_passed( last_iter );
-
 }
 
 
@@ -6901,7 +7166,7 @@
       ST *mplock = Create_Critical_Lock( );
       WN_INSERT_BlockFirst( *store_block, 
 	                  Gen_Critical( local_gtid, mplock, TRUE ));
-      unlock = Gen_End_Critical ( local_gtid, mplock,TRUE );
+      unlock = Gen_End_Critical ( local_gtid, mplock, TRUE );
       WN_INSERT_BlockLast( *store_block, unlock );
       // Does this barrier necessary?
       WN_INSERT_BlockLast( *store_block, Gen_Barrier(local_gtid));
@@ -7598,9 +7863,7 @@
   WN_INSERT_BlockLast( mp_master_block, if_wn );
 
   // end_master call.
-// #ifndef KEY
   WN_INSERT_BlockLast( mp_master_block, Gen_End_Master(local_gtid));
-// #endif
 
   return mp_master_block;
 } // Lower_Master()
@@ -7837,10 +8100,10 @@
           lock_stack[lptr++] = lock_st;
           if(lock_st){
             Linenum_Pusher p(WN_Get_Linenum(kid));
-            new_kid = Gen_Critical( local_gtid, lock_st,FALSE ); /*oscar attempt 1*/
+            new_kid = Gen_Critical( local_gtid, lock_st, FALSE ); /*oscar attempt 1*/
           } else {
             Linenum_Pusher p(WN_Get_Linenum(kid));
-            new_kid = Gen_Critical( local_gtid, unnamed_lock_st,FALSE );
+            new_kid = Gen_Critical( local_gtid, unnamed_lock_st, FALSE );
           }
           if (WN_prev(kid))
             WN_next(WN_prev(kid)) = new_kid;
@@ -8008,8 +8271,10 @@
 #ifdef KEY /* Bug 4828 */
           if (region_type != WN_PRAGMA_PWORKSHARE_BEGIN)
 #endif
-  	  Strip_Nested_MP ( WN_region_body(kid), FALSE );
-     } else {
+  	  Strip_Nested_MP ( WN_region_body(kid), TRUE );
+        } else if (region_type == WN_PRAGMA_TASK_BEGIN) {
+          /* do nothing */
+        } else {
 	  if (region_type == WN_PRAGMA_PDO_BEGIN)
 	    Move_Non_POD_Finalization_Code(WN_region_body(kid));
 
@@ -8677,9 +8942,9 @@
           lock_stack[lptr++] = lock_st;
           Create_Gtid_ST( );
           if (lock_st)
-            new_kid = Gen_Critical( local_gtid, lock_st,FALSE );
+            new_kid = Gen_Critical( local_gtid, lock_st, FALSE );
           else
-            new_kid = Gen_Critical( local_gtid, unnamed_lock_st,FALSE );
+            new_kid = Gen_Critical( local_gtid, unnamed_lock_st, FALSE );
           WN_prev(new_kid) = prev_kid;
           if (prev_kid)
             WN_next(prev_kid) = new_kid;
@@ -8859,7 +9124,7 @@
 */
 
 static void 
-Extract_Do_Info ( WN * do_tree )
+Extract_Per_Level_Do_Info ( WN * do_tree, UINT32 level_id )
 {
   // standardize do tree.
   Standardize_Do(do_tree);
@@ -8983,43 +9248,62 @@
 
   /* Generate mp scheduling expressions */
 
-  base_node = do_init;
+  base_node.push_back(do_init);
     // used by Rewrite_Do, need to be copied ?
-  limit_node = WN_COPY_Tree( do_limit );
+  limit_node.push_back(WN_COPY_Tree( do_limit ));
+  stride_node.push_back(do_stride);
 
-  if (((WN_operator(WN_end(do_tree)) == OPR_LT) && was_kid0) ||
-      ((WN_operator(WN_end(do_tree)) == OPR_GT) && !was_kid0)) { 
-    WN* wn_exp0 = WN_Sub(do_index_type, do_limit, WN_COPY_Tree(do_init));
-    wn_exp0 = WN_Integer_Cast(wn_exp0, do_index_type, WN_rtype(wn_exp0));
-    WN* wn_exp1 = WN_Add(do_index_type, wn_exp0, WN_COPY_Tree(do_stride));
-    wn_exp1 = WN_Integer_Cast(wn_exp1, do_index_type, WN_rtype(wn_exp1));
-    WN* wn_exp2 = WN_Sub(do_index_type, wn_exp1, WN_Intconst(do_index_type, 1));
-    wn_exp2 = WN_Integer_Cast(wn_exp2, do_index_type, WN_rtype(wn_exp2));
-    WN* wn_exp3 = WN_Div(do_index_type, wn_exp2, WN_COPY_Tree(do_stride));
-    ntrip_node = wn_exp3; 
-  } else if (((WN_operator(WN_end(do_tree)) == OPR_GT) && was_kid0) ||
-           ((WN_operator(WN_end(do_tree)) == OPR_LT) && !was_kid0)) { 
-    WN* wn_exp0 = WN_Sub(do_index_type, do_limit, WN_COPY_Tree(do_init));
-    wn_exp0 = WN_Integer_Cast(wn_exp0, do_index_type, WN_rtype(wn_exp0));
-    WN* wn_exp1 = WN_Add(do_index_type, wn_exp0, WN_Intconst(do_index_type, 1));
-    wn_exp1 = WN_Integer_Cast(wn_exp1, do_index_type, WN_rtype(wn_exp1));
-    WN* wn_exp2 = WN_Add(do_index_type, wn_exp1, WN_COPY_Tree(do_stride));
-    wn_exp2 = WN_Integer_Cast(wn_exp2, do_index_type, WN_rtype(wn_exp2));
-    WN* wn_exp3 = WN_Div(do_index_type, wn_exp2, WN_COPY_Tree(do_stride));
-    ntrip_node = wn_exp3; 
-  } else { 
-    WN* wn_exp0 = WN_Sub(do_index_type, do_limit, WN_COPY_Tree(do_init));
-    wn_exp0 = WN_Integer_Cast(wn_exp0, do_index_type, WN_rtype(wn_exp0));
-    WN* wn_exp1 = WN_Add(do_index_type, wn_exp0, WN_COPY_Tree(do_stride));
-    wn_exp1 = WN_Integer_Cast(wn_exp1, do_index_type, WN_rtype(wn_exp1));
-    WN* wn_exp2 = WN_Div(do_index_type, wn_exp1, WN_COPY_Tree(do_stride));
-    ntrip_node = wn_exp2; 
-  } 
-  stride_node = do_stride;
+  TYPE_ID current_index_type = do_index_type[level_id];
+  if (collapse_count == 1) {
+    if (((WN_operator(WN_end(do_tree)) == OPR_LT) && was_kid0) ||
+        ((WN_operator(WN_end(do_tree)) == OPR_GT) && !was_kid0)) { 
+      WN* wn_exp0 = WN_Sub(current_index_type, do_limit, WN_COPY_Tree(do_init));
+      wn_exp0 = WN_Integer_Cast(wn_exp0, current_index_type, WN_rtype(wn_exp0));
+      WN* wn_exp1 = WN_Add(current_index_type, wn_exp0, WN_COPY_Tree(do_stride));
+      wn_exp1 = WN_Integer_Cast(wn_exp1, current_index_type, WN_rtype(wn_exp1));
+      WN* wn_exp2 = WN_Sub(current_index_type, wn_exp1, WN_Intconst(current_index_type, 1));
+      wn_exp2 = WN_Integer_Cast(wn_exp2, current_index_type, WN_rtype(wn_exp2));
+      WN* wn_exp3 = WN_Div(current_index_type, wn_exp2, WN_COPY_Tree(do_stride));
+      ntrip_node = wn_exp3; 
+    } else if (((WN_operator(WN_end(do_tree)) == OPR_GT) && was_kid0) ||
+               ((WN_operator(WN_end(do_tree)) == OPR_LT) && !was_kid0)) { 
+      WN* wn_exp0 = WN_Sub(current_index_type, do_limit, WN_COPY_Tree(do_init));
+      wn_exp0 = WN_Integer_Cast(wn_exp0, current_index_type, WN_rtype(wn_exp0));
+      WN* wn_exp1 = WN_Add(current_index_type, wn_exp0, WN_Intconst(current_index_type, 1));
+      wn_exp1 = WN_Integer_Cast(wn_exp1, current_index_type, WN_rtype(wn_exp1));
+      WN* wn_exp2 = WN_Add(current_index_type, wn_exp1, WN_COPY_Tree(do_stride));
+      wn_exp2 = WN_Integer_Cast(wn_exp2, current_index_type, WN_rtype(wn_exp2));
+      WN* wn_exp3 = WN_Div(current_index_type, wn_exp2, WN_COPY_Tree(do_stride));
+      ntrip_node = wn_exp3; 
+    } else { 
+      WN* wn_exp0 = WN_Sub(current_index_type, do_limit, WN_COPY_Tree(do_init));
+      wn_exp0 = WN_Integer_Cast(wn_exp0, current_index_type, WN_rtype(wn_exp0));
+      WN* wn_exp1 = WN_Add(current_index_type, wn_exp0, WN_COPY_Tree(do_stride));
+      wn_exp1 = WN_Integer_Cast(wn_exp1, current_index_type, WN_rtype(wn_exp1));
+      WN* wn_exp2 = WN_Div(current_index_type, wn_exp1, WN_COPY_Tree(do_stride));
+      ntrip_node = wn_exp2; 
+    } 
+  }
 
 }
 
 /*
+* Extract do info for mp scheduling. 
+*/
+
+static void 
+Extract_Do_Info ( WN * do_tree )
+{
+  base_node.clear();
+  limit_node.clear();
+  stride_node.clear();
+  for (UINT32 i = 0; i < collapse_count; i++) {
+    Extract_Per_Level_Do_Info(do_tree, i);
+    do_tree = WN_first(WN_do_body(do_tree));
+  }
+}
+
+/*
 * Rewrite do statement. New version. by csc. 
 * Changes: rewrite do_tree's control vars.
 * Note that, the iteration space is [lower, upper],
@@ -9153,6 +9437,75 @@
   return do_tree;
 } // Rewrite_Do_New()
 
+static void
+Rewrite_Collapsed_Do ( WN * block, WN * do_tree, vector<BOOL>& is_LE )
+{
+  WN * start_block = WN_CreateBlock();
+  WN * cond = NULL;
+  WN * incr_block = NULL;
+  
+  for (UINT32 i = 0; i < collapse_count; i++) {
+    if (i > 0) {
+      do_tree = WN_first(WN_do_body(do_tree));
+    }
+
+    WN        *do_idname  = WN_index(do_tree);
+    ST        *do_id_st   = WN_st(do_idname);
+    WN_OFFSET  do_id_ofst = WN_offsetx(do_idname);
+    UINT       do_id_field_id = WN_field_id(do_idname);
+    
+    ST * lower = local_lower[i];
+    ST * upper = local_upper[i];
+    ST * stride = local_stride[i];
+
+    WN * wn_local_start = Gen_MP_Load(lower, 0);
+    WN * lower_wn = WN_Integer_Cast(wn_local_start, ST_mtype(do_id_st), WN_rtype(wn_local_start));
+    WN * init_wn = WN_Stid(ST_mtype(do_id_st), do_id_ofst, do_id_st, ST_type(do_id_st), lower_wn, do_id_field_id);
+    WN_INSERT_BlockLast(start_block, init_wn);
+
+    WN * wn_local_end = Gen_MP_Load(upper, 0);
+    WN * upper_wn = WN_Integer_Cast(wn_local_end, ST_mtype(do_id_st), WN_rtype(wn_local_end));
+    WN * index_wn = WN_Ldid(ST_mtype(do_id_st), do_id_ofst, do_id_st, ST_type(do_id_st), do_id_field_id);
+    WN * cond_wn = WN_NE(ST_mtype(do_id_st), index_wn, upper_wn);
+    if (cond == NULL) {
+      cond = cond_wn;
+    } else {
+      cond = WN_LIOR(cond_wn, cond);
+    }
+
+    WN * current_incr_block = WN_CreateBlock();
+    WN * next_index_wn = WN_Add(ST_mtype(do_id_st), WN_COPY_Tree(index_wn), WN_COPY_Tree(stride_node[i]));
+    WN * incr_wn = WN_Stid(ST_mtype(do_id_st), do_id_ofst, do_id_st, ST_type(do_id_st), next_index_wn, do_id_field_id);
+    WN_INSERT_BlockLast(current_incr_block, incr_wn);
+    if (incr_block != NULL) {
+      WN * limit_wn = WN_Integer_Cast(WN_Ldid(ST_mtype(limit_st[i]), limit_ofst[i], limit_st[i], ST_type(limit_st[i]), 0), ST_mtype(do_id_st), ST_mtype(limit_st[i]));
+      WN * overflow_cond;
+      if (is_LE[i]) {
+        overflow_cond = WN_GT(ST_mtype(do_id_st), WN_COPY_Tree(index_wn), limit_wn);
+      } else {
+        overflow_cond = WN_LT(ST_mtype(do_id_st), WN_COPY_Tree(index_wn), limit_wn);
+      }
+
+      WN * base_wn = WN_Integer_Cast(WN_Ldid(ST_mtype(base_st[i]), base_ofst[i], base_st[i], ST_type(base_st[i]), 0), ST_mtype(do_id_st), ST_mtype(base_st[i]));
+      WN * reset_index_wn = WN_Stid(ST_mtype(do_id_st), do_id_ofst, do_id_st, ST_type(do_id_st), base_wn, do_id_field_id);
+      WN_INSERT_BlockLast(incr_block, reset_index_wn);
+
+      WN * overflow_test = WN_CreateIf(overflow_cond, incr_block, WN_CreateBlock());
+      WN_INSERT_BlockLast(current_incr_block, overflow_test);
+    }
+    incr_block = current_incr_block;
+  }
+
+  WN * loop_body = WN_do_body(do_tree);
+  WN_do_body(do_tree) == NULL;
+
+  WN_INSERT_BlockLast(block, start_block);
+
+  WN * while_loop = WN_CreateWhileDo(cond, loop_body);
+  WN_INSERT_BlockLast(loop_body, incr_block);
+  WN_INSERT_BlockLast(block, while_loop);
+}
+
 /*
 Transform do statement. This is a new version
 written by csc.
@@ -9161,22 +9514,18 @@
 contained in other trees..
 */
 
-static WN *
+static inline WN *
 Transform_Do( WN * do_tree, 
-                         SCHEDULE_TYPE schedule, 
-                         WN * chunk_size)
+              SCHEDULE_TYPE schedule, 
+              WN * chunk_size)
 {
   WN        *wn, *wn_tmp;
-  WN        *do_idname  = WN_index(do_tree);
-  ST        *do_id_st   = WN_st(do_idname);
-  WN_OFFSET  do_id_ofst = WN_offsetx(do_idname);
-  WN        *do_stride;
+  vector<WN*> do_stride(collapse_count);
   WN        *do_schedule;
   WN	      *if_wn;
   WN        *test_wn, *then_wn, *else_wn;
   WN        *while_wn;
   WN        *while_test, *while_body;
-//  WN        *wn_prev_do, *wn_next_do;
 
   WN        *call_wn;
   
@@ -9187,15 +9536,9 @@
   WN_OFFSET return_ofst;
   WN        *return_wn = WN_CreateBlock( );
 
-  // Now, move to global scope.
-//  ST        *limit_st;
-//  WN_OFFSET  limit_ofst;
-
-//  WN_OFFSET  temp_ofst;
   WN        *do_init_call = NULL;
   BOOL       is_do32 = TRUE;
-  BOOL       is_LE = TRUE;
-  BOOL       is_kid0_do_id = TRUE;
+  vector<BOOL> is_LE(collapse_count, TRUE);
 
   is_do32 = fast_doacross ? TRUE : FALSE;
 //  wn_prev_do = WN_prev( do_tree );
@@ -9207,103 +9550,104 @@
   // as the local ones. So, we have to do works in Extra_Do_Info again.
   // Maybe we should modify Extra_Do_Info instead.
 
-  if ((WN_operator(WN_kid0(WN_kid0(WN_step(do_tree)))) == OPR_LDID) &&
-      (WN_st(WN_kid0(WN_kid0(WN_step(do_tree)))) == do_id_st) &&
-      (WN_offsetx(WN_kid0(WN_kid0(WN_step(do_tree)))) == do_id_ofst))
-  {
-    do_stride = WN_kid1(WN_kid0(WN_step(do_tree)));
+  do_prefix = WN_CreateBlock( );
+  do_suffix = WN_CreateBlock( );
+  WN *current_do = do_tree;
+  for (UINT32 i = 0; i < collapse_count; i++) {
+    WN        *do_idname  = WN_index(current_do);
+    ST        *do_id_st   = WN_st(do_idname);
+    WN_OFFSET  do_id_ofst = WN_offsetx(do_idname);
+    BOOL       is_kid0_do_id;
+    if ((WN_operator(WN_kid0(WN_kid0(WN_step(current_do)))) == OPR_LDID) &&
+        (WN_st(WN_kid0(WN_kid0(WN_step(current_do)))) == do_id_st) &&
+        (WN_offsetx(WN_kid0(WN_kid0(WN_step(current_do)))) == do_id_ofst))
+    {
+      do_stride[i] = WN_kid1(WN_kid0(WN_step(current_do)));
 #ifdef KEY
-    if (WN_operator (WN_kid0 (WN_step (do_tree))) == OPR_SUB)
-    { // the loop goes down, don't miss '-' in (- non-const-stride)
-      OPCODE negop = OPCODE_make_op (OPR_NEG, WN_rtype (do_stride), MTYPE_V);
-      do_stride = WN_CreateExp1 (negop, do_stride);
+      if (WN_operator (WN_kid0 (WN_step (current_do))) == OPR_SUB)
+      { // the loop goes down, don't miss '-' in (- non-const-stride)
+        OPCODE negop = OPCODE_make_op (OPR_NEG, WN_rtype (do_stride[i]), MTYPE_V);
+        do_stride[i] = WN_CreateExp1 (negop, do_stride[i]);
+      }
+#endif // KEY
     }
-#endif // KEY
-  }
-  else
-    do_stride = WN_kid0(WN_kid0(WN_step(do_tree)));
+    else
+      do_stride[i] = WN_kid0(WN_kid0(WN_step(current_do)));
 
-  // first, we determine the operation type of do termination comparison.
-  // In the form: do_index opr const
-  // opr is supposed to be LE/GE, but since currently we don't standardize
-  // the loop, we also should include LT/GT.
-  if(( WN_kid0( WN_end( do_tree )) != NULL ) &&
-     ( WN_operator( WN_kid0( WN_end( do_tree ))) == OPR_LDID ) &&
-     ( WN_st( WN_kid0( WN_end( do_tree ))) == do_id_st ) &&
-     ( WN_offsetx( WN_kid0( WN_end( do_tree ))) == do_id_ofst )) 
-  {
-    is_kid0_do_id = TRUE;
-  }
-  else
-  {
-    is_kid0_do_id = FALSE;
-  }
-  if(( WN_operator( WN_end( do_tree )) == OPR_LT ) ||
-     ( WN_operator( WN_end( do_tree )) == OPR_LE ))
-  {
-    is_LE = TRUE;
-  }
-  else
-  {
-    is_LE = FALSE;
-  }
-  if( is_kid0_do_id == FALSE )
-  {
-    if( is_LE == TRUE )
-      is_LE = FALSE;
+    // first, we determine the operation type of do termination comparison.
+    // In the form: do_index opr const
+    // opr is supposed to be LE/GE, but since currently we don't standardize
+    // the loop, we also should include LT/GT.
+    if(( WN_kid0( WN_end( current_do )) != NULL ) &&
+       ( WN_operator( WN_kid0( WN_end( current_do ))) == OPR_LDID ) &&
+       ( WN_st( WN_kid0( WN_end( current_do ))) == do_id_st ) &&
+       ( WN_offsetx( WN_kid0( WN_end( current_do ))) == do_id_ofst )) 
+    {
+      is_kid0_do_id = TRUE;
+    }
     else
-      is_LE = TRUE;
-  }
+    {
+      is_kid0_do_id = FALSE;
+    }
+    is_LE[i] = (( WN_operator( WN_end( current_do )) == OPR_LT ) ||
+                ( WN_operator( WN_end( current_do )) == OPR_LE ))
+      ^ (is_kid0_do_id == FALSE);
+    
+    // These vars need to be type casted?
+    //  Create_Preg_or_Temp( do_index_type, "temp_limit", &limit_st, &limit_ofst );
+    wn = WN_COPY_Tree ( limit_node[i] );
+    wn_tmp = WN_Stid ( do_index_type[i], limit_ofst[i], 
+                       limit_st[i], ST_type(limit_st[i]), wn );
+    WN_INSERT_BlockLast( do_prefix, wn_tmp );
 
-  do_prefix = WN_CreateBlock( );
-  do_suffix = WN_CreateBlock( );
-   // These vars need to be type casted?
-//  Create_Preg_or_Temp( do_index_type, "temp_limit", &limit_st, &limit_ofst );
-  wn = WN_COPY_Tree ( limit_node );
-  wn_tmp = WN_Stid ( do_index_type, limit_ofst, 
-                  limit_st, ST_type(limit_st), wn );
-  WN_INSERT_BlockLast( do_prefix, wn_tmp );
-//  Create_Temp( do_index_type, "do_upper", &local_upper );
-//  if ((WN_operator(WN_end(do_tree)) == OPR_LT) ||
-//      (WN_operator(WN_end(do_tree)) == OPR_GT))
-//    wn = WN_Add ( do_index_type,
-//		  Gen_MP_Load ( local_start, 0 ),
-//		  WN_Mpy ( do_index_type,
-//			   Gen_MP_Load ( local_ntrip, 0 ),
-//			   WN_COPY_Tree ( do_stride )));
-//  else
-//    wn = WN_Sub ( do_index_type,
-//		  WN_Add ( do_index_type,
-//			   Gen_MP_Load ( local_start, 0 ),
-//			   WN_Mpy ( do_index_type,
-//				    Gen_MP_Load ( local_ntrip, 0 ),
-//				    WN_COPY_Tree ( do_stride ))),
-//		  WN_COPY_Tree ( do_stride ));
+    wn = WN_COPY_Tree ( base_node[i] );
+    wn_tmp = WN_Stid ( do_index_type[i], base_ofst[i], 
+                       base_st[i], ST_type(base_st[i]), wn );
+    WN_INSERT_BlockLast( do_prefix, wn_tmp );
+    //  Create_Temp( do_index_type, "do_upper", &local_upper );
+    //  if ((WN_operator(WN_end(current_do)) == OPR_LT) ||
+    //      (WN_operator(WN_end(current_do)) == OPR_GT))
+    //    wn = WN_Add ( do_index_type,
+    //		  Gen_MP_Load ( local_start, 0 ),
+    //		  WN_Mpy ( do_index_type,
+    //			   Gen_MP_Load ( local_ntrip, 0 ),
+    //			   WN_COPY_Tree ( do_stride )));
+    //  else
+    //    wn = WN_Sub ( do_index_type,
+    //		  WN_Add ( do_index_type,
+    //			   Gen_MP_Load ( local_start, 0 ),
+    //			   WN_Mpy ( do_index_type,
+    //				    Gen_MP_Load ( local_ntrip, 0 ),
+    //				    WN_COPY_Tree ( do_stride ))),
+    //		  WN_COPY_Tree ( do_stride ));
     // Maybe using limit_node itself is also OK.
     // Note that, the type of local vars may be wrong.
-//  wn = WN_COPY_Tree ( limit_node );
-  wn_tmp = WN_Stid ( do_index_type, 0, 
-    local_upper, ST_type(local_upper),
-    Gen_MP_Load( limit_st, limit_ofst ));
-  WN_linenum( wn_tmp ) = line_number;
-  WN_INSERT_BlockLast ( do_prefix, wn_tmp );
+    //  wn = WN_COPY_Tree ( limit_node );
+    wn_tmp = WN_Stid ( do_index_type[i], 0, 
+                       local_upper[i], ST_type(local_upper[i]),
+                       Gen_MP_Load( limit_st[i], limit_ofst[i] ));
+    WN_linenum( wn_tmp ) = line_number;
+    WN_INSERT_BlockLast ( do_prefix, wn_tmp );
   
-//  Create_Temp( do_index_type, "do_lower", &local_lower );
-  wn = WN_COPY_Tree( base_node );
-  wn_tmp = WN_Stid( do_index_type, 0, 
-                  local_lower, ST_type(local_lower), wn );
-  WN_linenum( wn_tmp ) = line_number;
-  WN_INSERT_BlockLast( do_prefix, wn_tmp );
+    //  Create_Temp( do_index_type, "do_lower", &local_lower );
+    wn = WN_COPY_Tree( base_node[i] );
+    wn_tmp = WN_Stid( do_index_type[i], 0, 
+                      local_lower[i], ST_type(local_lower[i]), wn );
+    WN_linenum( wn_tmp ) = line_number;
+    WN_INSERT_BlockLast( do_prefix, wn_tmp );
 
-//  Create_Temp( do_index_type, "do_stride", &local_stride );
+    //  Create_Temp( do_index_type, "do_stride", &local_stride );
   
-//  Create_Temp( MTYPE_I4, "last_iter", &last_iter );
+    //  Create_Temp( MTYPE_I4, "last_iter", &last_iter );
 
-  wn = WN_Stid( MTYPE_I4, 0, last_iter, ST_type( last_iter ), 
+    wn = WN_Stid( MTYPE_I4, 0, last_iter, ST_type( last_iter ), 
                   WN_CreateIntconst( OPC_I4INTCONST, 0 )); 
-  WN_linenum( wn ) = line_number;
-  WN_INSERT_BlockLast( do_prefix, wn );
+    WN_linenum( wn ) = line_number;
+    WN_INSERT_BlockLast( do_prefix, wn );
 
+    current_do = WN_first(WN_do_body(current_do));
+  }
+
     
   do_schedule = WN_CreateIntconst( OPC_I4INTCONST, schedule );
   // What is the legal type of do_index_type?
@@ -9320,7 +9664,51 @@
   // How about If last_iter, local_upper|lower != NULL?
   // And how to determine the type of RTL calls?
 
-  if (( schedule == OMP_SCHED_STATIC_EVEN ) ||
+  if ( collapse_count > 1 ) {
+    // Prefix code for Dynamic code
+    do_init_call = WN_Create( OPC_VCALL, 4 + 4 * collapse_count);
+    WN_st_idx( do_init_call ) = GET_MPRUNTIME_ST(MPR_OMP_COLLAPSE_INIT);
+    WN_Set_Call_Non_Data_Mod( do_init_call );
+    WN_Set_Call_Non_Data_Ref( do_init_call );
+#ifndef KEY // bug 4671
+    WN_Set_Call_Non_Parm_Mod( do_init_call );
+#endif
+    WN_Set_Call_Non_Parm_Ref( do_init_call );
+    WN_Set_Call_Parm_Ref( do_init_call );
+    WN_linenum( do_init_call ) = line_number;
+    
+    WN_kid( do_init_call, 0 ) = WN_CreateParm( MTYPE_I4, 
+        Gen_MP_Load( local_gtid, 0 ),
+        Be_Type_Tbl( MTYPE_I4 ), WN_PARM_BY_VALUE );
+    WN_kid( do_init_call, 1 ) = WN_CreateParm( MTYPE_I4, do_schedule, 
+        Be_Type_Tbl( MTYPE_I4 ), WN_PARM_BY_VALUE );
+    wn_tmp = WN_Integer_Cast( chunk_size, MTYPE_I8, WN_rtype( chunk_size ));
+    WN_kid( do_init_call, 2 ) = WN_CreateParm( MTYPE_I8,
+       wn_tmp, Be_Type_Tbl( MTYPE_I8 ), WN_PARM_BY_VALUE ); 
+    WN_kid( do_init_call, 3 ) = WN_CreateParm( MTYPE_U4,
+       WN_Intconst( MTYPE_U4, collapse_count), Be_Type_Tbl( MTYPE_U4 ), WN_PARM_BY_VALUE);
+
+    for (UINT32 i = 0; i < collapse_count; i++) {
+      is_do32 = (do_index_type[i] != MTYPE_I8 && do_index_type[i] != MTYPE_U8);
+      WN_kid( do_init_call, 4 * i + 4 ) = WN_CreateParm( MTYPE_U4, WN_Intconst(MTYPE_U4, (is_do32 ? 0 : 1)), Be_Type_Tbl(MTYPE_I4), WN_PARM_BY_VALUE);
+
+      wn_tmp = Gen_MP_Load( local_lower[i], 0 );
+      WN_kid( do_init_call, 4 * i + 5 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8,
+        wn_tmp, Be_Type_Tbl( is_do32 ? MTYPE_I4 : MTYPE_I8 ), 
+        WN_PARM_BY_VALUE );
+      wn_tmp = Gen_MP_Load( local_upper[i], 0 );
+      WN_kid( do_init_call, 4 * i + 6 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8,
+        wn_tmp, Be_Type_Tbl( is_do32 ? MTYPE_I4 : MTYPE_I8 ), 
+        WN_PARM_BY_VALUE );
+      wn_tmp = WN_COPY_Tree( do_stride[i] );
+      wn_tmp = WN_Integer_Cast( wn_tmp, is_do32 ? MTYPE_I4 : MTYPE_I8, 
+	  	 WN_rtype( wn_tmp )); 
+      WN_kid( do_init_call, 4 * i + 7 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8,
+        wn_tmp, Be_Type_Tbl( is_do32 ? MTYPE_I4 : MTYPE_I8 ),
+        WN_PARM_BY_VALUE );
+    }
+       // The type of chunk size also need to be fixed.
+  } else if (( schedule == OMP_SCHED_STATIC_EVEN ) ||
       ( schedule == OMP_SCHED_STATIC )) 
 //      ( schedule == OMP_SCHED_ORDERED_STATIC_EVEN ) ||
 //      ( schedule == OMP_SCHED_ORDERED_STATIC ))
@@ -9349,17 +9737,17 @@
     WN_kid( do_init_call, 2 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
 #endif
-    wn_tmp = WN_Lda( Pointer_type, 0, local_lower );
+    wn_tmp = WN_Lda( Pointer_type, 0, local_lower[0] );
     WN_kid( do_init_call, 2 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
-    wn_tmp = WN_Lda( Pointer_type, 0, local_upper );
+    wn_tmp = WN_Lda( Pointer_type, 0, local_upper[0] );
     WN_kid( do_init_call, 3 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
-    wn_tmp = WN_Lda( Pointer_type, 0, local_stride );
+    wn_tmp = WN_Lda( Pointer_type, 0, local_stride[0] );
     WN_kid( do_init_call, 4 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
        // What if the do_stride is not the same type as M_I4/ M_I8?
-    wn = WN_COPY_Tree( do_stride );
+    wn = WN_COPY_Tree( do_stride[0] );
     wn_tmp = WN_Integer_Cast( wn, is_do32 ? MTYPE_I4 : MTYPE_I8, 
 	  	 WN_rtype( wn )); 
     WN_kid( do_init_call, 5 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8, 
@@ -9371,8 +9759,7 @@
     WN_kid( do_init_call, 6 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8,
        wn_tmp, Be_Type_Tbl( is_do32 ? MTYPE_I4 : MTYPE_I8 ),
        WN_PARM_BY_VALUE ); 
-  }else 
-  {
+  } else {
     // Prefix code for Dynamic code
     do_init_call = WN_Create( OPC_VCALL, 6);
     WN_st_idx( do_init_call ) = GET_MPRUNTIME_ST( is_do32
@@ -9391,11 +9778,11 @@
         Be_Type_Tbl( MTYPE_I4 ), WN_PARM_BY_VALUE );
     WN_kid( do_init_call, 1 ) = WN_CreateParm( MTYPE_I4, do_schedule, 
         Be_Type_Tbl( MTYPE_I4 ), WN_PARM_BY_VALUE );
-    wn_tmp = Gen_MP_Load( local_lower, 0 );
+    wn_tmp = Gen_MP_Load( local_lower[0], 0 );
     WN_kid( do_init_call, 2 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8,
        wn_tmp, Be_Type_Tbl( is_do32 ? MTYPE_I4 : MTYPE_I8 ), 
        WN_PARM_BY_VALUE );
-    wn_tmp = Gen_MP_Load( local_upper, 0 );
+    wn_tmp = Gen_MP_Load( local_upper[0], 0 );
     WN_kid( do_init_call, 3 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8,
        wn_tmp, Be_Type_Tbl( is_do32 ? MTYPE_I4 : MTYPE_I8 ), 
        WN_PARM_BY_VALUE );
@@ -9403,7 +9790,7 @@
 //    WN_kid( do_init_call, 5 ) = WN_CreateParm( Pointer_type, wn_tmp,  
 //        WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
        // What if the do_stride is not the same type as M_I4/ M_I8?
-    wn_tmp = WN_COPY_Tree( do_stride );
+    wn_tmp = WN_COPY_Tree( do_stride[0] );
     wn_tmp = WN_Integer_Cast( wn_tmp, is_do32 ? MTYPE_I4 : MTYPE_I8, 
 	  	 WN_rtype( wn_tmp )); 
     WN_kid( do_init_call, 4 ) = WN_CreateParm( is_do32 ? MTYPE_I4 : MTYPE_I8,
@@ -9419,36 +9806,156 @@
 
   WN_INSERT_BlockLast( do_prefix, do_init_call );
 
-  if ( schedule == OMP_SCHED_STATIC_EVEN ) 
-//      ( schedule == OMP_SCHED_ORDERED_STATIC_EVEN )) 
+  if ( collapse_count > 1 ) {
+    // Rewrite DO body for Other SCHEDULEs
+    // while test
+    // need to be rewritten.
+    call_wn = WN_Create( OPC_I4CALL, 1 + 2 * collapse_count);
+    WN_st_idx( call_wn ) = GET_MPRUNTIME_ST(MPR_OMP_COLLAPSE_NEXT);
+    WN_Set_Call_Non_Data_Mod( call_wn );
+    WN_Set_Call_Non_Data_Ref( call_wn );
+#ifndef KEY // bug 4671
+    WN_Set_Call_Non_Parm_Mod( call_wn );
+#endif
+    WN_Set_Call_Non_Parm_Ref( call_wn );
+    WN_Set_Call_Parm_Mod( call_wn );
+    WN_Set_Call_Parm_Ref( call_wn );
+    WN_linenum( call_wn ) = line_number;
+    
+    WN_kid( call_wn, 0 ) = WN_CreateParm( MTYPE_I4, 
+        Gen_MP_Load( local_gtid, 0),
+        Be_Type_Tbl( MTYPE_I4 ), WN_PARM_BY_VALUE );
+#ifndef KEY
+    wn_tmp = WN_Lda( Pointer_type, 0, last_iter );
+    WN_kid( call_wn, 1 ) = WN_CreateParm( Pointer_type, wn_tmp,  
+        WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
+#endif
+    for (UINT32 i = 0; i < collapse_count; i++) {
+      wn_tmp = WN_Lda( Pointer_type, 0, local_lower[i] );
+      WN_kid( call_wn, 2 * (collapse_count - i) - 1 ) = WN_CreateParm( Pointer_type, wn_tmp,  
+        WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
+      wn_tmp = WN_Lda( Pointer_type, 0, local_upper[i] );
+      WN_kid( call_wn, 2 * (collapse_count - i)) = WN_CreateParm( Pointer_type, wn_tmp,  
+        WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
+       // What if the do_stride is not the same type as M_I4/ M_I8?
+    }
+    WN_INSERT_BlockLast( do_prefix , call_wn );
+
+    Create_Preg_or_Temp ( MTYPE_I4, "mpni_status", &return_st, &return_ofst );
+    GET_RETURN_PREGS(rreg1, rreg2, MTYPE_I4);
+    wn = WN_Stid ( MTYPE_I4, return_ofst, return_st, ST_type(return_st),
+		    WN_LdidPreg ( MTYPE_I4, rreg1 ));
+    WN_linenum(wn) = line_number;
+    WN_INSERT_BlockLast( do_prefix , wn );
+
+    while_test = Gen_MP_Load( return_st, return_ofst );
+    // while body
+    while_body = WN_CreateBlock( );
+    // insert do
+    Rewrite_Collapsed_Do ( while_body, do_tree, is_LE );
+    WN_INSERT_BlockLast( while_body, WN_COPY_Tree( call_wn ));
+    WN_INSERT_BlockLast( while_body, WN_COPY_Tree( wn )); 
+      // increase lower and upper.
+     
+    // replace original do with a new While wrapping do
+    while_wn = WN_CreateWhileDo( while_test, while_body );
+      // Does this cause new problems? if late operation must
+      // check the type of do_tree, we must take a new method
+      // to work around this.
+#ifdef KEY
+    {
+      // Initialize the do-loop index variable outside the while loop, so
+      // that a thread not doing any iteration does not land up with a 
+      // wrong (uninitialized) value of the index variable. This is 
+      // especially important for lastprivate handling which does a compare
+      // to find out the last iteration.
+      //
+      WN * current_do_tree = do_tree;
+      for (UINT i = 0; i < collapse_count; i++) {
+        WN * ldid_lower = WN_Ldid (TY_mtype (Ty_Table[ST_type (local_lower[i])]),
+                                   0,
+				   local_lower[i],
+				   ST_type (local_lower[i]));
+        ST * do_index = WN_st (WN_index (current_do_tree));
+        INT do_offset = WN_offset (WN_index (current_do_tree));
+        WN * init_do_index = WN_Stid (WN_rtype (ldid_lower),
+                                      do_offset,
+				      do_index,
+				      ST_type (do_index),
+				      ldid_lower);
+        WN_linenum (init_do_index) = WN_linenum(current_do_tree);
+        // Insert the initialization before the schedule_next function call
+        WN_INSERT_BlockBefore (do_prefix, call_wn, init_do_index);
+        current_do_tree = WN_first(WN_do_body(current_do_tree));
+      }
+    }
+#endif
+
+    do_tree = while_wn; 
+
+    // last thread node.
+    if( lastlocal_nodes || nested_lastlocal_nodes)
+    {
+#ifdef KEY
+      WN * old_do_tree = do_tree;
+      // generate an if-stmt to check if this is the last iteration, and then
+      // set last_iter, which will be checked to set lastprivate variables
+      if( is_LE[0] )
+      {
+        test_wn = WN_GT (do_index_type[0],
+                         Gen_MP_Load( WN_st(WN_kid0(old_do_tree)), WN_offsetx(WN_kid0(old_do_tree)) ),
+                         Gen_MP_Load( limit_st[0], limit_ofst[0] ));
+
+      }
+      else
+      {
+        test_wn = WN_LT (do_index_type[0],
+                         Gen_MP_Load( WN_st(WN_kid0(old_do_tree)), WN_offsetx(WN_kid0(old_do_tree)) ),
+                         Gen_MP_Load( limit_st[0], limit_ofst[0] ));
+      }
+      then_wn =  WN_CreateBlock( );
+      wn_tmp = WN_Stid ( MTYPE_I4 , 0, 
+        last_iter, ST_type( last_iter ), 
+		    WN_CreateIntconst( OPC_I4INTCONST, 1 ));
+      WN_INSERT_BlockLast( then_wn, wn_tmp );
+      else_wn = WN_CreateBlock( );
+      if_wn = WN_CreateIf( test_wn, then_wn, else_wn );
+      WN_linenum( if_wn ) = line_number;
+      WN_INSERT_BlockLast( do_suffix, if_wn );
+#endif
+      // Indeed, the RTL set the last_iter, so nothing to do.
+        // adjust lower/upper first.
+      // Flush/ barrier needed?
+    }
+  } else if ( schedule == OMP_SCHED_STATIC_EVEN ) 
   {
     // Rewrite DO body for STATIC EVEN SCHEDULE
     // If clause to fix upper returned by RTL
     // Must consider situation for stride < 0
       // adjust upper
-    if( is_LE )
+    if( is_LE[0] )
     {
-       test_wn = WN_GT( do_index_type,
-                        Gen_MP_Load( local_upper, 0 ),
-                        Gen_MP_Load( limit_st, limit_ofst ));
+       test_wn = WN_GT( do_index_type[0],
+                        Gen_MP_Load( local_upper[0], 0 ),
+                        Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     else
     {
-       test_wn = WN_LT( do_index_type,
-                        Gen_MP_Load( local_upper, 0 ),
-                        Gen_MP_Load( limit_st, limit_ofst ));
+       test_wn = WN_LT( do_index_type[0],
+                        Gen_MP_Load( local_upper[0], 0 ),
+                        Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     then_wn =  WN_CreateBlock( );
-    wn_tmp = WN_Stid ( do_index_type, 0, 
-      local_upper, ST_type(local_upper), 
-		  Gen_MP_Load( limit_st, limit_ofst ));
+    wn_tmp = WN_Stid ( do_index_type[0], 0, 
+      local_upper[0], ST_type(local_upper[0]), 
+		  Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     WN_INSERT_BlockLast( then_wn, wn_tmp );
     else_wn = WN_CreateBlock( );
     if_wn = WN_CreateIf( test_wn, then_wn, else_wn );
     WN_linenum( if_wn ) = line_number;
     WN_INSERT_BlockLast( do_prefix, if_wn );
 
-    Rewrite_Do_New( do_tree, local_lower, local_upper, NULL );
+    Rewrite_Do_New( do_tree, local_lower[0], local_upper[0], NULL );
     //return_wn = do_tree;
     // Maybe we should insert do_tree here.
     // Now, setup lastthread info. The lastiter is not set
@@ -9457,13 +9964,13 @@
 
     if ( lastlocal_nodes || nested_lastlocal_nodes )
     {
-      if( is_LE )
+      if( is_LE[0] )
       {
 // Bug 4660
 #ifdef KEY
-        test_wn = WN_GT (do_index_type,
+        test_wn = WN_GT (do_index_type[0],
                          Gen_MP_Load( WN_st(WN_kid0(do_tree)), WN_offsetx(WN_kid0(do_tree)) ),
-                         Gen_MP_Load( limit_st, limit_ofst ));
+                         Gen_MP_Load( limit_st[0], limit_ofst[0] ));
 
 #else
         test_wn = WN_CAND( WN_LE( do_index_type,
@@ -9478,9 +9985,9 @@
       {
 // Bug 4660
 #ifdef KEY
-        test_wn = WN_LT (do_index_type,
+        test_wn = WN_LT (do_index_type[0],
                          Gen_MP_Load( WN_st(WN_kid0(do_tree)), WN_offsetx(WN_kid0(do_tree)) ),
-                         Gen_MP_Load( limit_st, limit_ofst ));
+                         Gen_MP_Load( limit_st[0], limit_ofst[0] ));
 #else
         test_wn = WN_CAND( WN_GE( do_index_type,
                                   Gen_MP_Load( local_lower, 0 ),
@@ -9526,55 +10033,55 @@
   {
     // Rewrite DO body for STATIC SCHEDULE 
     // while test
-    if( is_LE)
+    if( is_LE[0])
     {
-      while_test = WN_LE( do_index_type,
-                          Gen_MP_Load( local_lower, 0 ),
-                          Gen_MP_Load( limit_st, limit_ofst ));
+      while_test = WN_LE( do_index_type[0],
+                          Gen_MP_Load( local_lower[0], 0 ),
+                          Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     else
     {
-      while_test = WN_GE( do_index_type,
-                          Gen_MP_Load( local_lower, 0 ),
-                          Gen_MP_Load( limit_st, limit_ofst ));
+      while_test = WN_GE( do_index_type[0],
+                          Gen_MP_Load( local_lower[0], 0 ),
+                          Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     // while body
     while_body = WN_CreateBlock( );
       // adjust upper
-    if( is_LE )
+    if( is_LE[0] )
     {
-       test_wn = WN_GT( do_index_type,
-                        Gen_MP_Load( local_upper, 0 ),
-                        Gen_MP_Load( limit_st, limit_ofst ));
+       test_wn = WN_GT( do_index_type[0],
+                        Gen_MP_Load( local_upper[0], 0 ),
+                        Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     else
     {
-       test_wn = WN_LT( do_index_type,
-                        Gen_MP_Load( local_upper, 0 ),
-                        Gen_MP_Load( limit_st, limit_ofst ));
+       test_wn = WN_LT( do_index_type[0],
+                        Gen_MP_Load( local_upper[0], 0 ),
+                        Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     then_wn =  WN_CreateBlock( );
-    wn_tmp = WN_Stid ( do_index_type, 0, 
-      local_upper, ST_type(local_upper), 
-		  Gen_MP_Load( limit_st, limit_ofst ));
+    wn_tmp = WN_Stid ( do_index_type[0], 0, 
+      local_upper[0], ST_type(local_upper[0]), 
+		  Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     WN_INSERT_BlockLast( then_wn, wn_tmp );
     else_wn = WN_CreateBlock( );
     if_wn = WN_CreateIf( test_wn, then_wn, else_wn );
     WN_linenum( if_wn ) = line_number;
     WN_INSERT_BlockLast( while_body, if_wn );
       // insert do
-    Rewrite_Do_New( do_tree, local_lower, local_upper, NULL);
+    Rewrite_Do_New( do_tree, local_lower[0], local_upper[0], NULL);
     WN_INSERT_BlockLast( while_body, do_tree );
       // increase lower and upper.
-    wn_tmp = WN_Add( do_index_type, 
-                     Gen_MP_Load( local_lower, 0 ),
-                     Gen_MP_Load( local_stride, 0 )); 
-    wn = Gen_MP_Store( local_lower, 0, wn_tmp );
+    wn_tmp = WN_Add( do_index_type[0], 
+                     Gen_MP_Load( local_lower[0], 0 ),
+                     Gen_MP_Load( local_stride[0], 0 )); 
+    wn = Gen_MP_Store( local_lower[0], 0, wn_tmp );
     WN_INSERT_BlockLast( while_body, wn );
-    wn_tmp = WN_Add( do_index_type,
-                     Gen_MP_Load( local_upper, 0 ),
-                     Gen_MP_Load( local_stride, 0 ));
-    wn = Gen_MP_Store( local_upper, 0, wn_tmp );
+    wn_tmp = WN_Add( do_index_type[0],
+                     Gen_MP_Load( local_upper[0], 0 ),
+                     Gen_MP_Load( local_stride[0], 0 ));
+    wn = Gen_MP_Store( local_upper[0], 0, wn_tmp );
     WN_INSERT_BlockLast( while_body, wn );
     
     // replace original do with a new While wrapping do
@@ -9595,33 +10102,33 @@
     if ( lastlocal_nodes || nested_lastlocal_nodes )
     {
         // adjust lower/upper first.
-      wn_tmp = WN_Sub( do_index_type, 
-                       Gen_MP_Load( local_lower, 0 ),
-                       Gen_MP_Load( local_stride, 0 )); 
-      wn = Gen_MP_Store( local_lower, 0, wn_tmp );
+      wn_tmp = WN_Sub( do_index_type[0], 
+                       Gen_MP_Load( local_lower[0], 0 ),
+                       Gen_MP_Load( local_stride[0], 0 )); 
+      wn = Gen_MP_Store( local_lower[0], 0, wn_tmp );
       WN_INSERT_BlockLast( do_suffix, wn );
-      wn_tmp = WN_Sub( do_index_type,
-                       Gen_MP_Load( local_upper, 0 ),
-                       Gen_MP_Load( local_stride, 0 ));
-      wn = Gen_MP_Store( local_upper, 0, wn_tmp );
+      wn_tmp = WN_Sub( do_index_type[0],
+                       Gen_MP_Load( local_upper[0], 0 ),
+                       Gen_MP_Load( local_stride[0], 0 ));
+      wn = Gen_MP_Store( local_upper[0], 0, wn_tmp );
       WN_INSERT_BlockLast( do_suffix, wn );
-      if( is_LE )
+      if( is_LE[0] )
       {
-        test_wn = WN_CAND( WN_LE( do_index_type,
-                                  Gen_MP_Load( local_lower, 0 ),
-                                  Gen_MP_Load( limit_st, limit_ofst )),
-                           WN_GE( do_index_type,
-                                  Gen_MP_Load( local_upper, 0 ),
-                                  Gen_MP_Load( limit_st, limit_ofst )));
+        test_wn = WN_CAND( WN_LE( do_index_type[0],
+                                  Gen_MP_Load( local_lower[0], 0 ),
+                                  Gen_MP_Load( limit_st[0], limit_ofst[0] )),
+                           WN_GE( do_index_type[0],
+                                  Gen_MP_Load( local_upper[0], 0 ),
+                                  Gen_MP_Load( limit_st[0], limit_ofst[0] )));
       }
       else
       {
-        test_wn = WN_CAND( WN_GE( do_index_type,
-                                  Gen_MP_Load( local_lower, 0 ),
-                                  Gen_MP_Load( limit_st, limit_ofst )),
-                           WN_LE( do_index_type,
-                                  Gen_MP_Load( local_upper, 0 ),
-                                  Gen_MP_Load( limit_st, limit_ofst )));
+        test_wn = WN_CAND( WN_GE( do_index_type[0],
+                                  Gen_MP_Load( local_lower[0], 0 ),
+                                  Gen_MP_Load( limit_st[0], limit_ofst[0] )),
+                           WN_LE( do_index_type[0],
+                                  Gen_MP_Load( local_upper[0], 0 ),
+                                  Gen_MP_Load( limit_st[0], limit_ofst[0] )));
       }
       then_wn =  WN_CreateBlock( );
       wn_tmp = WN_Stid ( MTYPE_I4 , 0, 
@@ -9678,13 +10185,13 @@
     WN_kid( call_wn, 1 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
 #endif
-    wn_tmp = WN_Lda( Pointer_type, 0, local_lower );
+    wn_tmp = WN_Lda( Pointer_type, 0, local_lower[0] );
     WN_kid( call_wn, 1 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
-    wn_tmp = WN_Lda( Pointer_type, 0, local_upper );
+    wn_tmp = WN_Lda( Pointer_type, 0, local_upper[0] );
     WN_kid( call_wn, 2 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
-    wn_tmp = WN_Lda( Pointer_type, 0, local_stride );
+    wn_tmp = WN_Lda( Pointer_type, 0, local_stride[0] );
     WN_kid( call_wn, 3 ) = WN_CreateParm( Pointer_type, wn_tmp,  
         WN_ty( wn_tmp ), WN_PARM_BY_REFERENCE );
        // What if the do_stride is not the same type as M_I4/ M_I8?
@@ -9701,29 +10208,29 @@
     // while body
     while_body = WN_CreateBlock( );
       // adjust upper
-    if( is_LE )
+    if( is_LE[0] )
     {
-       test_wn = WN_GT( do_index_type,
-                        Gen_MP_Load( local_upper, 0 ),
-                        Gen_MP_Load( limit_st, limit_ofst ));
+       test_wn = WN_GT( do_index_type[0],
+                        Gen_MP_Load( local_upper[0], 0 ),
+                        Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     else
     {
-       test_wn = WN_LT( do_index_type,
-                        Gen_MP_Load( local_upper, 0 ),
-                        Gen_MP_Load( limit_st, limit_ofst ));
+       test_wn = WN_LT( do_index_type[0],
+                        Gen_MP_Load( local_upper[0], 0 ),
+                        Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     }
     then_wn =  WN_CreateBlock( );
-    wn_tmp = WN_Stid ( do_index_type, 0, 
-      local_upper, ST_type(local_upper), 
-		  Gen_MP_Load( limit_st, limit_ofst ));
+    wn_tmp = WN_Stid ( do_index_type[0], 0, 
+      local_upper[0], ST_type(local_upper[0]), 
+		  Gen_MP_Load( limit_st[0], limit_ofst[0] ));
     WN_INSERT_BlockLast( then_wn, wn_tmp );
     else_wn = WN_CreateBlock( );
     if_wn = WN_CreateIf( test_wn, then_wn, else_wn );
     WN_linenum( if_wn ) = line_number;
     WN_INSERT_BlockLast( while_body, if_wn );
       // insert do
-    Rewrite_Do_New( do_tree, local_lower, local_upper, local_stride);
+    Rewrite_Do_New( do_tree, local_lower[0], local_upper[0], local_stride[0]);
     WN_INSERT_BlockLast( while_body, do_tree );
     WN_INSERT_BlockLast( while_body, WN_COPY_Tree( call_wn ));
     WN_INSERT_BlockLast( while_body, WN_COPY_Tree( wn )); 
@@ -9743,10 +10250,10 @@
       // especially important for lastprivate handling which does a compare
       // to find out the last iteration.
       //
-      WN * ldid_lower = WN_Ldid (TY_mtype (Ty_Table[ST_type (local_lower)]),
+      WN * ldid_lower = WN_Ldid (TY_mtype (Ty_Table[ST_type (local_lower[0])]),
                                  0,
-				 local_lower,
-				 ST_type (local_lower));
+				 local_lower[0],
+				 ST_type (local_lower[0]));
       ST * do_index = WN_st (WN_index (old_do_tree));
       INT do_offset = WN_offset (WN_index (old_do_tree));
       WN * init_do_index = WN_Stid (WN_rtype (ldid_lower),
@@ -9768,18 +10275,18 @@
 #ifdef KEY
       // generate an if-stmt to check if this is the last iteration, and then
       // set last_iter, which will be checked to set lastprivate variables
-      if( is_LE )
+      if( is_LE[0] )
       {
-        test_wn = WN_GT (do_index_type,
+        test_wn = WN_GT (do_index_type[0],
                          Gen_MP_Load( WN_st(WN_kid0(old_do_tree)), WN_offsetx(WN_kid0(old_do_tree)) ),
-                         Gen_MP_Load( limit_st, limit_ofst ));
+                         Gen_MP_Load( limit_st[0], limit_ofst[0] ));
 
       }
       else
       {
-        test_wn = WN_LT (do_index_type,
+        test_wn = WN_LT (do_index_type[0],
                          Gen_MP_Load( WN_st(WN_kid0(old_do_tree)), WN_offsetx(WN_kid0(old_do_tree)) ),
-                         Gen_MP_Load( limit_st, limit_ofst ));
+                         Gen_MP_Load( limit_st[0], limit_ofst[0] ));
       }
       then_wn =  WN_CreateBlock( );
       wn_tmp = WN_Stid ( MTYPE_I4 , 0, 
@@ -9795,7 +10302,7 @@
         // adjust lower/upper first.
       // Flush/ barrier needed?
     }
-}
+  }
 
   WN_INSERT_BlockLast( return_wn, do_prefix );
   WN_INSERT_BlockLast( return_wn, do_tree );
@@ -9821,111 +10328,8 @@
 #endif // !KEY
 
   return return_wn;
-} // Transform_Do()
+}
 
-/*  Rewrite do statement.  */
-//CAN BE DELETED.
-//TODO: delete and clean up this routine.
-static void 
-Rewrite_Do ( WN * do_tree )
-{
-  WN        *wn;
-  WN        *do_idname  = WN_index(do_tree);
-  ST        *do_id_st   = WN_st(do_idname);
-  WN_OFFSET  do_id_ofst = WN_offsetx(do_idname);
-  WN        *do_stride;
-  WN        *loop_info;
-  // The following variables move to global scope
-//  ST        *limit_st;
-//  WN_OFFSET  limit_ofst;
-
-  /* Generate do preamble code to calculate limit value */
-
-  if ((WN_operator(WN_kid0(WN_kid0(WN_step(do_tree)))) == OPR_LDID) &&
-      (WN_st(WN_kid0(WN_kid0(WN_step(do_tree)))) == do_id_st) &&
-      (WN_offsetx(WN_kid0(WN_kid0(WN_step(do_tree)))) == do_id_ofst))
-    do_stride = WN_kid1(WN_kid0(WN_step(do_tree)));
-  else
-    do_stride = WN_kid0(WN_kid0(WN_step(do_tree)));
-
-  Create_Preg_or_Temp ( do_index_type, "do_limit", &limit_st, &limit_ofst );
-  if ((WN_operator(WN_end(do_tree)) == OPR_LT) ||
-      (WN_operator(WN_end(do_tree)) == OPR_GT))
-    wn = WN_Add ( do_index_type,
-		  Gen_MP_Load ( local_start, 0 ),
-		  WN_Mpy ( do_index_type,
-			   Gen_MP_Load ( local_ntrip, 0 ),
-			   WN_COPY_Tree ( do_stride )));
-  else
-    wn = WN_Sub ( do_index_type,
-		  WN_Add ( do_index_type,
-			   Gen_MP_Load ( local_start, 0 ),
-			   WN_Mpy ( do_index_type,
-				    Gen_MP_Load ( local_ntrip, 0 ),
-				    WN_COPY_Tree ( do_stride ))),
-		  WN_COPY_Tree ( do_stride ));
-  do_prefix = WN_Stid ( do_index_type, limit_ofst, limit_st, ST_type(limit_st),
-			wn );
-  WN_linenum(do_prefix) = line_number;
-
-  /* Fix up the do loop controls */
-
-/* Temporary fix for PV 381272.  Wopt cannot handle double convert of I8 -> I1/2
-   and so we coerce I8 to I4 first.  Note that the offset (4) is correct for
-   little endian systems only. */
-  if ((ST_sclass(local_start) == SCLASS_AUTO) &&
-      (ST_btype(local_start) == MTYPE_I8) &&
-      ((WN_desc(WN_start(do_tree)) == MTYPE_I1) ||
-       (WN_desc(WN_start(do_tree)) == MTYPE_I2))) { 
-    WN_kid0(WN_start(do_tree)) = WN_RLdid ( Promote_Type(MTYPE_I4), MTYPE_I4, 4,
-					    local_start, MTYPE_To_TY(MTYPE_I4));
-  } else { 
-/* End temporary fix for PV 381272. */
-    WN* wn_local_start = Gen_MP_Load(local_start, 0);
-    WN* wn_new_local_start = WN_Integer_Cast(wn_local_start, Promote_Type(WN_desc(WN_start(do_tree))), WN_rtype(wn_local_start));
-    WN_kid0(WN_start(do_tree)) = wn_new_local_start; 
-  } 
-
-  WN* wn_ldid = WN_Ldid(do_index_type, limit_ofst, limit_st, 
-    ST_type(limit_st));
-  WN* wn_cvt_ldid = wn_ldid; 
-  if (WN_rtype(wn_cvt_ldid) != WN_desc(WN_end(do_tree)))
-     wn_cvt_ldid = WN_Integer_Cast(wn_cvt_ldid, WN_desc(WN_end(do_tree)),
-       WN_rtype(wn_cvt_ldid));
-  if (WN_kid0(WN_end(do_tree)) == NULL) { 
-    WN_kid0(WN_end(do_tree)) = wn_cvt_ldid; 
-  } else { 
-    WN_kid1(WN_end(do_tree)) = wn_cvt_ldid;
-  } 
-
-  /* Fix up the optional LOOP_INFO node */
-
-  loop_info = WN_do_loop_info(do_tree);
-#ifndef KEY
-  if (loop_info)
-#else
-  // Bug 4809 - we will preserve the loop trip count information if the 
-  // original trip count is an integer constant. For MP DO loops, the loop
-  // bounds are determined at run-time. However, if the loop bounds for the 
-  // original loop was a compile time constant, it helps to transfer that 
-  // information to the later compilation phases (the trip count information
-  // is an estimate). In the case of STREAM -mp, this fix specifically lets the
-  // CG loop optimizer to convert stores to non-temporal stores because the
-  // loop trip count is now made available.
-  if (loop_info && (!WN_loop_trip(loop_info) || 
-		    !WN_operator_is(WN_loop_trip(loop_info), OPR_INTCONST)))
-#endif
-  if (loop_info) {
-    WN_loop_trip_est(loop_info) = 0;
-    WN_loop_depth(loop_info) = 1;
-    WN_Reset_Loop_Nz_Trip ( loop_info );
-    if (WN_loop_trip(loop_info)) {
-      WN_DELETE_Tree ( WN_loop_trip(loop_info) );
-      WN_set_loop_trip ( loop_info, Gen_MP_Load ( local_ntrip, 0 ));
-    }
-  }
-} // Rewrite_Do()
-
 /*
 Scale FB for do_loop to reflect worksharing of its iterations among all
 the threads (e.g. multiply the number of iterations by
@@ -10137,10 +10541,10 @@
 		      "missing pragma (CRITICAL_SECTION_END) in MP processing");
 	  if (lock_st) {
 	    Linenum_Pusher p(WN_Get_Linenum(cur_node));
-	    wn = Gen_End_Critical(local_gtid, lock_st,FALSE);
+	    wn = Gen_End_Critical(local_gtid, lock_st, FALSE);
 	  } else {
 	    Linenum_Pusher p(WN_Get_Linenum(cur_node));
-      wn = Gen_End_Critical(local_gtid, unnamed_lock_st,FALSE);
+            wn = Gen_End_Critical(local_gtid, unnamed_lock_st, FALSE);
 	  }
 	  WN_next(WN_prev(cur_node)) = wn;
 	  WN_prev(wn) = WN_prev(cur_node);
@@ -10209,6 +10613,9 @@
         //WN_INSERT_BlockAfter (tree, prev_node, Gen_OMP_End_Ordered());
         WN_DELETE_FromBlock (tree, cur_node);
         break;
+        
+    case WN_PRAGMA_TASK_BEGIN:
+        break;
 
 	default:
 	  Fail_FmtAssertion (
@@ -10269,7 +10676,6 @@
       mpt = save_mpt;
 
     } else if (is_region &&
-               WN_first(WN_region_pragmas(cur_node)) &&
 	       WN_pragma(WN_first(WN_region_pragmas(cur_node))) ==
                   WN_PRAGMA_SINGLE_PROCESS_BEGIN) {
 
@@ -10293,7 +10699,6 @@
 
 #ifdef KEY /* Bug 4828 */
     } else if (is_region &&
-               WN_first(WN_region_pragmas(cur_node)) &&
 	       WN_pragma(WN_first(WN_region_pragmas(cur_node))) ==
                   WN_PRAGMA_PWORKSHARE_BEGIN) {
 
@@ -10318,9 +10723,8 @@
 #endif
 
     } else if (is_region &&
-               WN_first(WN_region_pragmas(cur_node)) &&
-	             ( WN_pragma(WN_first(WN_region_pragmas(cur_node))) ==
-               WN_PRAGMA_MASTER_BEGIN)) {
+               WN_pragma(WN_first(WN_region_pragmas(cur_node))) ==
+                  WN_PRAGMA_MASTER_BEGIN) {
 
       BOOL save_comp_gen_construct = comp_gen_construct;
       comp_gen_construct = ( WN_pragma_compiler_generated(
@@ -10340,6 +10744,11 @@
       comp_gen_construct = save_comp_gen_construct; // restore old value
       mpt = save_mpt;
 
+    } else if (is_region &&
+               WN_pragma(WN_first(WN_region_pragmas(cur_node))) ==
+                  WN_PRAGMA_TASK_BEGIN) {
+      
+      /* Do nothing. This region will be handled properly during excution of lower_mp for nested PU. */
     } else {
       for (i = 0; i < WN_kid_count(cur_node); i++)
         if (WN_kid(cur_node, i) &&
@@ -10551,6 +10960,7 @@
   WN_Delete ( cur_node );
 #endif
 
+  collapse_count = 1;
   while (cur_node = next_node) {
 
     next_node = WN_next(cur_node);
@@ -10696,6 +11106,10 @@
 	    WN_Delete ( cur_node );
 	  break;
 
+        case WN_PRAGMA_COLLAPSE:
+          collapse_count = WN_pragma_arg1(cur_node);
+          break;
+
 	default:
 	  Fail_FmtAssertion (
 		  "out of context pragma (%s) in MP {region pragma} processing",
@@ -10773,7 +11187,7 @@
     WN *nested_non_pod_finalization_nodes;
 
     if (non_pod_finalization_nodes) {
-        // In non-orphaned PDO, Process_Parallel_Region() will have
+        // In non-orphaned PDO, Process_MP_Region() will have
         // extracted non-POD finalization code (if any), so we re-insert it
       if (mpt != MPP_PDO)
         Fail_FmtAssertion("out of place non-POD finalization code");
@@ -10826,43 +11240,22 @@
 
     /* Determine user's real do index and type. */
 
-    do_index_st = WN_st(WN_index(pdo_node));
+    Extract_Index_Info(pdo_node);
 
-    do_index_type = TY_mtype(ST_type(do_index_st));
-    if (do_index_type == MTYPE_I1 || do_index_type == MTYPE_I2)
-      do_index_type = MTYPE_I4;
-    else if (do_index_type == MTYPE_U1 || do_index_type == MTYPE_U2)
-      do_index_type = MTYPE_U4;
-
+    if (collapse_count == 1) {
 #if defined(TARG_X8664) || defined(TARG_MIPS)
-    // Bug 7275 - this depends on the library implementation
-    if (MTYPE_byte_size(do_index_type) == 4)
+      // Bug 7275 - this depends on the library implementation
+      if (MTYPE_byte_size(do_index_type[0]) == 4)
 #else 
-    if (do_index_type == MTYPE_I4)
+      if (do_index_type[0] == MTYPE_I4)
 #endif
-    {
-/*        if (lastthread_node)
-           fast_doacross = TRUE;
-        else if (ordered_node)
-           fast_doacross = FALSE;
-        else if (mpsched_node)
-           fast_doacross =
-             (WN_pragma_arg1(mpsched_node) == WN_PRAGMA_SCHEDTYPE_SIMPLE);
-        else if (pu_mpsched_node)
-           fast_doacross =
-             (WN_pragma_arg1(pu_mpsched_node) == WN_PRAGMA_SCHEDTYPE_SIMPLE);
-        else if (chunk_node || pu_chunk_node)
-           fast_doacross = FALSE;
-        else
-*/           fast_doacross = TRUE;
-    }
-    else
+        fast_doacross = TRUE;
+      else
+        fast_doacross = FALSE;
+    } else {
       fast_doacross = FALSE;
-        
-//    if( orphaned )
-//    {
+    }      
     Make_Local_Temps( );
-//    }
     /* Translate do statement itself. */
 
     Extract_Do_Info ( pdo_node );
@@ -11024,62 +11417,6 @@
     WN *last = WN_last( return_wn );
     WN_INSERT_BlockAfter( body_block, prev_node, return_wn );
     prev_node = last;
-    // Why always 64?
-/*
-    if (nested_ordered_node && WN_pragma_omp(nested_ordered_node)) {
-      WN *order_begin;
-      order_begin = Gen_OMP_Pdo_Ordered_Begin(WN_kid0(nested_do_order_lb),
-                                              WN_kid0(nested_do_order_stride));
-      WN_INSERT_BlockAfter (body_block, prev_node, order_begin);
-      prev_node = order_begin;
-    }
-
-    if (numthreads_node)
-      wn = Gen_MP_BeginPDO_64 ( base_node, ntrip_node, stride_node,
-				WN_CreateIntconst ( OPC_I4INTCONST,
-						    num_constructs ),
-				WN_COPY_Tree ( WN_kid0(numthreads_node) ),
-				mpsched_wn, chunk_wn, is_omp );
-    else
-      wn = Gen_MP_BeginPDO_64 ( base_node, ntrip_node, stride_node,
-				WN_CreateIntconst ( OPC_I4INTCONST,
-						    num_constructs ),
-				WN_CreateIntconst ( OPC_I4INTCONST, 0 ),
-				mpsched_wn, chunk_wn, is_omp );
-
-
-    WN_INSERT_BlockAfter(body_block, prev_node, wn);
-    prev_node = wn;
-*/
-/*    wn1 = Gen_MP_NextIters_64 ( WN_CreateIntconst ( OPC_I4INTCONST,
-						    num_constructs ),
-				WN_Lda ( Pointer_type, 0, mpbase_st ),
-				WN_Lda ( Pointer_type, 0, mptrips_st ),
-				WN_Lda ( Pointer_type, 0, mpflags_st ),
-                                is_omp );
-    WN_INSERT_BlockAfter(body_block, prev_node, wn1);
-    prev_node = wn1;
-
-    Create_Preg_or_Temp ( MTYPE_I4, "mpni_status", &return_st, &return_ofst );
-    GET_RETURN_PREGS(rreg1, rreg2, MTYPE_I4);
-    wn2 = WN_Stid ( MTYPE_I4, return_ofst, return_st, ST_type(return_st),
-		    WN_LdidPreg ( MTYPE_I4, rreg1 ));
-    WN_linenum(wn2) = line_number;
-    WN_INSERT_BlockAfter(body_block, prev_node, wn2);
-    prev_node = wn2;
-
-    while_block = WN_CreateBlock ( );
-    WN_INSERT_BlockLast ( while_block, do_prefix );
-    WN_INSERT_BlockLast ( while_block, pdo_node );
-    WN_INSERT_BlockLast ( while_block, WN_COPY_Tree ( wn1 ) );
-    WN_INSERT_BlockLast ( while_block, WN_COPY_Tree ( wn2 ) );
-    wn = WN_CreateWhileDo ( WN_Ldid ( MTYPE_I4, return_ofst, return_st,
-				      ST_type(return_st) ),
-			    while_block );
-    WN_linenum(wn) = line_number;
-    WN_INSERT_BlockAfter(body_block, prev_node, wn);
-    prev_node = wn;
- */ 
 #ifdef KEY
     if (code_after_pdo) {
         // insert post-PDO sandwich code after all lowered PDO code
@@ -11728,11 +12065,11 @@
 /*  Process the contents of a parallel region.  */
 
 static void 
-Process_Parallel_Region ( void )
+Process_MP_Region ( void )
 {
   WN *wn, *reduction_init_block = NULL, *reduction_store_block = NULL;
 
-  Is_True(mpt == MPP_PARALLEL_REGION, ("not in a PARALLEL region"));
+  Is_True(mpt == MPP_PARALLEL_REGION || mpt == MPP_TASK_REGION, ("not in a PARALLEL region or TASK region"));
 
   /* Initialization. */
 
@@ -11754,7 +12091,7 @@
 
 
   Push_Some_Globals( );
-  Create_MicroTask( PAR_FUNC_REGION );
+  Create_MicroTask( mpt == MPP_PARALLEL_REGION ? PAR_FUNC_REGION : PAR_FUNC_TASK );
   Delayed_MP_Translation( stmt_block );
 
   /* Create any needed local temps. */
@@ -11793,14 +12130,15 @@
   }
 
   Transform_Parallel_Block ( stmt_block );
-  Verify_No_MP(stmt_block);
 #ifdef KEY
   Gen_Threadpriv_Func(reference_block, parallel_func, FALSE);
 #endif
 
-  if (reduction_count)  /* Generate init/finish reduction code */
+  if (reduction_count) {  /* Generate init/finish reduction code */
+    Is_True(mpt == MPP_PARALLEL_REGION, ("reduction only allowed in parallel region"));
     Gen_MP_Reduction(var_table, local_count, &reduction_init_block,
                      &reduction_store_block);
+  }
 
   /* Consolidate all portions of nested parallel procedure */
 
@@ -11811,6 +12149,8 @@
 			                  0, 0 ));
   if (firstprivate_block)
     WN_INSERT_BlockLast ( parallel_func, firstprivate_block );
+  if (mpt == MPP_TASK_REGION)
+    WN_INSERT_BlockLast ( parallel_func, Gen_Task_Body_Start() );
   if (reduction_init_block)
     WN_INSERT_BlockLast ( parallel_func, reduction_init_block );
   WN_INSERT_BlockLast ( parallel_func, stmt_block );
@@ -11823,6 +12163,11 @@
   WN_linenum(wn) = line_number;
   WN_INSERT_BlockLast ( parallel_func, wn );
 
+  /* Generate code to call __ompc_task_exit() before any return */
+
+  if (mpt == MPP_TASK_REGION)
+    Insert_Task_Exits(parallel_func);
+
   /* Transfer any mappings for nodes moved from parent to parallel function */
 
   Transfer_Maps ( pmaptab, cmaptab, parallel_func, 
@@ -11858,7 +12203,7 @@
   Current_pu = &Current_PU_Info_pu();
   Current_Map_Tab = pmaptab;
   Pop_Some_Globals( );
-} // Process_Parallel_Region()
+} // Process_MP_Region()
 
 // Localize variables in serialzed version of parallel region
 static void Localize_in_serialized_parallel (void)
@@ -11875,6 +12220,7 @@
                                           &non_pod_finalization_nodes );
 }
 
+
 /*  This is the main routine used to process parallel do's and parallel
     regions.  It calls support routines to handle the contents of the
     parallel do/region and copyin, but this code does everything else.  */
@@ -11900,6 +12246,7 @@
   WN   *chunk_wn;		/* Real wn for chunk node */
   WN   *mp_call_wn;		/* Real wn for mp call */
   WN   *if_cond_wn;		/* Real wn for if condition */
+  BOOL mp_if;			/* MP if transformation flag */
   ST   *lock_st;		/* ST for critical section lock */
   ST   *ntrip_st;		/* ST for loop trip count */
   ST   *return_st;		/* ST for mp status return */
@@ -11908,7 +12255,6 @@
   PREG_NUM rreg1, rreg2;	/* Pregs with I4 return values */
   INT32 num_criticals;		/* Number of nested critical sections */
   BOOL  while_seen;		/* While seen where do should be */
-  BOOL  mp_if;			/* MP if transformation flag */
 
   /* Validate input arguments. */
 
@@ -11981,6 +12327,8 @@
   firstprivate_block = NULL;
   liveout_block      = NULL;
   if_cond_wn         = NULL;
+  untied             = FALSE;
+  collapse_count     = 1;
   if_preamble_block  = NULL;
   if_postamble_block = NULL;
   do_preamble_block  = NULL;
@@ -12059,8 +12407,7 @@
       WN_Delete( store_gtid );
       return return_wn;
 
-    } 
-    else if (WN_pragma(node) == WN_PRAGMA_TASKWAIT) {
+    } else if (WN_pragma(node) == WN_PRAGMA_TASKWAIT) {
       //orphaned taskwait, cca 06/27/08
       WN *call, *block;
       wn = WN_next(node);
@@ -12075,8 +12422,7 @@
       WN_DELETE_Tree(node);
       WN_Delete(block);
       return return_wn;
-    }
-    else if (WN_pragma(node) == WN_PRAGMA_CHUNKSIZE) {
+    } else if (WN_pragma(node) == WN_PRAGMA_CHUNKSIZE) {
       pu_chunk_node = node;
       return (WN_next(node));
 
@@ -12152,7 +12498,7 @@
     case WN_PRAGMA_SINGLE_PROCESS_BEGIN:
       ++num_constructs;
       mpt = MPP_ORPHANED_SINGLE;
-      Strip_Nested_MP(WN_region_body(node), FALSE);
+      Strip_Nested_MP(WN_region_body(node), TRUE);
       wn = Gen_MP_SingleProcess_Region(node);
       if ((WN_next(wn) = WN_next(node)) != NULL)
         WN_prev(WN_next(wn)) = wn;
@@ -12189,6 +12535,10 @@
       mpt = MPP_PARALLEL_REGION;
       break;
 
+    case WN_PRAGMA_TASK_BEGIN:
+      mpt = MPP_TASK_REGION;
+      break;
+
     default:
       printf("pragma value = %d", (int)wid); /* for test. by jhs,02.9.3 */
       Fail_FmtAssertion (
@@ -12456,11 +12806,18 @@
 	case WN_PRAGMA_DEFAULT:
 	  break;
 	  
-	  default:
-	    Fail_FmtAssertion (
-	       "out of context pragma (%s) in MP {top-level pragma} processing",
-	       WN_pragmas[WN_pragma(cur_node)].name);
+        case WN_PRAGMA_UNTIED:
+          untied = TRUE;
+          break;
 
+        case WN_PRAGMA_COLLAPSE:
+          collapse_count = WN_pragma_arg1(cur_node);
+          break;
+
+        default:
+          Fail_FmtAssertion ("out of context pragma (%s) in MP {top-level pragma} processing",
+                             WN_pragmas[WN_pragma(cur_node)].name);
+
 	}
 
       }
@@ -12607,38 +12964,21 @@
     line_number = WN_linenum(do_node);
     Set_Error_Line(line_number);
 
-    do_index_st = WN_st(WN_index(do_node));
+    Extract_Index_Info(do_node);
 
-    do_index_type = TY_mtype(ST_type(do_index_st));
-    if (do_index_type == MTYPE_I1 || do_index_type == MTYPE_I2)
-      do_index_type = MTYPE_I4;
-    else if (do_index_type == MTYPE_U1 || do_index_type == MTYPE_U2)
-      do_index_type = MTYPE_U4;
-
+    if (collapse_count == 1) {
 #if defined(TARG_X8664) || defined(TARG_MIPS)
-    // Bug 7275 - this depends on the library implementation
-    if (MTYPE_byte_size(do_index_type) == 4)
+      // Bug 7275 - this depends on the library implementation
+      if (MTYPE_byte_size(do_index_type[0]) == 4)
 #else 
-    if (do_index_type == MTYPE_I4)
+        if (do_index_type[0] == MTYPE_I4)
 #endif
-    {
-/*      if (lastthread_node)
-        fast_doacross = TRUE;
-      else if (ordered_node)
-        fast_doacross = FALSE;
-      else if (mpsched_node)
-        fast_doacross =
-           (WN_pragma_arg1(mpsched_node) == WN_PRAGMA_SCHEDTYPE_SIMPLE);
-      else if (pu_mpsched_node)
-        fast_doacross =
-            (WN_pragma_arg1(pu_mpsched_node) == WN_PRAGMA_SCHEDTYPE_SIMPLE);
-      else if (chunk_node || pu_chunk_node)
-        fast_doacross = FALSE;
-      else
-*/        fast_doacross = TRUE;
+          fast_doacross = TRUE;
+        else
+          fast_doacross = FALSE;
+    } else {
+      fast_doacross = FALSE;
     }
-    else
-      fast_doacross = FALSE;
 
     vsize = (local_count + 1) * sizeof(VAR_TABLE);
     var_table = (VAR_TABLE *) alloca ( vsize );
@@ -12691,17 +13031,19 @@
     /* do this always -- serial portion may have had a begin/end ordered */
     Cleanup_Ordered (serial_stmt_block, serial_stmt_block);
 
-    Create_Preg_or_Temp ( do_index_type, "trip_count", &ntrip_st, &ntrip_ofst );
-    temp_node = WN_Stid ( do_index_type, ntrip_ofst, ntrip_st, 
-		    ST_type(ntrip_st), ntrip_node );
-    WN_linenum(temp_node) = line_number;
+    if (collapse_count == 1) {
+      Create_Preg_or_Temp ( do_index_type[0], "trip_count", &ntrip_st, &ntrip_ofst );
+      temp_node = WN_Stid ( do_index_type[0], ntrip_ofst, ntrip_st, 
+                            ST_type(ntrip_st), ntrip_node );
+      WN_linenum(temp_node) = line_number;
 #ifdef KEY
-    if (!ntrip_calc)
+      if (!ntrip_calc)
 #endif
-    ntrip_calc = WN_CreateBlock ( );
-    WN_INSERT_BlockLast ( ntrip_calc, temp_node );
-    ntrip_node = WN_Ldid ( do_index_type, ntrip_ofst, ntrip_st,
-			   ST_type(ntrip_st) );
+        ntrip_calc = WN_CreateBlock ( );
+      WN_INSERT_BlockLast ( ntrip_calc, temp_node );
+      ntrip_node = WN_Ldid ( do_index_type[0], ntrip_ofst, ntrip_st,
+                             ST_type(ntrip_st) );
+    }
 
     if( numthreads_node )
     {
@@ -12788,7 +13130,7 @@
       serial_stmt_block = Copy_Non_MP_Tree ( stmt_block );
 
 
-    Process_Parallel_Region ( );
+    Process_MP_Region ( );
 
 #ifndef KEY // bug 7281
     if (is_omp) {
@@ -12798,7 +13140,7 @@
       if( numthreads_node )
       {
     	mp_call_wn = Gen_Fork ( parallel_proc, 
-          	WN_COPY_Tree ( WN_kid0 ( numthreads_node ))); 
+               WN_COPY_Tree ( WN_kid0 ( numthreads_node ))); 
       }
       else
       {
@@ -12832,8 +13174,31 @@
     RID_Delete ( Current_Map_Tab, node );
     WN_Delete ( node );
 
-  }
+  } else if (mpt == MPP_TASK_REGION) {
 
+    Collect_Default_Variables(stmt_block, firstprivate_nodes, WN_PRAGMA_FIRSTPRIVATE);
+
+    if (serial_stmt_block) {
+      Is_True(inside_versioning_if,
+              ("where did serial_stmt_block come from???"));
+      Move_Non_POD_Finalization_Code(serial_stmt_block);
+      Strip_Nested_MP ( serial_stmt_block, FALSE );
+    } else
+      serial_stmt_block = Copy_Non_MP_Tree ( stmt_block );
+
+    vsize = (local_count + 1) * sizeof(VAR_TABLE);
+    var_table = (VAR_TABLE *) alloca ( vsize );
+    BZERO ( var_table, vsize );
+
+    Process_MP_Region ( );
+
+    mp_call_wn = Gen_Task_Create ( parallel_proc );
+	
+    RID_Delete ( Current_Map_Tab, node );
+    WN_Delete ( node );
+    
+ }
+
   /* Build final code for parallel loops and regions. */
 
   if ((mpt == MPP_PARALLEL_DO) || (mpt == MPP_PARALLEL_REGION)) {
@@ -12879,11 +13244,11 @@
 	      WN_INSERT_BlockLast ( replace_block, ntrip_calc );
       if (if_node)
 	      if_cond_wn = WN_COPY_Tree ( WN_kid0(if_node) );
-      if (mpt == MPP_PARALLEL_DO) {
+      if (mpt == MPP_PARALLEL_DO && collapse_count == 1) {
            // The threshold can be more proper(other than 1). csc
-	      temp_node = WN_GT ( do_index_type, WN_Ldid ( do_index_type, ntrip_ofst,
+	      temp_node = WN_GT ( do_index_type[0], WN_Ldid ( do_index_type[0], ntrip_ofst,
 		                  ntrip_st, ST_type(ntrip_st) ), 
-                                  WN_Intconst ( do_index_type, 1 ));
+                                  WN_Intconst ( do_index_type[0], 1 ));
 	      if (if_cond_wn)
 	        if_cond_wn = WN_CAND ( if_cond_wn, temp_node );
 	      else
@@ -12995,6 +13360,8 @@
       WN_INSERT_BlockLast ( replace_block, do_preamble_block );
     WN_INSERT_BlockLast ( replace_block, stmt_block );
 
+  } else if (mpt == MPP_TASK_REGION) {
+    WN_INSERT_BlockLast ( replace_block, mp_call_wn );
   }
 
 
Index: be/com/wn_lower.cxx
===================================================================
--- be/com/wn_lower.cxx	(リビジョン 3578)
+++ be/com/wn_lower.cxx	(リビジョン 3580)
@@ -15352,12 +15352,12 @@
          WN *ld, *wn;
 #if defined(TARG_IA32) || defined(TARG_X8664)
 	 if (Is_Target_32bit()) {
-	   WN *formal = WN_formal(current_function, 1);// 2nd incoming parameter
+	   WN *formal = WN_formal(current_function, WN_num_formals(tree) - 1);// last incoming parameter
 	   ld = WN_Ldid(Pointer_type, 0, WN_st(formal), WN_type(formal));
 	 }
 	 else
 #endif
-         ld = WN_LdidPreg( Pointer_type, First_Int_Preg_Param_Offset+1 ); //$r33
+           ld = WN_LdidPreg( Pointer_type, First_Int_Preg_Param_Offset + WN_num_formals(tree) - 1); //$r33
                                                                                                                                                              
          wn = WN_Stid( Pointer_type, 0, slink, ST_type( slink ), ld );
                                                                                                                                                              
@@ -15451,6 +15451,7 @@
       WN_INSERT_BlockFirst(WN_func_body(tree), block);
     }
 #endif
+
     return tree;
   }
   else
Index: be/com/wn_verifier.cxx
===================================================================
--- be/com/wn_verifier.cxx	(リビジョン 3578)
+++ be/com/wn_verifier.cxx	(リビジョン 3580)
@@ -240,6 +240,7 @@
   MEM_POOL_Initialize(&_mem_pool, "Verifier_Pool", FALSE);
   MEM_POOL_Push(&_mem_pool);
   _map = WN_MAP_Create(&_mem_pool);
+  WN_MAP_Set_dont_copy(_map, TRUE);
   
   // Empty WHIRL tree is OK
 
Index: be/com/be_memop_annot.cxx
===================================================================
--- be/com/be_memop_annot.cxx	(リビジョン 3578)
+++ be/com/be_memop_annot.cxx	(リビジョン 3580)
@@ -264,6 +264,7 @@
   MEMOP_ANNOT_MGR::Init (); 
   if (_wn_map != WN_MAP_UNDEFINED) { WN_MAP_Delete (_wn_map);}
   _wn_map = WN_MAP_Create (_mp);
+  WN_MAP_Set_dont_copy(_wn_map, TRUE);
 }
 
 WN_MEMOP_ANNOT_MGR::WN_MEMOP_ANNOT_MGR (MEM_POOL* mp): 
Index: be/com/wn_mp.h
===================================================================
--- be/com/wn_mp.h	(リビジョン 3578)
+++ be/com/wn_mp.h	(リビジョン 3580)
@@ -92,7 +92,7 @@
 extern "C" {
 #endif
 
-extern WN * lower_mp (WN *, WN *, INT32);
+extern WN * lower_mp (WN * block, WN * node, INT32 actions);
 extern void LowerMP_PU_Init (void);
 
 extern WN * Gen_MP_Getlock ( ST * lock );
@@ -117,6 +117,7 @@
   MPP_ORPHANED_PDO,
   MPP_PARALLEL_DO,
   MPP_PARALLEL_REGION,
+  MPP_TASK_REGION,
   MPP_MASTER,
   MPP_ORPHANED_MASTER,
 #ifdef KEY /* Bug 4828 */
Index: be/com/opt_alias_mgr.cxx
===================================================================
--- be/com/opt_alias_mgr.cxx	(リビジョン 3578)
+++ be/com/opt_alias_mgr.cxx	(リビジョン 3580)
@@ -454,7 +454,9 @@
   _vec = CXX_NEW(DYN_ARRAY<POINTS_TO*>(&_mem_pool), &_mem_pool);
   _vec->Initidx(_preg_id);   // do not use the 0-entry and 1-entry.
   _map = WN_MAP32_Create(&_mem_pool);
+  WN_MAP_Set_dont_copy(_map, TRUE);
   _homing_map = WN_MAP32_Create(&_mem_pool);
+  WN_MAP_Set_dont_copy(_homing_map, TRUE);
   _last_alias_id = _preg_id;   // starting from preg id.
   _no_alias_info_id = New_alias_id();
 
Index: be/com/privatize_common.cxx
===================================================================
--- be/com/privatize_common.cxx	(リビジョン 3578)
+++ be/com/privatize_common.cxx	(リビジョン 3580)
@@ -471,6 +471,12 @@
   new_name = (char *) alloca(strlen(old_name) + 32);
   sprintf(new_name, "__thdprv_common_%s", old_name);
 
+  ST_TAB* global_st_tab = Scope_tab[GLOBAL_SYMTAB].st_tab;
+  for (ST_TAB::iterator iter = global_st_tab->begin(); iter != global_st_tab->end(); ++iter) {
+    if (strcmp(ST_name(*iter), new_name) == 0)
+      return &*iter;
+  }
+
   ST *new_thdprv_st = New_ST(GLOBAL_SYMTAB); 
 
   ST_SCLASS sclass = ((ST_sclass (old_st) == SCLASS_FSTATIC

属性に変更があったパス: be
___________________________________________________________________
追加: svn:mergeinfo
   /branches/config/osprey/be:r3003-3087 をマージしました
   /branches/open64-booster/osprey/be:r2027-3263 をマージしました
   /trunk/osprey/be:r1120-3577 をマージしました
   /branches/openmp3.0/osprey/be:r3478-3556 をマージしました
   /branches/open64-ppc32/osprey/be:r2293-2750 をマージしました
   /branches/nextgenalias/osprey/be:r2766-3433 をマージしました

Index: kgccfe/gnu/c-parse.y
===================================================================
--- kgccfe/gnu/c-parse.y	(リビジョン 3578)
+++ kgccfe/gnu/c-parse.y	(リビジョン 3580)
@@ -2348,7 +2348,7 @@
         |  atomic_construct
         |  ordered_construct
         |  sl2_sections_construct
-        |  task_construct
+	|  task_construct
         ;
 
 sl2_sections_construct:
@@ -2434,11 +2434,12 @@
         }      	   
         ;
 
+                                                                                
 pragma_directives:
         barrier_directive
         | flush_directive
         | threadprivate_directive
-    {}
+	{}
 	| taskwait_directive
         | options_directive
 	| exec_freq_directive
@@ -2722,6 +2723,7 @@
 	  add_stmt (build_omp_stmt (task_cons_e, NULL));
 	  $$ = NULL;
 	}
+    ;
 
 task_directive:
         PRAGMA_OMP TASK '\n'
Index: kgccfe/omp_types.h
===================================================================
--- kgccfe/omp_types.h	(リビジョン 3578)
+++ kgccfe/omp_types.h	(リビジョン 3580)
@@ -65,7 +65,7 @@
   exec_freq_dir,
   task_cons_b,
   task_cons_e,
-  taskwait_dir
+  taskwait_dir,
 };
 
 ///////////////////////////////////////////////////////////////////

属性に変更があったパス: kgccfe
___________________________________________________________________
追加: svn:mergeinfo
   /trunk/osprey/kgccfe:r1120-3577 をマージしました
   /branches/openmp3.0/osprey/kgccfe:r3478-3556 をマージしました
   /branches/open64-ppc32/osprey/kgccfe:r2293-2750 をマージしました
   /branches/nextgenalias/osprey/kgccfe:r2766-3433 をマージしました
   /branches/config/osprey/kgccfe:r3003-3087 をマージしました
   /branches/open64-booster/osprey/kgccfe:r2027-3263 をマージしました

Index: common/com/wn_util.cxx
===================================================================
--- common/com/wn_util.cxx	(リビジョン 3578)
+++ common/com/wn_util.cxx	(リビジョン 3580)
@@ -1941,7 +1941,8 @@
 
       if (need_pragma == TRUE &&
           (Pragma_is_Parallel_Region(pragma) ||
-           Pragma_is_Work_Sharing(pragma))) {
+           Pragma_is_Work_Sharing(pragma) ||
+           pragma == WN_PRAGMA_TASK_BEGIN)) {
       
         WN *local_pwn = WN_CreatePragma (pragma_id, st, ofst, 0);
         if (make_compiler_generated) {
Index: common/com/wn_pragmas.h
===================================================================
--- common/com/wn_pragmas.h	(リビジョン 3578)
+++ common/com/wn_pragmas.h	(リビジョン 3580)
@@ -381,6 +381,7 @@
   WN_PRAGMA_TASK_END, /* by cca, 11/20/2007 */
   WN_PRAGMA_TASKWAIT, /*by cca, 11/20/2007 */
   WN_PRAGMA_UNTIED, /*by cca, 11/20/2007 */
+  WN_PRAGMA_COLLAPSE,
   MAX_WN_PRAGMA			/* last one in enum			*/
 } WN_PRAGMA_ID;
 
Index: common/com/wn_pragmas.cxx
===================================================================
--- common/com/wn_pragmas.cxx	(リビジョン 3578)
+++ common/com/wn_pragmas.cxx	(リビジョン 3580)
@@ -98,6 +98,7 @@
   { PUSER_MP, WN_PRAGMA_SCOPE_POINT,	"COPYIN" },
   { PUSER_MP, WN_PRAGMA_SCOPE_ON,	"CRITICAL_SECTION_BEGIN" },
   { PUSER_MP, WN_PRAGMA_SCOPE_OFF,	"CRITICAL_SECTION_END" },
+
   { PUSER_MP, WN_PRAGMA_SCOPE_POINT,	"DOACROSS" },
   { PUSER_MP, WN_PRAGMA_SCOPE_SPECIAL,	"IF" },
   { PUSER_MP, WN_PRAGMA_SCOPE_SPECIAL,	"LASTLOCAL" },
@@ -268,12 +269,11 @@
   { PUSER_MP, WN_PRAGMA_SCOPE_ON,	"PWORKSHARE_BEGIN" }, /* by jhs, 04.3.10 */
   { PUSER_MP, WN_PRAGMA_SCOPE_OFF,	"PWORKSHARE_END" }, /* by jhs, 04.3.10 */
   { PUSER_MP, WN_PRAGMA_SCOPE_SPECIAL,  "THREADPRIVATE" }, /* by jhs, 02.9.18 */
-#if 1
-  { PUSER_MP, WN_PRAGMA_SCOPE_ON, "BEGIN_TASK" }, /* by cca, 11/21/2007 */
-  { PUSER_MP, WN_PRAGMA_SCOPE_OFF, "END_TASK" }, /* by cca, 11/21/2007 */
+  { PUSER_MP, WN_PRAGMA_SCOPE_ON, "TASK_BEGIN" }, /* by cca, 11/21/2007 */
+  { PUSER_MP, WN_PRAGMA_SCOPE_OFF, "TASK_END" }, /* by cca, 11/21/2007 */
   { PUSER_MP, WN_PRAGMA_SCOPE_POINT, "TASKWAIT" }, /* by cca, 11/21/2007 */
   { PUSER_MP, WN_PRAGMA_SCOPE_SPECIAL, "UNTIED" }, /* by cca, 11/21/2007 */
-#endif
+  { PUSER_MP, WN_PRAGMA_SCOPE_SPECIAL, "COLLAPSE" },
 
 
   { PUSER_NULL,	WN_PRAGMA_SCOPE_UNKNOWN, NULL }	/* MAX_WN_PRAGMA */

属性に変更があったパス: common
___________________________________________________________________
追加: svn:mergeinfo
   /branches/nextgenalias/osprey/common:r2766-3433 をマージしました
   /branches/config/osprey/common:r3003-3087 をマージしました
   /branches/open64-booster/osprey/common:r2027-3263 をマージしました
   /trunk/osprey/common:r1120-3577 をマージしました
   /branches/openmp3.0/osprey/common:r3478-3556 をマージしました
   /branches/open64-ppc32/osprey/common:r2293-2750 をマージしました

Index: wgen/omp_directive.h
===================================================================
--- wgen/omp_directive.h	(リビジョン 3578)
+++ wgen/omp_directive.h	(リビジョン 3580)
@@ -39,7 +39,11 @@
 
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////////
+extern void expand_start_task (gs_t);
+extern void expand_end_task ( );
 
+///////////////////////////////////////////////////////////////////////////////////////////////////////
+
 extern void expand_start_for (gs_t stmt);
 extern void expand_end_for (void);
 
Index: wgen/wgen_omp_check_stack.h
===================================================================
--- wgen/wgen_omp_check_stack.h	(リビジョン 3578)
+++ wgen/wgen_omp_check_stack.h	(リビジョン 3580)
@@ -32,6 +32,7 @@
    wgen_cscf,
    wgen_other_ompscope,
    wgen_omp_parallel,
+   wgen_omp_task,
    wgen_omp_for,
    wgen_omp_single,
    wgen_omp_sections,
@@ -42,6 +43,7 @@
    wgen_omp_critical,
    wgen_omp_ordered,
    wgen_omp_barrier,
+   wgen_omp_taskwait,
    wgen_omp_flush,
    wgen_omp_atomic,
    wgen_omp_threadprivate
@@ -58,7 +60,8 @@
    clause_private        =0x80,
    clause_reduction      =0x100,
    clause_schedule      =0x200,
-   clause_shared        =0x400   
+   clause_shared        =0x400,
+   clause_untied        =0x800
 
 }WGEN_CLAUSE_KIND;
 
Index: wgen/wgen_omp_directives.cxx
===================================================================
--- wgen/wgen_omp_directives.cxx	(リビジョン 3578)
+++ wgen/wgen_omp_directives.cxx	(リビジョン 3580)
@@ -819,6 +819,16 @@
     }
     break;
 
+    case GS_OMP_CLAUSE_UNTIED:
+      WGEN_Set_Cflag(clause_untied); 
+      wn = WN_CreatePragma(WN_PRAGMA_UNTIED, (ST_IDX)NULL, 0, 0);
+      break;
+
+    case GS_OMP_CLAUSE_COLLAPSE:
+      wn = WN_CreateXpragma(WN_PRAGMA_COLLAPSE, (ST_IDX) NULL, 1);
+      WN_kid0(wn) = WGEN_Expand_Expr (gs_omp_clause_collapse_level(clauses));
+      break;
+
     default:
       DevWarn ("WGEN_process_omp_clause: unhandled OpenMP clause");
   }
@@ -835,6 +845,16 @@
   }
 }
 
+void WGEN_process_omp_for_collapse (gs_t stmt, WN * region)
+{
+  UINT collapse_count = gs_tree_vec_length(gs_omp_for_init(stmt));
+  if (collapse_count > 1) {
+    WN * collapse = WN_CreatePragma(WN_PRAGMA_COLLAPSE, (ST *)NULL, collapse_count, 0);
+    WN_set_pragma_omp(collapse);
+    WN_INSERT_BlockLast(WN_region_pragmas(region), collapse);
+  }
+}
+
 void WGEN_expand_start_parallel (gs_t stmt)
 {
   /* create a region on current block */
@@ -898,6 +918,68 @@
 
 
 
+
+void WGEN_expand_start_task (gs_t stmt)
+{
+  /* create a region on current block */
+       
+  WN * region = WGEN_region(REGION_KIND_MP);
+
+  WN *wn;
+
+  wn = WN_CreatePragma(WN_PRAGMA_TASK_BEGIN, 
+       	                     (ST_IDX) NULL, 
+       	                     0, 
+       	                     0);   
+  WN_set_pragma_omp(wn);
+  WGEN_Stmt_Append (wn, Get_Srcpos());
+       
+  ///// omp check stack action ///////
+  SRCPOS srcpos = Get_Srcpos();
+  WGEN_CS_push(wgen_omp_task, SRCPOS_linenum(srcpos),
+               SRCPOS_filenum(srcpos));
+  WGEN_Set_Prag(WGEN_Stmt_Top());
+  WGEN_Set_Region (region);
+
+
+  /////required?///////
+  Set_PU_has_mp (Get_Current_PU ());
+  Set_FILE_INFO_has_mp (File_info);
+  Set_PU_uplevel (Get_Current_PU ());
+
+  gs_t clauses = gs_omp_task_clauses (stmt);
+
+  for (; clauses; clauses = gs_omp_clause_chain(clauses))
+    WGEN_process_omp_clause(clauses, region);
+
+  WGEN_Stmt_Pop (wgen_stmk_region_pragmas);
+
+  if (lang_cplus)
+  {
+    if (!dtor_call_stack.empty() &&
+        WN_operator (dtor_call_stack.top()) == OPR_CALL)
+      dtor_call_stack.push (region);
+
+    if (!local_node_stack.empty() &&
+        WN_operator (local_node_stack.top()) == OPR_PRAGMA)
+      local_node_stack.push (region);
+  }
+}
+
+void WGEN_expand_end_task ()
+{
+    if (lang_cplus)
+    {
+      WN * wn = WGEN_Stmt_Top ();
+      WGEN_maybe_call_dtors (wn);
+      WGEN_maybe_localize_vars (wn);
+      WGEN_maybe_do_eh_cleanups ();
+    }
+
+    WGEN_Stmt_Pop (wgen_stmk_scope);
+    WGEN_CS_pop (wgen_omp_task);
+};
+
 static WN * Setup_MP_Enclosing_Region (void);
 
 void WGEN_expand_start_for (gs_t stmt)
@@ -957,6 +1039,8 @@
   for (; clauses; clauses = gs_omp_clause_chain(clauses))
     WGEN_process_omp_clause(clauses, region);
 
+  WGEN_process_omp_for_collapse(stmt, region);
+
   WGEN_Stmt_Pop (wgen_stmk_region_pragmas);
 
   if (lang_cplus)
@@ -1278,6 +1362,8 @@
   for (; clauses; clauses = gs_omp_clause_chain(clauses))
     WGEN_process_omp_clause(clauses);
 
+  WGEN_process_omp_for_collapse(gs_bind_expr_body(gs_omp_parallel_body(stmt)), region);
+
   // Now process any clauses in enclosed FOR pragma
   stmt = gs_omp_parallel_body(stmt);
   Is_True (gs_tree_code(stmt) == GS_BIND_EXPR,
@@ -1732,8 +1818,31 @@
        WGEN_CS_pop(wgen_omp_barrier);
 }
 
+void  WGEN_expand_taskwait ()
+{
+       WN *wn;
+       wn = WN_CreatePragma(WN_PRAGMA_TASKWAIT, 
+                             (ST_IDX) NULL,
+                             0,
+                             0);   
+       WN_set_pragma_omp(wn);
+       WGEN_Stmt_Append (wn, Get_Srcpos());
+       //////////////// OPENMP CHECK STACK /////////////
+       SRCPOS srcpos = Get_Srcpos();
+       WGEN_CS_push(wgen_omp_taskwait,SRCPOS_linenum(srcpos), SRCPOS_filenum(srcpos));
+    
+    //   WGEN_check_barrier();
+ 
+       /////required?///////
+       Set_PU_has_mp (Get_Current_PU ());
+       Set_FILE_INFO_has_mp (File_info);
+       Set_PU_uplevel (Get_Current_PU ());
+       
+       WGEN_CS_pop(wgen_omp_taskwait);
+}
 
 
+
 // Generate OMP non-pod finalization code required for lastprivate
 // variables.
 // Generate:
Index: wgen/wgen_expr.cxx
===================================================================
--- wgen/wgen_expr.cxx	(リビジョン 3578)
+++ wgen/wgen_expr.cxx	(リビジョン 3580)
@@ -234,6 +234,7 @@
  GS_OMP_MASTER,			OPERATOR_UNKNOWN,
  GS_OMP_ORDERED,		OPERATOR_UNKNOWN,
  GS_OMP_PARALLEL,		OPERATOR_UNKNOWN,
+ GS_OMP_TASK,			OPERATOR_UNKNOWN,
  GS_OMP_SECTION,		OPERATOR_UNKNOWN,
  GS_OMP_SECTIONS,		OPERATOR_UNKNOWN,
  GS_OMP_SINGLE,			OPERATOR_UNKNOWN,
@@ -8796,6 +8797,11 @@
                 WGEN_expand_barrier ();
                 whirl_generated = TRUE;
                 break;
+
+              case GSBI_BUILT_IN_GOMP_TASKWAIT:
+                WGEN_expand_taskwait ();
+                whirl_generated = TRUE;
+                break;
 #endif
 
               case GSBI_BUILT_IN_RETURN_ADDRESS:
@@ -10455,6 +10461,7 @@
     case GS_OMP_MASTER:
     case GS_OMP_ORDERED:
     case GS_OMP_PARALLEL:
+    case GS_OMP_TASK:
     case GS_OMP_SECTIONS:
     case GS_OMP_SINGLE:
       WGEN_Expand_Stmt(exp);
Index: wgen/wgen_omp_directives.h
===================================================================
--- wgen/wgen_omp_directives.h	(リビジョン 3578)
+++ wgen/wgen_omp_directives.h	(リビジョン 3580)
@@ -44,6 +44,12 @@
 WGEN_expand_end_parallel ();
 
 extern void 
+WGEN_expand_start_task (gs_t stmt);
+
+extern void 
+WGEN_expand_end_task ();
+
+extern void 
 WGEN_expand_start_for (gs_t stmt);
 
 extern void 
@@ -93,6 +99,8 @@
 
 extern void  WGEN_expand_barrier ( );
 
+extern void  WGEN_expand_taskwait ( );
+
 extern void  WGEN_expand_flush (WN_list *flush_variables);
 
 extern void WGEN_expand_start_do_loop (WN *, WN *, WN *, WN *);
Index: wgen/omp_directive.cxx
===================================================================
--- wgen/omp_directive.cxx	(リビジョン 3578)
+++ wgen/omp_directive.cxx	(リビジョン 3580)
@@ -112,8 +112,23 @@
      WGEN_expand_end_parallel ();
 }
 
+void
+expand_start_task (gs_t stmt)
+{
+     WGEN_expand_start_task (stmt);
+     WGEN_Expand_Stmt (gs_omp_task_body(stmt));
+     expand_end_task ();
 
+}
 
+void
+expand_end_task ()
+{
+     WGEN_expand_end_task ();
+}
+
+
+
 extern void WGEN_Expand_DO (gs_t);
 
 void
Index: wgen/wgen_stmt.cxx
===================================================================
--- wgen/wgen_stmt.cxx	(リビジョン 3578)
+++ wgen/wgen_stmt.cxx	(リビジョン 3580)
@@ -4024,6 +4024,10 @@
       expand_start_parallel_or_combined_parallel (stmt);
       break;
 
+    case GS_OMP_TASK:
+      expand_start_task (stmt);
+      break;
+
     case GS_OMP_CRITICAL:
       expand_start_critical (stmt);
       break;
@@ -4066,6 +4070,7 @@
 {
   Is_True (gs_tree_code(stmt) == GS_OMP_FOR, ("Unexpected tree code"));
   gs_t init, incr, cond, body;
+  gs_int_t nest_cnt;
 
   WGEN_Record_Loop_Switch  (GS_DO_STMT);
 
@@ -4073,7 +4078,14 @@
   incr = gs_omp_for_incr (stmt);
   cond = gs_omp_for_cond (stmt);
   body = gs_omp_for_body (stmt);
-  expand_start_do_loop (init, cond, incr);
+
+  nest_cnt = gs_tree_vec_length(init);
+  Is_True (nest_cnt == gs_tree_vec_length(incr), ("The length of incr vector and init vector should match"));
+  Is_True (nest_cnt == gs_tree_vec_length(cond), ("The length of cond vector and init vector should match"));
+
+  for (int i = nest_cnt - 1; i >= 0; i--)
+    expand_start_do_loop (gs_tree_vec_elt(init, i), gs_tree_vec_elt(cond, i), gs_tree_vec_elt(incr, i));
+
   while (body)
   {
     WGEN_Expand_Stmt (body);
@@ -4092,7 +4104,8 @@
   }
 
   // loop ends
-  expand_end_do_loop ();
+  for (int i = 0; i < nest_cnt; i++)
+    expand_end_do_loop ();
 
   // label for break
   if (break_continue_info_stack [break_continue_info_i].break_label_idx)
@@ -4282,6 +4295,7 @@
 
 #ifdef FE_GNU_4_2_0
     case GS_OMP_PARALLEL:
+    case GS_OMP_TASK:
     case GS_OMP_CRITICAL:
     case GS_OMP_SECTION:
     case GS_OMP_SECTIONS:

属性に変更があったパス: wgen
___________________________________________________________________
追加: svn:mergeinfo
   /trunk/osprey/wgen:r1120-3577 をマージしました
   /branches/openmp3.0/osprey/wgen:r3478-3556 をマージしました
   /branches/open64-ppc32/osprey/wgen:r2293-2750 をマージしました
   /branches/nextgenalias/osprey/wgen:r2766-3433 をマージしました
   /branches/config/osprey/wgen:r3003-3087 をマージしました
   /branches/open64-booster/osprey/wgen:r2027-3263 をマージしました

