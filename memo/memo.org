* TODO
  - [X] deque の実装・テスト
  - [ ] deque の push/pop/take のマイクロベンチマーク
    - もしあまりにもMassiveThreadsより遅ければ，関数のインライン化も考える
    - 「飽くまでもテスト実装で，全体の動きを把握することを目的とする」のであれば，先に全体を作るのもあり

  - [ ] task の実装・テスト

  - [ ] worker の実装・テスト

  - [ ] team の実装・テスト

  - [ ] 全コード内のTODOの解消

* 実装のアイディア
** callstack
   *callstackはOMP_STACKSIZEによって調整されることが仕様によって定まっている*

** カーネルレベルスレッド
   - pthreadでおkかな?
     - Nanos4もそうしてるみたい

   - sched_setaffinity() とかで，各スレッドがどのCPUにバインドされるかを確定させるのが望ましそう
     - Nanos4でもそうしてる雰囲気はある (nth_main.c)

   - sched_setaffinity() はLinux固有のシステムコールらしいけど，その辺どうなんだろう

** ユーザレベルスレッド

* Nanos4 code reading
** nth_bf_sched.c
   文字通り，breadth-firstのデフォルトscheduler．とても読みやすい
*** default_sp_schedule()
    - local queue -> team queue -> global queue の順にtaskを探し，得られたtaskを返している
    - 引数の from_desc は threadに対応している感じ
*** default_enqueue_desc()
    - tied task は，生成された際親と同じqueueに入る様子が分かる
    - untied task の実装がおざなりなのも分かるw

** nthreads.c
*** nth_create_task_ci
    新しくtaskを作り，それを返す関数

** nth_sched.c
*** nth_sch_switch_to()
    *task switchingはMachine dependentに行われている*

* BOTs アプリケーションの特性
** nqueens
   - だいたいこんな感じ
     #+BEGIN_SRC c
/* nqueens() 内 */

/* try each possible position for queen <j> */
for (i = 0; i < n; i++) {
  #pragma omp task untied if(depth < bots_cutoff_value)
  {
    /* allocate a temporary array and copy <a> into it */
    char * b = alloca(n * sizeof(char));
    memcpy(b, a, j * sizeof(char));
    b[j] = (char) i;
    if (ok(j + 1, b))
      nqueens(n, j + 1, b,&csols[i],depth+1);
  }
}
#pragma omp taskwait
for ( i = 0; i < n; i++) *solutions += csols[i];
     #+END_SRC

   - 1 task が再帰的に，n個のtaskを作っている
   - task 生成時に， if(depth < bots_cutoff_value) という風に depth 制限をしている

** strassen
   - depth が cutoff よりも小さい時に， 1 task が再帰的に何個かのtaskを作っている

* BOTs 実験結果
** <2011-04-27 水>
*** tuna
**** protein
     - *mcc はいずれも失敗している -> Segmentation Fault になるのが原因*
     - gcc と icc は task の方が serial よりも有意に速い
     - gcc と icc の間であまり優劣はなさそう

**** fft
     - *mcc with task が失敗している -> OMP_NUM_THREADが多いと(少なくとも32までならおk)memory allocation error*
       - OMP_NUM_THREADS=32 の結果を貼っておく
     - icc,mcc は task の方がserialより速い
     - gcc は task の方がserialより遅い

**** fib
     - どのコンパイラも実行時間は短いので，誤差程度と考えられるかも
     - ただし，いずれのコンパイラも task を使うより serial の方が速い

**** sort
     - mcc with tied task がちょっと遅いかなぐらいで，それ以外に有意な差はなさそう
     - task を使った場合と serial を使った場合の実行時間は同じようなもの

**** sparse
     - *gcc では， for task と for tied task が失敗している*
       - こんな感じのエラー
         #+BEGIN_QUOTE
*** glibc detected *** ./sparselu.gcc.for-omp-tasks: double free or corruption (out): 0x00007fb08001e500 ***
======= Backtrace: =========
/lib/libc.so.6[0x7fb0a81849a8]
/lib/libc.so.6(cfree+0x76)[0x7fb0a8186ab6]
/home/nakatani/software/gcc/lib64/libgomp.so.1[0x7fb0a868795a]
/home/nakatani/software/gcc/lib64/libgomp.so.1[0x7fb0a8689679]
./sparselu.gcc.for-omp-tasks[0x401804]
/home/nakatani/software/gcc/lib64/libgomp.so.1[0x7fb0a8687efa]
/lib/libpthread.so.0[0x7fb0a846afc7]
/lib/libc.so.6(clone+0x6d)[0x7fb0a81e064d]
======= Memory map: ========
00400000-00405000 r-xp 00000000 00:12 315925599                          /home/nakatani/src/bots/bin/sparselu.gcc.for-omp-tasks
00604000-00605000 rw-p 00004000 00:12 315925599                          /home/nakatani/src/bots/bin/sparselu.gcc.for-omp-tasks
00605000-00606000 rw-p 00605000 00:00 0 

...

         #+END_QUOTE

     - for task, for tied task の mcc がひどい
     - serial の方が速い

**** strassen
     - gcc はtaskの方がserialよりも速い
     - *mcc が serial で失敗している -> script のバグ． log2dat._log2blocks() 原因なのは分かるが，治らない...*

**** health
     - gcc は，serial よりも明らかに task の方が遅くなっている． *180秒くらい掛かっているので注意*
     - icc では，serial よりtaskが速くなっている
     - mccはserialよりもtaskの方が遅い

**** floorplan
     - いずれもserialの方がtaskより圧倒的に速い
     - *icc の task では 2000秒ぐらいかかっている．* 実験時に注意
     - gccもひどく，task では 600秒ぐらい掛かっている
     - その点 mcc の task は頑張っていて，せいぜい 4,50秒
     - いずれも serial では10秒ちょい

**** nqueen
     - *gcc の task は 3000秒くらい掛かっている．*
     - gcc の serial は 60秒くらい
     - mcc は task で3,400秒，serialで74秒
     - icc は task の方が serial よりも7倍くらい速い

*** minnie
    実験中に応答がなくなるお・・・


* libgomp src
** データ構造
*** global task queue
    gomp_task* gomp_team::task_queue が実態．ただの配列w

*** private task queue
    struct gomp_thread_pool が実態．
    何か割と(無駄に)複雑なことをするためのデータメンバがある感じ．
    重要そうなメンバは以下
    - gomp_thread** threads: これがqueueとしての実態．taskでなくthreadを格納しているが．．．

*** task
*** user-level thread
    struct gomp_thread が実態．重要そうなメンバは以下
    - fn,data: thread が spawn された瞬間に実行すべき関数とその引数
      - これってきもくね? なに upon launch って

    - ts: team の状態を表す
      - thread が team の状態を把握しとくのって・・・うーん・・・

    - task: bindされたtask

    - thread_pool: private task queue っぽい


*** kernel-level thread

** task.c : GOMP_task
   - これを呼んでいるのは， gcc/omp-low.c の中． gcc/omp-builtins.def の中で， GOMP_task には BUILT_IN_GOMP_TASK という
     名前が付けられている
     - gcc/omp-low.c : expand_task_call() が omp の pragma をパースしている

** omp-low.c
   expand_task_call() がGOMP_taskを作っている
   gimpleがなんか環境(レジスタ値)とかを扱っているのか・・・?
   -> "GIMPLE is a family of intermediate representations (IR) based on the tree data structure. At present, there are only two kinds of GIMPLE: "
      らしい．つまり，GCCの中間コード

* OpenUH src
  場所は， ~/src/OpenUH/osprey/libopenmp/

** Data structure
*** task
    coroutine が実体．これが omp_task_t に typedef されてる
    - coroutine* caller:
    - coroutine* creator: 親task
    - coroutine* restarget:
    - coroutine* next: 兄弟task
    - coroutine* prev: 兄弟task
    - func, data: 実行すべき関数と引数(outliningを使っているんでしょうね)
    - volatile num_children: 子の数．taskwaitで使うんだろうね．volatileは，その変数の使用箇所の最適化を防ぐ．
      最適化はsingle threadを前提に行われるので，複数のthreadが触れる可能性のある変数はvolatile宣言すべき
    - depth: taskのcalltreeを考えたときの深さ．cutoffに使っている (in __ompc_task_depth_cond() )
    - threadid: thread id でしょうね
    - pthread_mutex_t lock:

    stackがないカラクリは

*** user-level thread

*** task queue

** Events
*** Creating task
    co_create() で，
    - stackの確保
    - coroutine 型をした task に初期値を与える
    - coroutine* co の最初の領域に stack を，次の領域に coroutine を割り当て，co を返す
      - つまり co_create() によって返されるポインタは，task with stack の形をしている

*** Task switching
    外部ライブラリに任せてswapcontextしている
* MassiveThreads src
  dequeはJavaのFork/Join(FjTaskRunner)を参考にしているらしい

** 各taskのresultの扱い
   new_thread->result=(*fn)(new_thread->result);
   とし，ストレートにtask構造体(myth_thread)のメンバにresultをいれている．

   この処理をしているのは誰? 各workerのpthreadは，実際どのようにして各taskを実行しているの? 無限ループかな?
   -> ビンゴ
   //Main loop of scheduler
   static void myth_sched_loop(void)
   これが pthread_create() された myth_worker_thread_fn() の中で呼ばれている

   この中で，myth_running_env_t env;に対し新しいtaskのenvを与え，アセンブリでswap contextしている．
   俺がやるときは，アセンブリは使えないので，スケジューラループをpthreadで回し続け，その中で各taskのfuncを呼ぶ．

   swap context で instruction pointer も書き換えてるんだから，そりゃ制御も移る

   tied taskではcontext switch なんかできない．でも，「outliningした何行目から再開するか，その際に使用する環境(レジスタ値)」という情報をoutliningした関数に渡せば，taskwaitでの中断などが実現できそう(libgomp観る必要はある)
   -> GOMP_task()の引数のfnは，outlineされた関数のはずであるが，outliningの仕組みはlibgompではどうなっているのか?
   -> Assembly を読む?
